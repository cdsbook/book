[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Computing and Data for Scientists",
    "section": "",
    "text": "Preface\nPreface."
  },
  {
    "objectID": "src/book/01_intro.html#what-are-the-computational-and-data-sciences",
    "href": "src/book/01_intro.html#what-are-the-computational-and-data-sciences",
    "title": "1  Introduction",
    "section": "1.1 What are the computational and data sciences?",
    "text": "1.1 What are the computational and data sciences?\nIn this book we are concerned with two closely related fields:\n\nComputational science (also called scientific computing) combines computer science (i.e. the theory of how computers work) with applied problems.\nData science similarly combines statistics (i.e. the mathematical theory of analyzing data) with applied problems.\n\nBasically we want to know how to apply computers and data to solve real world problems.\nThere are problems in the world that we could choose to address with computers and data. In this book we will focus on problems from the natural and social sciences (i.e. physics, biology, economics, psychology, etc.)."
  },
  {
    "objectID": "src/book/01_intro.html#why-this-book",
    "href": "src/book/01_intro.html#why-this-book",
    "title": "1  Introduction",
    "section": "1.2 Why this book?",
    "text": "1.2 Why this book?\nMany books on these topics (particularly data science) are interested in business problems. Businesses are typically concerned with learning about the future, typically in the interest of making more profit tomorrow.\nThere is nothing inherently wrong with this, and indeed scientists are sometimes interested in predicting the future as well. However scientists are more generally interested in the broader goal of understanding how the world works. W\nAlthough scientists and business people have to worry about many of the same problems when dealing with computers and data, our different end goals mean that we put more emphasis on different parts of the process.\nAs scientists, we want to learn the underlying truths of the universe.\n\nWe want to make sure that any discoveries we make are real (we don’t want to fool ourselves, a pit that is surprisingly easy to fall into). We therefore care about making sure that our experiments are reproducible. (I.e. if we have discovered something real, any other scientist should be able to follow our steps to get to the same result.)\nSince we care about understanding over prediction, we prefer simpler mathematical approaches that are easier to interpret.\nAs scientists we should try to focus on simplicity and clarity over glamour!\n\nThat’s not to say that a data scientist at a company shouldn’t care about reproducibility, simplicity, and clarity. But if a company can make a million dollars tomorrow with a sexy but complicated and hard-to-reproduce analysis, then they should do so! But this would be bad practice for a scientist."
  },
  {
    "objectID": "src/book/01_intro.html#reproducibility",
    "href": "src/book/01_intro.html#reproducibility",
    "title": "1  Introduction",
    "section": "1.3 Reproducibility",
    "text": "1.3 Reproducibility\nThe rest of this book is focused on how to use computers and data in ways that are good scientific practice. However, let us take a moment to talk more about the importance of reproducibility.\nUnfortunately, scientists are not rewarded for making sure that their analyses are reproducible, or for checking that they can reproduce the work of other scientists.\nIn a recent survey of 1500 scientists (Baker 2016), a shockingly high percentage said that they were unable to reproduce another scientist’s work:\n\n\n\n\n\nFigure 1.1: Data from (Baker 2016).\n\n\n\n\nConsider how the process of modern science works:\n\nThe scientist needs to get money to pay for their future work, so they apply for grants. In the USA, a lot of grant money is distributed by the federal government; other grants may come from private organizations. However, there is never enough money for every scientist who applies, and so the grant distributors have to pick their favorites. And what makes a proposal likely to be funded? Doing exciting, new research (not routine reproduction of other scientists existing work).\nEven if you decide to reproduce an experiment, its not always simple to do so. Scientists are supposed to describe the steps they followed in their experiment, but they do not always provide enough details for somebody else to replicate their work exactly. Sometimes, scientists can’t even reproduce their own work (this is not uncommon in laboratory experiments in chemistry or biology, where you are working with minuscule substances that you hope are in your test tubes, but which might be subtly different from one day to the next).\nIn the computational and data sciences, there are often subtle choices we can make in terms of parameter values, or choice of algorithm, or simply the way that you write your code, which can significantly alter the output of your computer program. Thus you may also need the original computer code that was used for the analysis!\nIf the experiment is the analysis of an existing dataset, then anyone wishing to reproduce the work will need access to the same data and code. Unfortunately, scientists do not always make their data or code available when they publish their results.\n\nSometimes this is for good reasons (e.g. protecting the privacy of the people in the dataset, or not publishing the genetic sequence of a dangerous virus).\nOther times the reasons might be more self-serving. For example, if you were able to get a large grant of money to do a difficult experiment, you might want to publish several different analyses of the results. Unfortunately there are no prizes for second place in science, and the first person to make a discovery gets the glory. If somebody else publishes a discovery that you were about to publish yourself, then scientists call this getting “scooped”. To avoid getting scooped, you might decide to restrict access to your data until you have published all the analyses of it that you want to.\nYou can restrict data access in responsible ways, typically by publishing the data at the same time as your study but placing an embargo on it. This delays the publication of the data, typically by several months or years, until you have had a chance to publish your other analyses, but ensures that it will eventually become available to any other scientists who want to check your work or extend upon it.\nAnd sometimes scientists don’t have a good reason for not publishing their data/code, except that it wasn’t required of them1. Fortunately this attitude is now changing, as science as changed its incentives around reproducibility. For example:\n\n\nIn the USA, the federal government has started to require that any science done using its grants has to make data available when results are published.\nNew initiatives like the Center for Open Science2 are promoting the sharing of data and code, for example by creating the Open Science Framework3 for new studies\n\n\nThere are now various websites that will permanently archive code and data so that it can be easily shared with other researchers, such as DataDryad and FigShare.\n\n\n\n\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a."
  },
  {
    "objectID": "src/book/01_intro.html#footnotes",
    "href": "src/book/01_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a problem of both the scientists as well as science as a discpline.↩︎\nhttps://www.cos.io/↩︎\nhttps://osf.io/↩︎"
  },
  {
    "objectID": "src/book/02_setup.html#code-editors",
    "href": "src/book/02_setup.html#code-editors",
    "title": "2  Setup",
    "section": "2.1 Code editors",
    "text": "2.1 Code editors\nProgramming code is typically written in plain text files (but instead of the file extension .txt they will typically use a different extension that indicates the programming language being used). Traditionally, code in the R programming language was written in R script files with the .R file extension.\nBecause these code files are just text, all you need to open them is a text editor, like Windows’s Notepad or Mac’s TextEdit. However, you can also use a dedicated code editing program, which will add a lot of useful features that makes writing code much easier. These editing programs are often called Interactive Development Environments, but because that is a mouthful, we usually just refer to them using their acronym: IDEs.\nThe most popular IDE for the R programming language is called RStudio, and there are two easy ways to use it.\n\nDownload and install the free RStudio Desktop version to your computer.\nUse the online Posit Cloud1 version through a web browser (no installation required, but only free for a certain number of hours each month2).\n\nThese two versions are functionally equivalent, so the only things you really have to decide is how averse you are to paying money and whether you can install software on the computer you are using.\n\nHow to pick the best RStudio editor for you.\n\n\n\n\n\n\n\n\nDon’t want to pay\nCan pay if necessary\n\n\n\n\nCan install software\nInstall RStudio Desktop.\nEither works.3\n\n\nCan’t install software\nUse Posit Cloud online, and don’t exceed the monthly free quota.\nUse Posit Cloud.\n\n\n\nLater in this chapter I will give instructions for getting each of these options up and running.\nThere are also other IDEs that you can use to write R code. A popular general IDE is VS Code, which can be used to write in any different programming language. The downside is that it lacks a lot of R specific features that you can find in RStudio. However, instructions to get set-up with VS Code are provided in an Appendix: Section B.3."
  },
  {
    "objectID": "src/book/02_setup.html#version-control",
    "href": "src/book/02_setup.html#version-control",
    "title": "2  Setup",
    "section": "2.2 Version control",
    "text": "2.2 Version control\nMost programming is not actually that difficult, once you learn how to think like a computer. The hard part of programming is writing large complicated programs with other programmers.\nOne of the reasons this is challenging is because we need a way to collaboratively edit the same set of files that contain our code.\nThis is not just a problem for programmers - if you and a friend were writing a report together in Microsoft Word, you might find yourself emailing the Word document back-and-forth. In fact, Microsoft Word has helpful a feature called Track Changes that allows you to see who has edited different parts of a file.\nUnfortunately, you can’t edit the file while your friend is, otherwise you will end up with different versions of the document, and the only way to recombine them will be to compare them side-by-side and manually copy over any differences.\nIn these modern times, you could instead use an online collaborative program like Google Docs - but while that is fine for text, programs need to be run, and that usually needs you and your friend to be working on separate computers so that your versions of the code don’t interfere with each other when running.\nSoftware engineers have come up with solutions to these problems, which they call version control (because it enables you to control the version of the program that you are running). The dominant version control software used today is called Git, and it is so popular that IDE’s like RStudio automatically include integrations to work with it.\nWe will talk more about how Git works in Chapter 12 but for now you can think of it as like the save points in a video game. When you reach a significant point (like adding an important software feature, or defeating a boss in a video game) you can save your progress at that point. This allows you rewind your progress back to that point if you make a mistake in the future.\nGit can also figure out how to automatically combine different versions of a project (i.e. your version and your friend’s version), and it will keep track of all the changes you record in a save point as well as who made them.\nWe can also use websites like GitHub.com to share projects that are managed with Git. This is useful for a number of reasons, but from a reproducibility perspective it allows other scientists to download our code and run it for themselves."
  },
  {
    "objectID": "src/book/02_setup.html#step-1-create-a-github-account",
    "href": "src/book/02_setup.html#step-1-create-a-github-account",
    "title": "2  Setup",
    "section": "2.3 Step 1: Create a GitHub account",
    "text": "2.3 Step 1: Create a GitHub account\nIf you already have a GitHub account, then proceed to step 2.\nIf not, then go to https://github.com and create a free account. A few suggestions:\n\nIf you have any interest in working in tech in the future, then pick something vaguely professional as your GitHub username. You don’t want to have to explain to a future employer why your GitHub username is squeaky_boi69. Think of your GitHub profile as the the programming equivalent of your LinkedIn profile.\nGitHub will ask if you want to upgrade to a fancy paid account when you register, but you should stick with the free account which has everything we need.\nGitHub will also ask you a bunch of questions when you sign up about what you want to use it for. It really doesn’t matter what you respond to these questions, so feel free to skip through them."
  },
  {
    "objectID": "src/book/02_setup.html#step-2-set-up-rstudio",
    "href": "src/book/02_setup.html#step-2-set-up-rstudio",
    "title": "2  Setup",
    "section": "2.4 Step 2: Set up RStudio",
    "text": "2.4 Step 2: Set up RStudio\n\n2.4.1 Option 1: Install RStudio Desktop on your computer\nIf you are installing RStudio Desktop on your computer, then you should follow these steps:\n\nFirst, you should first install Git by following the appropriate installation instructions for your operating system on the Git website: https://git-scm.com/downloads\n(Note that Git is almost certainly already installed if you are using Linux.)\nNext you should install R by going to the appropriate page for your operating system:\n\nWindows: download and install R from the .exe installer on this page: https://cran.rstudio.com/bin/windows/base/\nThen also install RTools from this page (make sure the version number of RTools matches the version number of R that you just installed, e.g. if you installed R v__4.2__.3 then you will need to install RTools v.__4.2__): https://cran.rstudio.com/bin/windows/Rtools/\nMac: download and install R from the appropriate .pkg installer for your version of macOS on this page: https://cran.rstudio.com/bin/macosx/\nLinux: follow the instructions for your flavor of Linux: https://cran.rstudio.com/\n\nFinally we are can install RStudio Desktop. Download the installer for your operating system here and then install from it: https://posit.co/download/rstudio-desktop/\nNote that RStudio Desktop is free, but Posit (the company that created RStudio) also offers several paid versions, so make sure you get the free RStudio Desktop version.\nAfter you have installed RStudio, you should be able to start the program, which should look like this:\n\nNext we need to install a program called (this will turn our files containing R code into nicely formatted PDFs).\nTo do this, open RStudio. There should be a pane on the left called “Console”. In this Console, copy and paste the following two lines and hit enter to run them:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOnce you have done this, proceed to Section 2.5 to connect GitHub to RStudio.\n\n\n2.4.2 Option 2: Create an online RStudio account at Posit Cloud\nGot to https://posit.cloud/ and create an account. You can sign up for the free plan, which at the time of writing includes 25 hours of online RStudio access per month before they ask you to pay.\nTo connect your GitHub account, click on your profile name/icon in the top right of the posit.cloud homepage, and then on the Authentication page.\nFind the line on this page where it says GitHub, and slowly click any unchecked checkboxes (wait a couple of seconds between each checkbox in case a prompt appears).\nSome of the checkboxes may open up new webpages taking you to GitHub, which will ask you to verify that you want to authorize Posit to access your GitHub account. Make sure you agree all of these, otherwise you might not be able to edit code on GitHub."
  },
  {
    "objectID": "src/book/02_setup.html#sec-rstudio-github-connection",
    "href": "src/book/02_setup.html#sec-rstudio-github-connection",
    "title": "2  Setup",
    "section": "2.5 Step 3: Connect RStudio to GitHub",
    "text": "2.5 Step 3: Connect RStudio to GitHub\n\nFirst, go to RStudio (or launch a new empty RStudio project if you are using the online RStudio at posit.cloud) and open the tab in the left hand pane called Terminal. If you do not see a Terminal tab, then you can create one from the top menu of RStudio Desktop by going to “Tools &gt; Terminal &gt; New Terminal”.\nIn this terminal, set your GitHub username by running this line, making sure to replace your name inside the quotation marks:\ngit config --global user.name \"Your Name Here\"\nThen run this commend, again making sure to replace the email inside the quotation marks with the same email you used to sign up for GitHub:\ngit config --global user.email \"you@emailHost.com\"\nI would also recommend running one final line in the Terminal (this will enable your computer to store your GitHub login details - otherwise you will be typing them in a lot).\ngit config --global credential.helper store\nThen go to the Console tab (which should be next to the Terminal), and copy and paste these lines of R code one at a time:\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nthen this line (which will open a GitHub web page - see below for what to fill in)\nusethis::create_github_token()\n\nOn the token webpage that appears, make sure that you are creating a “Classic token”, and not a “Fine-grained” token. Then you will need to set the following options:\n\nIn the Note field, write something that indicates where this token will be used, e.g. RStudio.\nFor the expiration date, pick a date about in the future after which you will no longer need the token. E.g. if you are following these instructions for a class, pick a date after the end of the semester.\nYou should not select no expiration date - that is a security risk.\n\nYou can leave all the checked scopes as the defaults (you need the first set of repo scopes), and then scroll down to the green Generate token button at the bottom of the webpage and click it.\nThe next page that appears will display a token, a random series of letters and numbers that is basically a temporary password that you can use to authorize a restricted set of activities on your GitHub account (without having to share your master password with RStudio). You will never see this token again after you leave this page, so don’t close the webpage until you have finished this section, or you will have to create an entirely new token.\nReturn to the Console tab in RStudio, and run this line:\ngitcreds::gitcreds_set()\nAt the prompt, copy and paste the token from GitHub and click enter.\n(If you ever need to replace the token, just run gitcreds::gitcreds_set() in the RStudio Console again.)"
  },
  {
    "objectID": "src/book/02_setup.html#footnotes",
    "href": "src/book/02_setup.html#footnotes",
    "title": "2  Setup",
    "section": "",
    "text": "Formerly known as RStudio Cloud.↩︎\nAt the time of writing, you get 25 hours per month for free, after which you have to pay.↩︎\nBear in mind that RStudio Cloud can only be used if you are connected to the internet, so if you want to work somewhere without internet then you will need to install RStudio Desktop. Also, RStudio Desktop will ultimately provide you with more flexibility.↩︎"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#data",
    "href": "src/book/03_r_programming_chapter.html#data",
    "title": "3  Introduction to R",
    "section": "3.1 Data",
    "text": "3.1 Data\nThe central component of everything we will be doing in R this semester is data. Even non-data science programs revolve around data.\nAt a very basic level, a computer is just a fancy calculator that adds and subtracts numbers. Even things like words and pictures are stored inside a computer as numbers.\nHowever, we often want to work work with data that is not numbers. For example, in the last section we were able to get R to print out the sentence Hello, World! To your computer that was just numbers flowing down wires as electrical signals. But the R programming language took care of converting our instruction into something your computer could understand.\nThis is the magic of programming languages! They allow us to write commands in (relatively) human-readable instructions, and then take care of translating that into the very unreadable numbers that computers work with.\nR allows us to work with several “higher-level” types of data. These data types include:\n\nthe numeric data type holds numbers such as 42 or -12.5 or 0. Unlike text, numbers are written without quotation marks around them.\nthe character data type holds text (i.e. letters, symbols, and the characters that represent numbers). We need to put the text inside quotation marks so that R knows where the text starts and ends: \"this is character data\".\n\nNote: in other programming languages this datatype is sometimes known as a “character string” or just a “string”.\n\nthe Boolean data type holds a value that is either TRUE or FALSE. (This is sometimes also referred to as the logical data type.)\n\n\n\n\nExercise: What data type is \"Introduction to Computing and Data for Scientists\"?\n\n\n\n\nExercise: Type the number 2 into the RStudio console. Hit the  key to run this line of code. What do you get back?\n\n\n\n\nExercise: What data type is FALSE? In the RStudio Console, type in typeof(FALSE), and hit the  key to run this line of code. What do you get back?"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#operators",
    "href": "src/book/03_r_programming_chapter.html#operators",
    "title": "3  Introduction to R",
    "section": "3.2 Operators",
    "text": "3.2 Operators\n\n3.2.1 Combining data with operators\nOkay, so now we know about data.\nBut data by itself is not especially useful. It just sits there until you do something to it. In programming languages there are often many ways of doing things to data, but one of the simplest is operators.\nOperators operate on data. The name may sound unusual, but you are already familiar with many operators, such as + and - for adding and subtracting numbers.\n\n\n\nTry entering a number after the &gt; in the Console (e.g. 1), then Enter, and see what happens.\n\nWhen you hit enter, the R interpreter reads in the line, evaluates it, and returns the answer. In this case, you entered 1, so the computer thinks ‘Hey, it’s a 1! Wow, a one! The result of 1 is… drum roll, please… 1!’ and returns the result of this expression, which is a one.\nCool! But not, I confess, particularly useful. Let’s fix that: next we’ll add two numbers together.\n\nAt the prompt, enter two numbers separated by a plus sign, +\n&gt; 1 + 1\nWhat do you get?\n\n(Note that I’ve left the Console’s &gt; prompt in the example code above, but I will leave it out in future.)\n\n\n\n\nGreat! Let’s move on and investigate operators in more depth…\n\n\n3.2.2 Operating on numbers\nHeart surgeons operate on hearts, brain surgeons operate on brains. You will be operating on numbers… does that make you a data surgeon?\nHere are some of the operators available to us in R:\n\n\n\nOperator\nExample\nResult\n\n\n\n\n+\n5 + 2\n7\n\n\n-\n5 - 2\n3\n\n\n*\n5 * 2\n10\n\n\n/\n5 / 2\n2.5\n\n\n^\n5 ^ 2\n25\n\n\n%%\n5 %% 2\n1\n\n\n&lt;\n5 &lt; 2\nFALSE\n\n\n&gt;\n5 &gt; 2\nTRUE\n\n\n==\n5 == 2\nFALSE\n\n\n\nSome of these might seem obvious, while others might be unfamiliar. Let’s go through them all and figure out what they do.\n\n\n3.2.3 The - operator\n\n\n\nIn the R Console, type 5 - 2 and hit enter to run the line of code.\nYou probably have a good idea of what - does, but try changing the numbers just to make sure!\n\n\n\n\n\n3.2.4 The * operator\n\n\n\nIn the R Console, type 3 * 2 and hit enter to run the line of code.\nWhat does * do?\n\n\n\n\n\n3.2.5 The / operator\n\n\n\nIn the R Console, type 3 / 2 and hit enter to run the line of code.\nWhat does / do? Just to be sure, try some other numbers.\n\n\n\n\n\n3.2.6 The ^ operator\n\n\n\nIn the R Console, type 3 ^ 2 and hit enter to run the line of code.\nWhat does ^ do? Try some other numbers like 2 ^ 3 or 16 ^ 0.5\n\n\n\n\n\n3.2.7 The %% operator\n\n\n\nNext up, a slightly trickier one, type 3 %% 2 and hit enter to run the line of code.\nWhat does %% do? You will probably have to try some other numbers to figure this one out.\nIf you have difficulty try also dividing the same numbers. E.g. try both 9 %% 4 and 9 / 4.\n\n\n\n\n\n3.2.8 The &lt; and &gt; operators\n\n\n\nSo far we have operated on numbers and got numbers back. However these next operators will return a different type of data.\nWhat do you get if you run 8 &gt; 10?\nCan you change the 8 into a number so that this expression returns TRUE?\nThen change the &gt; to a &lt; operator (i.e. reverse its direction). What is the result now?\n\n\n\n\nBoolean operators\nIn the previous section we learned about the Boolean data type which has only two possible values: TRUE or FALSE.\nOperators that always return a Boolean value are called Boolean operators. The greater than (&gt;) and less than (&lt;) operators are examples of Boolean operators.\nYou can think of Boolean operators as asking a question that can only be be answered with either “true” or “false”, such as “Is 8 greater than 10?”\n\n\n\n3.2.9 The == operators\n\n\n\nLet’s try another Boolean operator. What do you get if you run 8 == 10? What about 8 == 8?\nWhat do you think the == operator does?\n\n\n\n\nCombining comparisons\nSometimes we want to know if one datum is greater than or equal to another. You can use the Boolean operators &gt;= for such a comparison, or &lt;= to see if something is less than or equal to another.\n\n\n\n3.2.10 Which operator goes first?\nJust like in normal math, we can do sums in R with multiple operators:\n3 + 5 / 5 * 3 ^ 2\nIn such a case, which operation do we do first?\nAgain, just like in regular math, some operations are always done before others. For example, all multiplication and division will be done before any addition or subtraction.\n\nF.Y.I.\nThe order in which operators are calculated is known as operator precedence, and you can find the precedence of any operator here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html\n\nWe can change the order of operations with parentheses: ( and ). For example\n2 + 2 * 5 = 12\nwhereas\n(2 + 2) * 5 = 20\n\n\n\nModify this R code\n3 + 5 / 5 * 2 ^ 2\nso that it performs the calculation $\frac{3 + 5}{(5 imes 2) ^ 2}$.\nWhen correct, you should get the answer 0.08."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#storing-and-reusing-results-with-variables",
    "href": "src/book/03_r_programming_chapter.html#storing-and-reusing-results-with-variables",
    "title": "3  Introduction to R",
    "section": "3.3 Storing and reusing results with variables",
    "text": "3.3 Storing and reusing results with variables\nSo far we have learnt how to combine data to get different results.\nWe can do multiple separate calculations by putting each one on a separate line. When R reads your code, it treats everything on one line as a single expression that is separate from other lines:\n2 + 2\n5 * 5\nThis program will have two separate outputs: 4 and 25\nHowever, after these results are shown to us, they are thrown away! All that effort just discarded…\nWhat if we wish to save the result of a calculation so that we can reuse it in a subsequent line?\nIn this case, we need to store the result in a variable.\n\n\n\nRun these two lines of code in the RStudio Console and see what result you get.\na &lt;- 2 + 2\n5 * a\nThen take a look at the Environment tab in the top-right pane of RStudio. Do you see a variable called a? Does it hold the value calculated in the first line of code or the second?\n\n\n\n\n3.3.1 The “result” of the assignment operator\nWe store the result of an expression in a variable using the assignment operator: &lt;-\nvariable_name &lt;- value_to_be_stored\n\n\n\nRun these two lines of code in the RStudio Console and see what result you get.\n2 + 3\nb &lt;- 2 + 4\nIf you take another look at the Environment tab in the top-right pane of RStudio, you should see another variable called b. What value does it hold?\n\n\n\n\n\n2 + 3\n\n\nb &lt;- 2 + 4\n\n\nThe second line of code should not print out any output when it runs. This is because assignment has no “result” in the same way that a math expression does, so there is nothing to print out.\nIf you want to see the data that is stored in a variable, you can put the name of the variable on a line by itself:\nsome_variable\nR will evaluate this line: it will ask itself “What is the result of some_variable”, which is just whatever value is stored in that variable.\nFor example,\n\nc &lt;- 3\n\nc\n\n[1] 3\n\n\nThe other implication of this is that if you calculate something in R and do not assign the result to a variable then it will be printed out and then forgotten. So remember: if you calculate something important in R that you will need in the future, make sure that you store it in a variable.\n\n\n\nType the name of one of the variables in your Environment tab in the RStudio Console (e.g. a), and hit Enter to run it.\nDoes this return the data that you think is stored in that variable?\n\n\n\n\n\nb\n\n\n\n\n3.3.2 Variables are… variable\nVariables get their name because their value can vary. We have created the variable b that holds the value 6, but we can change the value of b and store a completely different value in it!\n\n\n\nTry assigning the value 7 to the variable b using the assignment operator &lt;-.\nSince we already created b in an earlier exercise, you should see that its value in the Environment tab updates.\n\n\n\n\n\nb &lt;- 7\n\n\n\n\n\n3.3.3 When does assignment happen\n&lt;- is an operator, just like + or *. As such, it has a precedence: it will happen before some operators but after others.\nHowever, it turns out that the &lt;- precedence is extremely low - i.e. it will happen after the result of all the other operators on that line of code have been calculated.\nSo, when you write:\na &lt;- 2 + 4\n…you are essentially doing this:\na &lt;- (2 + 4)"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#how-r-works",
    "href": "src/book/03_r_programming_chapter.html#how-r-works",
    "title": "3  Introduction to R",
    "section": "3.4 How R works",
    "text": "3.4 How R works\n\nR is an interpreted programming language.\nThat is a fancy way of saying that R runs (i.e. “interprets”) every line of code one at a time.\nSo far we have written a line of code and then run it. In a couple of exercises you may have run multiple lines of code where one line depended on a result from a previous line. However, R completely finished running the first line before moving onto the next one.\nWhen R interprets a line of code, it figures out how to convert your human-readable code into computer-readable instructions (which are a series of 0s and 1s, since a computer is basically a bunch of wires that can either have an electrical current flowing down them (which we denote as 1) or not (0)).\nBecause R is interpreted line-by-line, it is an ideal programming language for exploring and analyzing scientific data, where we typically figure out what to do next as we go along!\nWe will soon be learning how to write multiple lines of R code in a file and then run them from the file. However, even when R runs code from a file, it still figures out how to run it one line at a time.\n\nCompiled Programming Languages\nNot every programming language is interpreted like R. Some are compiled.\nThis means that you write all your code in a file, and then turn all of it into computer-readable instructions at once. This step is called compilation and can take a long time (up to hours for large programs in some languages!). It is typically slower to write programs in a compiled language because of this extra step.\nThe main advantage of a compiled programming language is that your computer can figure out how to optimize all the lines of code so that they run extremely fast.\nFamous examples of compiled programming languages are Java and C++."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#more-about-data",
    "href": "src/book/03_r_programming_chapter.html#more-about-data",
    "title": "3  Introduction to R",
    "section": "3.5 More about data",
    "text": "3.5 More about data\n\n\n3.5.1 Boolean data revisted\nAt the start of this chapter we mentioned that there is a type of data in R, called Boolean data, that can have one of two values: TRUE or FALSE.\nThen in the section on operators we saw that some operators (those that do comparisons) would give a TRUE/FALSE result even when the input data was not Boolean. For example,\n\n2 == 2\n\n[1] TRUE\n\n\nbecause 2 is equal to 2 (i.e. this is a true expression).\nThe value 2 is an example of numeric data. However, the result TRUE is an example of Boolean data.\nA Boolean value can only have one of two values (i.e. TRUE and FALSE). We often get a Boolean value when we do comparisons (e.g. checking if two values are equal, or if one is greater than the other).\nJust as with numeric data, we can store a Boolean value in a variable, e.g. d &lt;- FALSE.\n\n\n\nAssign the value TRUE to a variable called d.\n\n\n\n\n\n d &lt;- TRUE\n\n\n\n\n3.5.2 Combining pieces of data\nSo far we have looked at pieces of data by themselves:\n\na &lt;- 1\nb &lt;- \"Hello!\"\nc &lt;- TRUE\nprint(a)\n\n[1] 1\n\nprint(b)\n\n[1] \"Hello!\"\n\nprint(c)\n\n[1] TRUE\n\n\nBut what about if we want to combine multiple pieces of data together?\nR includes several types of container that can hold multiple pieces of data. We can then refer to that container by a single variable. For example, instead of the three variables above, we can create a list object that holds all three values:\n\nl &lt;- list(1, \"Hello!\", TRUE)\nl\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"Hello!\"\n\n[[3]]\n[1] TRUE\n\n\nWe create a list using list(). Every value in the list goes inside the parentheses, separated by commas.\n\n\n\nCreate a list holding 4 values (in this order): 10, “z”, FALSE, -0.1*10\n\n\n\n\n\nlist(10, \"z\", FALSE, -0.1*10)\n\n\n\n\n3.5.3 Vectors\nLists can hold data of different types. For example, our list l holds a number, a character string, and a Boolean value.\nAn alternative container in R is a vector. Vectors can also hold multiple pieces of data, but unlike a list, all the data in a vector must be the same type of data. For example, a vector could contain all numbers, or all characters, but not a mix of the two.\nWe create a vector with c():\n\nc(1,2,3)\n\n[1] 1 2 3\n\n\n\n\n\nCreate a vector holding 3 character strings (in this order): “This”, “is a”, “vector!”\n\n\n\n\n\nc(\"This\", \"is a\", \"vector!\")\n\n\n\nYou might be wondering what the numbers in square brackets at the start of each line in the output mean? E.g. [1]\nThese tell us where abouts in the vector we are. The number indicates the position in the vector of the first element displayed on that line.\nFor example, the [1] at the start of the line (before “Introduction”) shows that “Introduction” is the first element in this vector. ### Operations on vectors\n\nWe can use operators on more complicated data structures just as we did on the simpler data types. For example, we can add 2 vectors together:\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\nv1 + v2\n\n[1] 5 7 9\n\n\nAs you can see, the individual elements are added together.\n\n\n\nWhat happens if you add two vectors of different lengths? For example, run this code and see what happens:\n\n\n\nv3 &lt;- c(10, 20, 30, 40, 50)\nv4 &lt;- c(1, 2)\nv3 + v4\n\nWarning in v3 + v4: longer object length is not a multiple of shorter object\nlength\n\n\n\nWhat happens when you add two v3 and v4?\n\nThe 2 elements of v3 are added to the first 2 elements of v4, and the rest of v4 is unchanged.\nv4 is repeated to match the length of v3 before they are added.\nR guesses values for the remaining values of v3 so that the two vectors are the same length.\n\n\n\n\n\nv3 &lt;- c(10, 20, 30, 40, 50)\nv4 &lt;- c(1, 2)\nv3 + v4"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#expressions-and-how-they-are-evaluated",
    "href": "src/book/03_r_programming_chapter.html#expressions-and-how-they-are-evaluated",
    "title": "3  Introduction to R",
    "section": "3.6 Expressions and how they are evaluated",
    "text": "3.6 Expressions and how they are evaluated\nTODO\n\nExpressions and how they are evaluated. Lines and files."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#functions",
    "href": "src/book/03_r_programming_chapter.html#functions",
    "title": "3  Introduction to R",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nPerhaps, keen mathematician that you are, you want to calculate the length of the hypotenuse of a triangle. Dredging up memories of early math classes, you will doubtless recall Pythagoras’s theorem that the hypotenuse (the long side) of triangle is given by:\n\\(c = \\sqrt{a^2 + b^2}\\)\n(\\(c\\) is the hypotenuse [long side] and \\(a\\) and \\(b\\) are the short sides.)\n\n\n\nLet’s say we have a triangle where the shorter sides (a & b) are 3 and 4 units long. Can you calculate the length of side c in R using just the operators from the first section?\nHint #1: The square root is equal to the 0.5 power of a number: 4 ^ 0.5 = 2\nHint #2: Just like in regular math equations, R will calculate some operators before others. For example it will do all multiplications before any additions. However, just like in regular math, we can change the order of operations by wrapping parts of our calculation in parentheses: (...)\n\nDid you get the answer 5? Fantastic!\n\n\n\n\n\n3.7.1 Re-useable code = functions\nWhat’s that? Another complaint? You have to write out this long expression every time you need the hypotenuse of a triangle? (No doubt this is a frequent chore in your day-to-day life.)\nAgain, there is a solution! R allows us to save pieces of code in variables. Yes, you heard that right: variables don’t just have to store data, they can also store code!\nThese stored, reusable sections of code are called functions.\nFor example, you could create a function to calculate the sum of two numbers:\nadder &lt;- function(number1, number2) {\n    result &lt;- number1 + number2\n    return(result)\n}\nEntering these 4 lines at the console prompt will be slow and error-prone, so let’s try something different.\nClick on the “File” menu at the top of RStudio. Select “New File” and then “R Script”. A blank editor window should appear in a new pane above the console.\nCopy the adder function from the previous page into this empty script. Then press “Control + Alt + R” on your keyboard (simultaneously). This will run the contents of your script all at once.\nIf successful, you should see that adder appears in the Environment pane under a new section called Functions.\nHow do we use our adder function? Go back to the console, and type something like this:\n\nadder(3, 5)\n\nIf your function is working correctly you should get the result of the 2 numbers that you entered inside the braces.\nLet’s take another look at the adder function to understand what’s going on:\nadder &lt;- function(number1, number2) {\n    result &lt;- number1 + number2\n    return(result)\n}\nLine 1: The first line creates a new function with the function keyword and saves it to the name adder using the assignment operator &lt;-, just as we did for variables.\nAfter function are a pair of parentheses. Inside these, we put a list of the parameters that the function can take, separated by commas. In this case, our adder function has two paramters (the numbers to add together). We are going to give these numbers the temporary names number1 and number2 (creative, I know). We will use these parameter names inside the function to refer to these two numbers.\nWe end the line with an opening curly bracket { to indicate that the code that follows is part of the function.\nLine 2: This is the meat of our adder function. We add our two number paramters together and store them in a variable called result. Its important to note that result only exists inside the curly brackets of the adder function (i.e. it vanishes after the function has finished).\nLine 3: Here we specify what the function is should return: in this case we want to return the result variable.\nLine 4: We signal the end of the function with a closing curly bracket (matching the one from the end of line 1).\nYou might object (and not without reason) that our adder function is a very trivial example. Wouldn’t it just be easier to use the + operator?\nYes, it would! So let’s look at a more complicated function.\nWe can create a function to calculate the hypotenuse like this:\n\nhypotenuse &lt;- function(a, b) {\n  c &lt;- (a^2 + b^2)^0.5\n  return(c)\n}\n\nThen we can use this hypotenuse function as many times as we like. For example calculate the hypotenuse of a triangle with sides of length 3 and 4, we would run:\n\nhypotenuse(3, 4)\n\n[1] 5\n\n\n\n\n\nUse the hypotenuse() function to calculate the area of a triangle with sides of length 3 and 4.\nHint: Try changing the numbers inside the parentheses after hypotenuse.\n\nDid you get the answer 5? Fantastic!\n\n\n\n\nhypotenuse &lt;- function(a, b) {\n  c &lt;- (a^2 + b^2)^0.5\n  return(c)\n}\n\n\n\n\n3.7.2 How the hypotenuse function works\nThere are a few things to note about this code:\n\nWe tell R that we are creating a reusable function using the function keyword.\nfunction is followed by parentheses (...) that contain parameters. Parameters are the names that we give to the input data to the function.\n\nFor example, above we created two parameters: a and b\nYou can have as many parameters as you want in a function, from zero on up. They must be separated by commas.\n\nThe reusable code goes inside a pair of curly brackets {...}\n\nWe can now use the function’s parameters in this code (e.g. a and b). Essentially we temporarily create new variables with the parameter names (but these are)\n\nAt the end of the function we can return a particular result with return(...) - just replace the dots with a value or\nWe store the function in a name with the assignment operator &lt;- (just like we did with variables)\nWhen we want to run the code, we write the function name followed by parentheses, with any arguments inside the parentheses (separated by commas)\n\n\n\n\nReplace the blanks to create a function to calculate the area of a triangle instead. Save this function as triangle_area.\n_______ &lt;- function(a, b) { area &lt;- _______ return(area) }\nHint: The area of a triangle is \\(0.5 imes a imes b\\)."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#packages",
    "href": "src/book/03_r_programming_chapter.html#packages",
    "title": "3  Introduction to R",
    "section": "3.8 Packages",
    "text": "3.8 Packages\nFunctions are clearly useful - we can save a lot of time and effort by writing our code once as a function, and then just calling that function whenever we need to do that thing.\nOf course, we can save even more time by not writing the function ourselves but instead using a function that somebody else has written which does what we want.\nIn R (as in many other programming languages) we can import collections of functions (and other useful things, such as datasets) that other people have written. These collections are called packages.\n\n3.8.1 Installing packages\nBy default R will come with several useful packages installed. You can which ones are currently installed by going to the Packages tab of the bottom right pane in RStudio.\nTo install a new package, either:\n\nClick on the Install button in the Packages tab, and type the name of the package you want into to the pop-up that appears.\nGo to the RStudio Console and type in (making sure to replace the name of the package you want inside the quotes!):\ninstall.packages(\"some_package_name\")\nFor example, to install a package called the tidyverse (which we will be using for much of this book), you would run:\ninstall.packages(\"tidyverse\")\nHaving gone through this chapter, this code should hopefully make some sense! install.packages() is a function (built-in to the core R programmin language), and \"tidyverse\" is a character string that we are passing as the argument to that function.\n\nNote that it can take some time to install a package (e.g. the tidyverse package can take 10-15 minutes to install!), so it’s worth checking to see if it is already installed before you waste a lot of time.\n\n\n3.8.2 Loading packages\nA package only needs to be installed to your computer once.\nHowever, you need to load the functions and other objects from that package in every R session that you wish to use them (because they will not automatically be available to R even after you have installed them).\nTo load a package we use the library() function. For example, to load the tidyverse package, you would run:\nlibrary(tidyverse)"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html",
    "href": "src/book/04_rmarkdown_chapter.html",
    "title": "4  Literate Programming",
    "section": "",
    "text": "RMarkdown and Quarto\nReproducibility\n\nOpen Science Foundation: https://osf.io/\n\nIncludes replication studies, e.g.\n\nCancer Biology reproducibility project: https://osf.io/collections/rpcb/discover e.g. https://osf.io/nbryi/\nPsychology reproducibility project: https://osf.io/ezcuj/wiki/home/"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#visualization-principles",
    "href": "src/book/05_visualization_chapter.html#visualization-principles",
    "title": "5  Visualizing Data",
    "section": "5.1 Visualization principles",
    "text": "5.1 Visualization principles\nThis video will introduce you to the importance of visualization in data science, and how it can be used effectively (or ineffectively).\n\n\nSlides: PDF\n\nA picture is worth thousand words…\n\n…and a good graph is clearer than a table of data.\nIn pre-modern times, our ancestors did not have to worry about tables of data (or words). But they did have to interpret patterns in what they saw.\nWe are stuck with the same brains, and so while a column of numbers doesn’t mean much, if we can transform that column into a picture then patterns can become much clearer.\nExample: Challenger Disaster\nIn 1985 the\n\nlibrary(tidyverse)\ntufte_data &lt;- tibble(\n  flight = c(\"51-C\", \"41-B\", \"61-C\", \"41-C\", \"1\", \"6\", \"51-A\", \"51-D\", \"5\", \"3\", \"2\", \"9\", \"41-D\", \"51-G\", \"7\", \"8\", \"51-B\", \"61-A\", \"51-I\", \"61-B\", \"41-G\", \"51-J\", \"4\", \"51-F\"),\n  temperature = c(51,57,58,63,66,67,67,67,68,69,70,70,70,70,72,73,75,76,76,76,78,79,80,81),\n  damage = c(11,4,4,2,0,0,0,0,0,0,4,0,0,4,0,0,0,0,0,0,0,0,NA,0)\n)\ntufte_data %&gt;%\n  ggplot(aes(temperature, damage)) +\n  geom_point() +\n  geom_smooth() +\n  xlim(20,90)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nPictures of data are called “graphs”.\nThere are many ways that we can visualize the same data. Some of these ways may be good, but many of them will be bad."
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#data-overview",
    "href": "src/book/05_visualization_chapter.html#data-overview",
    "title": "5  Visualizing Data",
    "section": "5.2 Data Overview",
    "text": "5.2 Data Overview\nBefore we begin to make our own graphs, we need to learn some terminology for describing the underlying data.\n\n\nSlides: PDF\nTBA: written version"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#the-plot-function",
    "href": "src/book/05_visualization_chapter.html#the-plot-function",
    "title": "5  Visualizing Data",
    "section": "5.3 The plot() function",
    "text": "5.3 The plot() function\nTBD"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#one-variable-histograms-and-bar-charts",
    "href": "src/book/05_visualization_chapter.html#one-variable-histograms-and-bar-charts",
    "title": "5  Visualizing Data",
    "section": "5.4 One variable: histograms and bar charts",
    "text": "5.4 One variable: histograms and bar charts\nTBD"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#two-variables-scatter-plots",
    "href": "src/book/05_visualization_chapter.html#two-variables-scatter-plots",
    "title": "5  Visualizing Data",
    "section": "5.5 Two variables: scatter plots",
    "text": "5.5 Two variables: scatter plots\nTBD"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#trends-line-graphs",
    "href": "src/book/05_visualization_chapter.html#trends-line-graphs",
    "title": "5  Visualizing Data",
    "section": "5.6 Trends: line graphs",
    "text": "5.6 Trends: line graphs\nTBD"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#importance-of-visualization",
    "href": "src/book/05_visualization_chapter.html#importance-of-visualization",
    "title": "5  Visualizing Data",
    "section": "5.7 Importance of visualization",
    "text": "5.7 Importance of visualization\nIt is very important to create visualizations as part of exploratory data analysis. Numbers, including summary statistics, can be hard to interpret and sometimes misleading.\nConsider Anscombe’s quartet, a series of 4 small datasets (each has 11 observations of an x and a y variable).\nHere’s one of the datasets:\n\nanscombe %&gt;%\n  select(x1, y1) %&gt;%\n  head(11)\n\n   x1    y1\n1  10  8.04\n2   8  6.95\n3  13  7.58\n4   9  8.81\n5  11  8.33\n6  14  9.96\n7   6  7.24\n8   4  4.26\n9  12 10.84\n10  7  4.82\n11  5  5.68\n\n\nHere’s another:\n\nanscombe %&gt;%\n  select(x2, y2) %&gt;%\n  head(11)\n\n   x2   y2\n1  10 9.14\n2   8 8.14\n3  13 8.74\n4   9 8.77\n5  11 9.26\n6  14 8.10\n7   6 6.13\n8   4 3.10\n9  12 9.13\n10  7 7.26\n11  5 4.74\n\n\nThe numbers seem pretty similar, right? In fact, the summary statistics are identical:\n\nanscombe %&gt;%\n  select(x1:y4) %&gt;%\n  mutate(index = row_number(x1)) %&gt;%\n  gather(x1:y4, key=\"column\", value=\"value\") %&gt;%\n  group_by(column) %&gt;%\n  summarize(\n    mean = mean(value),\n    standard.deviation = sd(value)\n  )\n\n# A tibble: 8 × 3\n  column  mean standard.deviation\n  &lt;chr&gt;  &lt;dbl&gt;              &lt;dbl&gt;\n1 x1      9                  3.32\n2 x2      9                  3.32\n3 x3      9                  3.32\n4 x4      9                  3.32\n5 y1      7.50               2.03\n6 y2      7.50               2.03\n7 y3      7.5                2.03\n8 y4      7.50               2.03\n\n\nThe mean (center) and standard deviation (variation) is the same for all 4 x’s, and all 4 y’s.\nBut, if we plot the four distributions…\n\nanscombe %&gt;%\n  mutate(index = row_number(x1)) %&gt;%\n  gather(x1:y4, key=\"column\", value=\"value\") %&gt;%\n  separate(column, into = c(\"dimension\", \"dataset\"), sep=c(1)) %&gt;%\n  pivot_wider(names_from=dimension, values_from=value) %&gt;%\n  ggplot() +\n  geom_point(aes(x,y)) +\n  facet_wrap(~dataset, ncol=2)\n\n\n\n\nVisualization allows us to detect:\n\noutliers (points that lie away from the rest)\ncharacterize relationships between variables: are they linear (straight) or non-linear (curved)?\nidentify hypotheses (theories) about the data\nspot problems that might exist in the data\n\n\nTODO exercises"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#improving-your-graphs",
    "href": "src/book/05_visualization_chapter.html#improving-your-graphs",
    "title": "5  Visualizing Data",
    "section": "5.8 Improving your graphs",
    "text": "5.8 Improving your graphs\nTBD"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#wrangling-overview",
    "href": "src/book/06_wrangling_chapter.html#wrangling-overview",
    "title": "6  Wrangling Data",
    "section": "6.1 Wrangling overview",
    "text": "6.1 Wrangling overview\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#the-dplyr-package-and-the-tidverse",
    "href": "src/book/06_wrangling_chapter.html#the-dplyr-package-and-the-tidverse",
    "title": "6  Wrangling Data",
    "section": "6.2 The dplyr package and the tidverse",
    "text": "6.2 The dplyr package and the tidverse\n\nTBA: packages and loading with library()"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#the-presidential-dataset",
    "href": "src/book/06_wrangling_chapter.html#the-presidential-dataset",
    "title": "6  Wrangling Data",
    "section": "6.3 The presidential Dataset",
    "text": "6.3 The presidential Dataset\n\n6.3.1 Examining the data\n\nFor the first part of this chapter we will be using a dataset of US presidents. This dataset is stored in a variable called presidential.\n\n\nUse head() function to check some of the contents of the presidential dataframe. (The head() function prints the first six rows of a dataframe.)\nRun head(presidential) in the R Console and examine the output."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#picking-columns-with-select",
    "href": "src/book/06_wrangling_chapter.html#picking-columns-with-select",
    "title": "6  Wrangling Data",
    "section": "6.4 Picking columns with select()",
    "text": "6.4 Picking columns with select()\n\n6.4.1 The select function\nThe select() function can be used to pick certain columns of a dataset. The output of select() is a new dataframe containing just the columns that you specified.\nIgnore the video’s instruction to follow along in RStudio: you will try this function out on the next page of this tutorial instead.\n\n\nSlides: PDF\n\n\n6.4.2 A simple select()\n\n\nExercise TBM\n\n\n\n\n# Replace the blank with the name of the column\nselect(presidential, _____)\n\n\nselect(presidential, name)\n\n\ngrade_code(\"Nice work!\")\n\n\n\n\n6.4.3 Selecting multiple columns\nWe can put as many column names as we want into the select function, separating each by a comma.\n\n\nExercise TBM\n\n\n\n\n\n\n6.4.4 Selecting a range\nIn R, the range operator, : (the colon punctuation symbol) indicates a range.\nFor example, this code create a vector of all the integer numbers in the range of 1-10:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n(We could also have written c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) as you learned in the first week, but the range operator is a convenient shorthand in this scenario.)\nWe can also use the range operator to indicate a range of columns inside dplyr functions such as select:\nselect(presidential, name:end)\nThis selects all sequential columns from name to end (which in this case is name, start, and end).\n\n\nExercise TBM\n\n\n\n\n\nNote:\nYou combine ranges and individual column selections by separating them by commas, e.g.\nselect(presidential, name:start, party)"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#sorting-with-arrange",
    "href": "src/book/06_wrangling_chapter.html#sorting-with-arrange",
    "title": "6  Wrangling Data",
    "section": "6.5 Sorting with arrange()",
    "text": "6.5 Sorting with arrange()\n\n6.5.1 The arrange function\n\n\nSlides: PDF\n\n\n6.5.2 Arrange practice\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#piping-data-between-functions",
    "href": "src/book/06_wrangling_chapter.html#piping-data-between-functions",
    "title": "6  Wrangling Data",
    "section": "6.6 Piping data between functions",
    "text": "6.6 Piping data between functions\n\n6.6.1 The pipe %&gt;% operator\n\n\nSlides: PDF\nAs described in the video, the pipe operator takes the value on its left and inserts it as the first argument of the function on the right.\nIn other words:\nsome_data %&gt;% someFunction()\nis equivalent to:\nsomeFunction(some_data)\nIf there are other arguments supplied to the function, they get “pushed back” so that the data piped in can claim the “first argument” spot:\nsome_dataframe %&gt;% someFunction(this_is_really_argument_2)\nis the same as:\nsomeFunction(some_data, this_is_really_argument_2)\n\n\n6.6.2 Piping practice\n\n\nExercise TBM\n\n\n\n(Note that we can put a new line after the %&gt;% operator as above - R knows that there must be a right-hand side, so it treats both lines as the same line.)\n\nWhy does this matter?\nThe first argument of functions in the tidyverse is the dataframe. However, many functions also output a dataframe (as does a variable holding a dataframe, such as presidential). So we can just pipe from one function to another and build up a long chain of functions: i.e. a pipe:\nsome_dataframe %&gt;%\n  select(some_columns) %&gt;%\n  some_other_function() %&gt;%\n  a_third_function(different_argument)\n\nNote that I have invented some made-up functions and variable names in the code above - this is called pseudocode."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#boolean-logic",
    "href": "src/book/06_wrangling_chapter.html#boolean-logic",
    "title": "6  Wrangling Data",
    "section": "6.7 Boolean logic",
    "text": "6.7 Boolean logic\n\n6.7.1 Review of Boolean logic\n\n\nSlides: PDF\n\n\n6.7.2 Boolean logic quiz\nYou are already a little familiar with Boolean operators from the Introduction to R tutorial, so let’s refresh our memories with a quick quiz.\n\n\nQuiz TBA"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#picking-rows-with-filter",
    "href": "src/book/06_wrangling_chapter.html#picking-rows-with-filter",
    "title": "6  Wrangling Data",
    "section": "6.8 Picking rows with filter()",
    "text": "6.8 Picking rows with filter()\n\n6.8.1 The filter function\n\n\nSlides: PDF\n\n\n6.8.2 Some practice\n\n\nExercise TBM\n\n\n\n\n\n6.8.3 Filtering with multiple conditions\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#creating-columns-with-mutate",
    "href": "src/book/06_wrangling_chapter.html#creating-columns-with-mutate",
    "title": "6  Wrangling Data",
    "section": "6.9 Creating columns with mutate()",
    "text": "6.9 Creating columns with mutate()\n\n6.9.1 The mutate function\n\n\nSlides: PDF\n\n\n6.9.2 End year\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#grouping-and-summarizing",
    "href": "src/book/06_wrangling_chapter.html#grouping-and-summarizing",
    "title": "6  Wrangling Data",
    "section": "6.10 Grouping and summarizing",
    "text": "6.10 Grouping and summarizing\n\n6.10.1 The group_by and summarize functions\n\n\nSlides: PDF\n\n\n6.10.2 Practice\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#what-is-tidy-data",
    "href": "src/book/06_wrangling_chapter.html#what-is-tidy-data",
    "title": "6  Wrangling Data",
    "section": "6.11 What is tidy data?",
    "text": "6.11 What is tidy data?\nSo far we have looked at relatively simple data wrangling operations that return subsets of the data. However, we often want to reshape the dataframe to turn it into a format called tidy data. This video will introduce you to what tidy data looks like."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#converting-columns-to-rows",
    "href": "src/book/06_wrangling_chapter.html#converting-columns-to-rows",
    "title": "6  Wrangling Data",
    "section": "6.12 Converting columns to rows",
    "text": "6.12 Converting columns to rows\n\n6.12.1 The pivot_longer() function\n\n\n\n\n\n\n6.12.2 Practice\nLet’s try the pivot_longer() function on the presidential dataset. We will reshape this dataset to convert the two data columns (start and end) into rows, with a names column that indicates the name of the original column, and a values column that holds the dates.\nIn other words, we want to convert the presidential dataframe:\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\ninto this:\n\n\n\nname\ntype_of_date\ndate\nparty\n\n\n\n\nEisenhower\nstart\n1953-01-20\nRepublican\n\n\nEisenhower\nend\n1961-01-20\nRepublican\n\n\nKennedy\nstart\n1961-01-20\nDemocratic\n\n\nKennedy\nend\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\nThe data in both dataframes is the same, but we have changed the shape of the dataframe by converting columns into rows.\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#turning-rows-to-columns",
    "href": "src/book/06_wrangling_chapter.html#turning-rows-to-columns",
    "title": "6  Wrangling Data",
    "section": "6.13 Turning rows to columns",
    "text": "6.13 Turning rows to columns\n\n6.13.1 The pivot_wider() function\n\n\n\n\n\n\n6.13.2 Practice\nLet’s use the pivot_wider() function to undo the transformation we did earlier with the pivot_longer() function.\nI.e. we want to turn this:\n\n\n\nname\ntype_of_date\ndate\nparty\n\n\n\n\nEisenhower\nstart\n1953-01-20\nRepublican\n\n\nEisenhower\nend\n1961-01-20\nRepublican\n\n\nKennedy\nstart\n1961-01-20\nDemocratic\n\n\nKennedy\nend\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\nback into this:\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\n\n\nExercise TBM\n\n\n\n\n\n\n6.13.3 What happened to those dates?\nWhen we tried to reverse the transformation, we were not able to retrieve the start and end date columns back in the same format as we originally started with in the presidential dataframe.\nInstead of actual dates in the column, you should see values such as &lt;date [1]&gt;. This indicate that each cell of the table holds a list of dates instead of just a single date.\nYou code will also have generated a warning about this: Warning: Values are not uniquely identified; output will contain list-cols.\nWhat are these non-unique values that we are being warned about? If you look through the table created by pivot_wider(), you will notice that one president’s date list is longer than the others (see if you can find which president this is).\n\nIn fact, there were two US presidents with this same surname during the period of this dataset, and consequently, when we tried to widen the table and identify unique rows, we identified 2 start dates and 2 end dates for this presidential surname.\nTo avoid these problems when using pivot_wider(), we always need at least one column in the remaining non-widened columns that is unique for each row we wish to generate in our output. Here we fail that requirement, because one of the presidential surnames in our dataset is used by two seperate observations (presidents).\nBy default, pivot_wider() assumes that all remaining columns are unique for all widened rows. However, if there are only one or a few unique columns, these can be specified by supplying those column names to the id_cols argument of the pivot_wider() function."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#splitting-and-combining",
    "href": "src/book/06_wrangling_chapter.html#splitting-and-combining",
    "title": "6  Wrangling Data",
    "section": "6.14 Splitting and combining",
    "text": "6.14 Splitting and combining\n\n6.14.1 The separate function\n\n\nSlides: PDF\n\n\n\n6.14.2 The unite function\n\n\nSlides: PDF\nThat’s it for this tutorial!"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#other-data-wrangling-functions",
    "href": "src/book/06_wrangling_chapter.html#other-data-wrangling-functions",
    "title": "6  Wrangling Data",
    "section": "6.15 Other data wrangling functions",
    "text": "6.15 Other data wrangling functions\n\n6.15.1 Other helpful dplyr verbs\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#from-qplot-to-ggplot-...",
    "href": "src/book/07_ggplot_chapter.html#from-qplot-to-ggplot-...",
    "title": "7  Graphs with ggplot",
    "section": "7.1 From qplot() to ggplot() + ...",
    "text": "7.1 From qplot() to ggplot() + ...\nSo far we have created graphs with the qplot() function from the ggplot2 package.\nHowever qplot() is just a quick plotting function that allows us to make simple graphs. To get access to all the visualization features of the ggplot2 package, we will need to learn the proper way to create ggplot graphs.\nFor example, instead of using qplot() to create a histogram:\n\nqplot(x = height, bins = 30, data = starwars)\n\n\n\n\n…we will use an alternative pair of functions, ggplot() and geom_histogram(), like this:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height),\n    bins = 30\n  )\n\n\n\n\nAs you can see, the histograms are exactly the same. However, we’ve avoided this way of creating graphs up until now, because we’ve been trying to keep our code as simple as possible.\nNow we need to make the trade of slightly more complicated code in exchange for much better graphs. So let’s jump in and see how and why this works."
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#graphs-as-layers-and-transformations",
    "href": "src/book/07_ggplot_chapter.html#graphs-as-layers-and-transformations",
    "title": "7  Graphs with ggplot",
    "section": "7.2 Graphs as layers and transformations",
    "text": "7.2 Graphs as layers and transformations\nThe ggplot2 package gets its name because it s based on a concept called the Grammar of Graphics (hence ggplot…). This is a fascinating topic that we unfortunately don’t have the time to dive into, but the key idea is that all data visualizations can be described using a common set of terms and ideas (i.e. a “grammar”).\nOne of the most important parts of this grammar are the layers that we can add to a graph.\nFor example, the ggplot() function by itself just creates a blank canvas:\n\nggplot(data = starwars)\n\n\n\n\nTo actually get something interesting on this graph, we need to take the data and map it to some geometric shape.\nFor example, if we wanted to map the height column of the starwars dataset to the geometric shape of a histograms bars, we could use this code:\ngeom_histogram(mapping = aes(x = height))\nA few things to note:\n\nThe geom_histogram() functions is an example of what we call a geom function (each of which specifies a different type of geometry).\nGeom functions have a parameter called mapping which, unsurprisingly, tells R how to convert a column of data into a pretty layer.\nThe argument that we pass to the mapping parameter is a function called aes() (this is short for “aesthetic”). Inside the aes() function we need to specify any parts of the histogram’s appearance that are determined by a column in the dataset.\n\nIn our example the x-axis of the histogram needs to show the height variable.\n\n\nWe then add this layer to the canvas created by ggplot(). How do we add things together in R? With the addition operator + of course!\nAnd so our final code looks like this:\nggplot(data = starwars) +\n  geom_histogram(mapping = aes(x = height))\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height),\n    bins = 30\n  )"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#aesthetic-mappings-and-other-parameters",
    "href": "src/book/07_ggplot_chapter.html#aesthetic-mappings-and-other-parameters",
    "title": "7  Graphs with ggplot",
    "section": "7.3 Aesthetic mappings and other parameters",
    "text": "7.3 Aesthetic mappings and other parameters\nWe refer to this combination of the mapping parameter and its aes() argument as an aesthetic mapping, which is a fancy way of saying how do figure out how to convert columns of the dataset into some visual representation.\nBut not every part of a graph is determined by data in the dataset. For example, there is no column in the starwars dataset that tells us how many bins this histogram should have. Instead we need to specify that number ourself, with another piece of data (i.e. the number of bins).\nSince this number is not in the dataset, it is not an aesthetic mapping. Therefore we specify the number of bins as an argument of the geom_histogram() function, and not of the aes() function, like so:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height),\n    bins = 30\n  )\n\n\n\n\nDo you see how the geom_histogram() function has two parameters, mapping and bins, whereas the aes() function has a single parameter (x)?"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#scatter-plots-with-ggplot",
    "href": "src/book/07_ggplot_chapter.html#scatter-plots-with-ggplot",
    "title": "7  Graphs with ggplot",
    "section": "7.4 Scatter plots with ggplot()",
    "text": "7.4 Scatter plots with ggplot()\nHopefully you are beginning to see that the ggplot() syntax uses many of the same parameters as qplot(), just in different places.\nWe can create a scatter plot by switching to a geom function called geom_point() and passing an additional column to the aes() function to go on the y axis:\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = aes(x = height, y = mass)\n  )"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#piping-to-ggplot",
    "href": "src/book/07_ggplot_chapter.html#piping-to-ggplot",
    "title": "7  Graphs with ggplot",
    "section": "7.5 Piping to ggplot()",
    "text": "7.5 Piping to ggplot()\nOne advantage of ggplot() over qplot() is that we can pipe a dataframe to ggplot() because the first parameter of ggplot() is the dataset to be used.\nSo instead of explicity passing the starwars dataframe to the data parameter:\nggplot(data = starwars) + ...\n…we could write this:\nstarwars %&gt;%\n  ggplot() + ...\nThis might not seem like much, but it does mean that we can put the graphing functions at the end of a series of piped functions that transform and wrangle our dataset, for example:\nsome_dataset %&gt;%\n  mutate(...) %&gt;%\n  filter(...) %&gt;%\n  ggplot() + geom_FUNCTION(...)"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#labeling-graphs",
    "href": "src/book/07_ggplot_chapter.html#labeling-graphs",
    "title": "7  Graphs with ggplot",
    "section": "7.6 Labeling graphs",
    "text": "7.6 Labeling graphs\nTODO: This was moved from EDA and needs updating to be consistent with chapter\nIt is good practice to label all the graphs we create. We can do this by adding the labs() function to a graph:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_violin(mapping = aes(x = \"price\", y = price)) +\n  labs(title = \"Violin plot of price\", y = \"price ($)\")\n\n\n\n\nNotice how:\n\nthe labs() function is a separate function that we add on to the graph as a separate layer with the + operator\ninside the labs() function, we can supply an argument to the title parameter to change the title, and the y (and x) arguments to change the labels on those axes.\n\n\nTODO: book exercises\n\n\n\nUsing the labs() function, add a title and y-axis label to the boxplot of price that you created earlier (we will leave the x-axis label as its default, since that is adequate for this graph.)"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#what-is-eda",
    "href": "src/book/08_eda_chapter.html#what-is-eda",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.1 What is EDA?",
    "text": "8.1 What is EDA?\n\n\n8.1.1 Exploratory Data Analysis (EDA)\nExploratory data analysis, or EDA for short, is a vague, hard-to-define concept… but is also the activity that will occupy much of your time as a data scientist.\nHere’s the problem:\n\nWhen you first get a dataset, you don’t know very much about it. You probably choose the dataset because you thought it might help you solve a particular problem, but you don’t know how (or if) the data can help you do that, or what other nuances might be present in the dataset.\nSo, the first step is to do a little digging: open the data up, and:\n\nPlot a few basic graphs (EDA is mostly done with graphs because they make it very easy to spot patterns).\nCalculate some simple statistics (e.g. averages and ranges)\n\nThis basic analysis will probably give you some ideas about the data. You’ll probably spot some patterns that bear further investigation, or maybe some issues that you’ll need to address.\nHow do you do this? More analysis, visualization, and general experimentation, which in turn will reveal more potential patterns, and so the cycle continues…\n\nYour end goal is to find some interesting questions in the dataset that deserve some kind of follow-up.\nDoes this seem all seem vague and open-ended? That’s because it is fundamentally a creative activity. There’s no one “right way” to create a painting or a book. Exploratory data analysis is the same. And just like those other creative activities, the way to get better is to practice.\n\n\n8.1.2 Types of variation to explore\nHopefully you are now convinced why graphing data is important. But what type of graphs should you create?\nThe rest of this tutorial will go through different types of graphs, but here are some general concepts to bear in mind:\n\nWe can broadly divide EDA into two types: univariate and multivariate\n\nUnivariate, or “one variable”, analysis looks at the variation within a single variable’s values. Our goal here is to understand the distribution of this variable.\nMultivariate (i.e. “many variables”) refers to analysis of 2 or more variables where we want to examine the covariance between variables. In other words, how does one variable change in response to changes in another.\n\nAs a general principle, you should do univariate EDA first before multivariate EDA.\nWe have put together a flow chart to help you pick the right type of graph for particular types of data and analysis goals: available as a PDF here."
  },
  {
    "objectID": "src/book/08_eda_chapter.html#starting-eda",
    "href": "src/book/08_eda_chapter.html#starting-eda",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.2 Starting EDA",
    "text": "8.2 Starting EDA\n\n8.2.1 A “first steps” EDA checklist\nHere’s a checklist of things to check when you begin EDA on a dataset.\n\nFormulate your question\nLoad the data and check the dataset’s documentation\nGet an overview of the data\nLook at the first and last rows of your dataframe\nMake sure the data makes sense\nStart the EDA cycle.\n\nBased on: Exploratory Data Analysis, Chapter 4, a free online book by Roger Peng, a professor of biostatistics at Johns Hopkins University.\n\n\n8.2.2 Formulate your question\nIt’s generally a good idea to go into EDA with a question in mind.\nThe question doesn’t have to be perfect - indeed, you might well decide that the question can be improved after you’ve explored the data.\nHowever, having a question in mind allows us to focus our EDA and not get distracted by the huge number of possibilities that we can explore with a single dataset.\nSo: what makes a good question?\nIn general, we want:\n\nto be as specific as possible,\nwhile still being interesting.\n\nSpecificity is good because it makes our analysis shorter and more direct. Broad questions are often of more general interest, but it is harder to come up with an approach to answering them.\nAn analogy is a New Year’s resolution: consider “Exercise more” vs. “Go for a 30 minute run 3x per week.” One of these is specific and measureable (while still be relevant to the overall goal of increasing your fitness), while the other is much broader and less-defined (and potentially easier to weasel out of…).\nFor this tutorial, our starting question will be:\n\nWhat is the effect of the size of a diamond on its price?\n\nThis is a better starting place that a broader question such as “What factors influence diamond prices?” Where would we start with such a question? Not only would we want data on the physical characteristics of diamonds, but probably also on the supply of diamonds, the controls imposed by diamond cartels, the effect of advertizing to make diamonds seem exclusive, popular campaigns against blood diamonds… we could spend all day just thinking about the question, and never get to the actual EDA!\n\n\n8.2.3 Load the data and read the documentation\nFor this tutorial, we will be using the diamonds dataset, which is automatically loaded with the ggplot2 graphing package.\nThis means that you just need to run library(ggplot2) or library(tidyverse) (the tidyverse meta-package includes ggplot2), and the diamonds dataset will be available for you to use.\nIn the real world, loading data tends to be a much messier task (a topic for another day).\n\n8.2.3.1 Check the dataset’s documentation\nMost datasets come with documentation describing the data that they contain.\nIn R, most datasets that come as part of a package have some kind of documentation page. In RStudio, you can bring this up by running ?name_of_dataset in the console, e.g. ?diamonds. That won’t work in this tutorial, but you can also see the diamonds documentation page online here: https://ggplot2.tidyverse.org/reference/diamonds.html\n\n\n\n8.2.4 Get an overview of the data\nWe can use the glimpse function to get a compact overview of a dataframe.\nNote: the glimpse function is a modern “tidyverse” version of an older R function called str.\n\n\n\n\nRun the glimpse function to get an overview of the columns in the diamonds dataset.\n\n\n\n\n8.2.5 Look at the first and last rows of your dataframe\nSometimes dataset creators start with grand intentions, but motivation or reality gets in the way. Because of this, the data at the top of datasets often looks great. However, the last rows in the dataset may contain incomplete or problematic data that got added on at the end.\nWe can examine both the first and last rows of the dataset with the head and tail functions respectively. The syntax of these functions is head(dataset_name), e.g.:\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nto display 6 rows from the top (or bottom) of the dataset. We can change the number of rows to display with the second argument:\n\nhead(diamonds, 10)\n\n# A tibble: 10 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n\n\nWe could also rewrite the previous code chunk using the pipe operator:\n\ndiamonds %&gt;%\n  head(10)\n\n\nTODO: book exercises\n\n\n\nUse the tail function to display the last 8 rows of the diamonds dataframe. Note that this code uses the pipe operator %&gt;% to supply the first.\n\n\nAs you can see, the diamonds data is actually pretty nice at both the top and the bottom (as you might hope for an “offical” ggplot dataset). The example datasets that come with packages are often nice like this - unfortunately real-world data is usually less friendly.\n\n\n8.2.6 Make sure that the data makes sense\nWe expect certain things about our dataset to be true:\n\nDoes the dataset have the correct number of rows and columns?\nDo the numbers in each column make sense:\n\ndoes a column called date contain dates, or does it have random numbers that don’t seem to be dates?\ndoes a column called USA_states contain all 50 states as you might expect? Or does it also contain “non-state” regions such as the District of Columbia and Puerto Rico.\n\nWhat is an observation in the dataset?\n\nIn the diamonds dataset, each row represents a different diamond. This is probably what you would expect. But what if the dataset rows each represented a diamond store instead, and the numbers were the average of all the diamonds in that shop? In this scenario, the data (and what questions we can ask with it) would be very different.\n\nAre the values plausible?\n\nFor example, in the diamonds dataset, are the prices for the diamonds realistic? Is the range of values of the carat variable realistic (for example, most diamonds are pretty small, so if this dataset claimed to have data on 50,000 diamonds that were all 100 carats or larger, then we might be a little suspicious…)\n\n\n\n\n8.2.7 Start the EDA cycle\nNow that you’ve verified the data, you can start on the EDA process.\nAs a reminder, we usually want to start with univariate analysis of each variable separately. Programs and data are both complicated, so it is always a good idea to start small and simple, and add in complexity gradually.\nIf we do the opposite and start complicated, it is easy to make small mistakes and assumptions which might make our final conclusions wrong!"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#univariate-variation",
    "href": "src/book/08_eda_chapter.html#univariate-variation",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.3 Univariate variation",
    "text": "8.3 Univariate variation\n\n8.3.1 Non-graphical methods (i.e. summary statistics)\n\nIf a variable is categorical, we can summarize it by looking at the proportions of observations in different categories:\ndataframe %&gt;%\n  group_by(categorical_variable) %&gt;%\n  summarize(\n    count = n(),\n    proportion = n() / nrow(.),\n    percentage = 100 * proportion\n  )\nNotes on this code:\n\nthe n() function returns the number of rows. It does not need an argument.\nthe nrow() function also calculates the number of rows, but needs an argument (a variable who’s rows it should count).\nthe . argument inside nrow is used to refer to the original dataframe that we piped in.\n\n\nTODO: book exercises\n\n\n\nCalculate summary statistics for the categorical cut column of the diamonds dataframe.\n\n\nFor a continuous variable, we typically use a few statistics (a number representative of other numbers) to summarize the:\n\ncenter (i.e. mean, median, mode), &\nspread (e.g. the standard deviation, the range, the interquartile range, etc.)\n\nof a distribution.\ndataframe %&gt;%\n  summarize(\n    mean    = mean(continuous_variable),\n    median  = median(continuous_variable),\n    std.dev = sd(continuous_variable),\n    iqr     = IQR(continuous_variable),\n    min     = min(continuous_variable),\n    max     = max(continuous_variable)\n  )\n\nTODO: book exercises\n\n\n\nCalculate summary statistics the 6 summary statistics listed above for the continuous variable carat from the diamonds dataframe.\n\n\nIn our original question, we were interested in two variables: size (i.e. carat) and price. We should therefore calculate summary statistics for the price variable as well\n\nTODO: book exercises\n\n\n\nRepeat your summary statistics calculation for the continuous variable price.\n\n\n\n\n8.3.2 Graphical analysis of carat\nWith graphs, we want to visualize the distribution of a variable.\nTwo good graphs for visualizing the distributions of one continuous variable are histograms and boxplots. (You might also want to look up density plots and violin plots on the “What graph should I plot” cheatsheet.)\n\nTODO: book exercises\n\n\n\nCreate a histogram of the carat variable.\n\n\nPlots of distributions show common versus less common values of a variable. This allows us to answer questions like:\n\nWhat values are common? Does this match your prior expectations?\nWhat values are uncommon? Again, does this match your prior expectation?\nDoes the overall appearance show any unusual patterns that bear further investigation?\n\nOne thing you might note is that the distribution is not very smooth. To make it smoother, we could increase the size of the bins. However, to see what’s going on with this distribution, let’s decrease the bin size.\n\nTODO: book exercises\n\n\n\nCreate another histogram of the carat variable, but this time set the binwidth parameter to 0.01\n\n\nInteresting! There’s an odd sawtooth pattern to this data that suggests several clusters to the data. This pattern raises several interesting questions:\n\nWhat’s causing these groups (“clusters”)?\nWhy does each cluster have a sharp left-hand side, and a long tail on the right?\n\n\n\n8.3.3 price visualization\nWe also want to look a the distribution of the price variable. Let’s do that with a new type of graph: the box plot.\nBox plots (also called box-and-whisker plots) visualize not only the distribution but also show several summary statistics:\nTODO: replace with my own image that I know I own copyright of.\n\nknitr::include_graphics(\"../img/boxplot.png\")\n\n\n\n\nWe create a box plot using the geom_boxplot geom function. There are two required aesthetic mappings in a box plot (note that in ggplot the orientation of the boxplot is rotated 90 degrees):\n\nx should be a categorical variable to create different box plots for.\ny is the variable that we want to show the distribution of.\n\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_boxplot(mapping = aes(x = cut, y = price))\n\n\n\n\nYou might have spotted a problem here: what if we only want to create one box plot rather that breaking down the variable into a different box plot for each category of x? In this case, we need to provide a character string to x instead of a variable name - however, despite the fact that x is a fixed value, it still goes inside the aes function…\n\nTODO: book exercises\n\n\n\nCreate a single box plot of the price variable by providing the string \"price\" as the argument to the x parameter.\n\n\n\n\n8.3.4 Violin plots\nAn alternative to the boxplot is the violin plot. This combines aspects of the histogram and boxplot into a single graph.\nThe code for a violin plot is almost identical to the code or a boxplot - we just use the geom_violin() geom function:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_violin(mapping = aes(x = \"price\", y = price))\n\n\n\n\nTODO: Include a comparison of histogram, boxplot, and violins (all in the same plot, aligned for cmparison)."
  },
  {
    "objectID": "src/book/08_eda_chapter.html#multivariate-covariation",
    "href": "src/book/08_eda_chapter.html#multivariate-covariation",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.4 Multivariate covariation",
    "text": "8.4 Multivariate covariation\nWhen we have multiple variables we want to understand how their variation is related: this is called covariation.\n\n\n8.4.1 Co-variation with summary statistics\nTODO\n\n\n8.4.2 Graphical\nIf you have two continuous variables, such as price and carat in the diamonds dataset, then a scatter plot is generally a good method of examining their covariation:\n\nTODO: book exercises\n\n\n\nCreate a scatter plot with price on the y-axis and carat on the x-axis.\n\n\nWe can see several things about this graph:\n\nThere’s obviously a positive relationship between these two variables.\nIt’s hard to tell the density of the points (i.e. where most of the points fall), because in many places all the point overlap in a blog.\n\nWe have a several options to show density.\n\nWe could make the points transparent with the alpha parameter. Then dense regions would be darker (because there would be many overlapping transparent points). For example, with alpha = 0.1:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = carat, y = price), alpha=0.1) +\n  labs(title = \"Scatter plot of dimond price vs size\", y = \"price ($)\", x = \"size (carats)\")\n\n\n\n\nWe can also create a heat map, which is a colored grid where the intensity of the color represents a value on a scale - in this case, the color represents the density. This uses the geom_bin2d geom function:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_bin2d(mapping = aes(x = carat, y = price)) +\n  labs(title = \"Heat map of dimond price vs size\", y = \"price ($)\", x = \"size (carats)\")\n\n\n\n\nThis is essentially a 2-dimensional histogram. Each square is like a bar in a univariate histogram. In this heat map, the color represents the “height” of the 2-dimensional bin, i.e. how many diamonds fall into each cell of the heat map’s grid.\n\n\n\n\n8.4.3 Coloring and faceting by categorical variables\nTODO"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#your-turn",
    "href": "src/book/08_eda_chapter.html#your-turn",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.5 Your turn",
    "text": "8.5 Your turn\n\n8.5.1 Do some EDA\nThere are all kinds of other graphs we can plot.\nIn the code cells below, experiment on your own with some exploratory data analysis of the diamonds dataset.\nFrom our previous explorations, it there is a positive relationship between price and carat; in other words, as one variable increases, so does the other. However:\n\nthe relationship was not perfect: there seemed to be some uneven scatter. What other variables might be influencing this relationship?\nrelatedly, what other variables are related to price (& what is the variation within those other variables by themselves)?\n\n\nTODO: book exercises\n\n\nIn the following code boxes, play around with some EDA of your own. You might find it helpful to look at the “What graph should I plot” cheatsheet for inspiration, and remember that you can always Google a geom function (or ask an instructor) to get more information on how to use it."
  },
  {
    "objectID": "src/book/09_modeling_chapter.html#what-is-a-model",
    "href": "src/book/09_modeling_chapter.html#what-is-a-model",
    "title": "9  Modeling",
    "section": "9.1 What is a model",
    "text": "9.1 What is a model\n\n9.1.1 Video introduction\nThis video will introduce you to the concept of models, and in particular the linear model (which you may already be familiar with as a line of best fit).\n\n\nSlides: PDF\n\n\n9.1.2 “Correlation does not imply causation”\nA common saying when working with models is that “correlation does not imply causation”. What does this mean?\nWhen we create a linear model (e.g. a line of best fit), we have 2 variables:\n\n\\(y\\), the response variable\n\\(x\\), the explanatory variable\n\nWe are implicitly saying that the values of \\(x\\) explain the values of \\(y\\), or alternatively that the \\(y\\) variable responds to changes in \\(x\\). This language implies that \\(x\\) is directly causing \\(y\\), and not the other way around.\nHowever…\nThe linear model cannot actually tell us which variable is causing changes in the other (causation). Just because we decide to call one variable \\(y\\) and the other \\(x\\), does not mean that is the direction of effect in the real world.\nFor example, taller people generally weigh more, but there is nothing stopping us from making height the response variable (y), even though most people would say that it makes no sense that your weight causes your height.\nIn fact, none of the variables in your model could directly cause a change in the other. For example, there might be a third variable (that’s not in your model) that actually causes the change in your model.\nTherefore, even if there seems to be a straight line relationship between two variables, we cannot definitively say that once causes the change in the other - only that they are correlated."
  },
  {
    "objectID": "src/book/09_modeling_chapter.html#creating-models-in-r",
    "href": "src/book/09_modeling_chapter.html#creating-models-in-r",
    "title": "9  Modeling",
    "section": "9.2 Creating models in R",
    "text": "9.2 Creating models in R\n\n9.2.1 Video introduction\n\n\nSlides: PDF (same as before)\n\n\n9.2.2 Let’s create a linear model\nLet’s create your first linear model in R, using a simulated dataset called sim_df.\nThis is what the first few rows of the dataset look like:\n\nsim_df %&gt;% head()\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1  1.69  4.68\n2  1.72  5.72\n3  3.18  4.97\n4  4.32  8.35\n5  5.20 10.3 \n6  5.95 12.5 \n\n\nAs you can see, there are two variables, \\(x\\) and \\(y\\). Here’s what they look like:\n\nsim_df %&gt;%\n  ggplot() +\n  geom_point(aes(x,y))\n\n\n\n\nThis looks like a pretty good candidate for a linear model, so let’s create one.\nWe’ll do that with the lm function. The general syntax of this function is:\nlm(response ~ explanatory, data = _______)\n\nresponse and explanatory are the column names of those two variables.\nThe data parameter expects the dataframe containing those columns.\n\n\nWhat’s going on with response ~ explanatory? This is what’s know as an R formula.\nThe mathematical forumla for a linear model is \\(y=mx+c\\). In R, we would write this as y ~ x: i.e. we replace \\(=\\) with ~, and we only include the actual variables. From this, R infers that we want to estimate the model’s coefficients (the slope, \\(m\\), and the intercept, \\(c\\)).\n\n\nFill in the blanks in the lm function with the two variables, y and x, and the dataframe sim_df.\n\n\nsim_model &lt;- lm(_____ ~ _______, data = _____)\n\n\n# hint text\n\"Make sure to get the variables in the right order!\"\n\n\nsim_model &lt;- lm(y ~ x, data = sim_df)\n\n\ngrade_code()\n\n\nNote: Unfortunately the lm function is a part of the core R programming language that predates the tidyverse packages. Unlike the tidyverse functions, the first argument of lm is the formula, not the dataframe. Therefore, you cannot pipe a dataframe to lm, because pipe inserts the dataframe into the first argument position (which will cause an error)."
  },
  {
    "objectID": "src/book/09_modeling_chapter.html#fitting-models",
    "href": "src/book/09_modeling_chapter.html#fitting-models",
    "title": "9  Modeling",
    "section": "9.3 Fitting models",
    "text": "9.3 Fitting models\n\n9.3.1 Video introduction\n\n\nSlides: PDF (same as before)\n\n\n9.3.2 Your model’s coefficients\nWe usually want to report the slope and intercept of a linear model. The slope is especially useful to report, because it indicates how much \\(y\\) changes as \\(x\\) changes.\nWe can report the model coefficients with the tidy() function from the broom package. The syntax is like this:\nyour_model %&gt;%\n  tidy()\n\nReplace the blank with the correct function to report the coefficients.\n\n\nsim_model %&gt;%\n  _______()\n\n\nsim_model %&gt;%\n  tidy()\n\n\n# check code\ngradethis::grade_code()\n\n\nquestion(\n  \"What is the slope of the linear model?\",\n  answer(\"1.27\"),\n  answer(\"1.75\", correct = TRUE),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"What is the intercept of the linear model?\",\n  answer(\"1.27\", correct = TRUE),\n  answer(\"1.75\"),\n  allow_retry = TRUE\n)\n\n\n\n9.3.3 \\(R^2\\): how good is your model?\nSo we’ve reported the slope and intercept: however, these don’t tell us about how well the model fits the data, i.e. how close the points are to the model’s line.\nFor example, you could have a steep slope (suggesting that a change in \\(x\\) is correlated with a big change in \\(y\\)), but the points could be scattered a long way from the line (i.e. large residuals). For example, consider these two graphs:\n\nset.seed(42)\nx1 &lt;- 1:10 + rnorm(10, 0, 0.5) \nx2 &lt;- 1:10 + rnorm(10, 0, 0.5)\nx &lt;- c(x1, x2)\ny &lt;- c(x1 + rnorm(10, 0, 3), 2*x2 + rnorm(10, 0, 0.5))\nseries &lt;- c(rep(\"Low slope, large scatter\", 10), rep(\"Steep slope, low scatter\", 10))\ntibble(x, y, series) %&gt;%\n  ggplot(aes(x,y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE) +\n  facet_wrap(~ series)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can use a statistic called \\(R^2\\) (R-squared). You can think of \\(R^2\\) as the % of the variation in \\(y\\) that is explained by the explanatory variable, \\(x\\).\n\\(R^2\\) varies between 0 and 1. 1 means that the model explains all the variation: i.e. the points fall exactly on the line of best fit! As the points get further from the line, \\(R^2\\) will decrease.\nWe can report the \\(R^2\\) by piping a model to the glance() function.\nyour_model %&gt;%\n  glance()\n\nFill in the blank with the correct function to report the \\(R^2\\) value of your model.\n\n\nsim_model %&gt;%\n  _______()\n\n\nsim_model %&gt;%\n  glance()\n\n\n# check code\ngradethis::grade_code()\n\nYou’ll note that glance reports a whole bunch of statistics. We only care about the first column, r.squared, and can ignore the rest.\n\nquestion(\n  \"What is the R-squared value of your model?\",\n  answer(\"0.935\", correct=TRUE),\n  answer(\"0.926\"),\n  answer(\"1.52\"),\n  answer(\"114\"),\n  allow_retry = TRUE\n)\n\n\n\n9.3.4 What’s a good \\(R^2\\)?\nWe can use the \\(R^2\\) to determine how “good” a model is. However, what value of \\(R^2\\) is “good”?\nIt turns out that the answer to this question varies widely between different areas of science.\nIn physics, we often collect data from simple systems where data can be collected very accurately, so physicists expect high \\(R^2\\) values in their experiments. For example, if you are measuring the speed of an object, there will be only one or two variables that will affect the speed, so you should get a very good correlation. Physical variable are generally easy to predict.\nIn contrast, the social sciences (e.g. psychology or economics) will accept lower \\(R^2\\) values, because there social systems are very complex with a huge number of interacting variables. For example, you could measure the effect of a good night’s sleep on students’ exam results the next day, but there are going to ge a lot of other things that affect how well a student does (such as how much they studied). As a result, even if sleep and exam results are related, the correlation will be very weak. Psychologists may be quite happy with \\(R^2\\) values of less than 0.5 (i.e. 50%).\nFor the purposes of this class, you can think of an \\(R^2\\) &gt; 0.5 as good, an \\(R^2\\) &gt; 0.25 as okay, and an \\(R^2\\) &gt; 0.1 as weak."
  },
  {
    "objectID": "src/book/09_modeling_chapter.html#model-assumptions",
    "href": "src/book/09_modeling_chapter.html#model-assumptions",
    "title": "9  Modeling",
    "section": "9.4 Model assumptions",
    "text": "9.4 Model assumptions\n\n9.4.1 Assumptions of the linear model\nAll models make assumptions - that’s how they are able to simplify complexity.\nThe linear model makes these assumptions:\n\nLinearity: we assume that there is a linear relationship between the response and explanatory variables (i.e. that they fall more or less along a straight line).\nNearly normal residuals: the residuals are normally distributed around the model line.\nConstant variation of residuals: the variation in residual size (above and below the model line) is similar in all parts of the model.\nIndependent observations: Each observation (i.e. each \\((x,y)\\) point) was generated independently from the others.\n\nThese might seem a little complicated, but they all make some intuitive sense when you think about what they mean, so let’s go through them one-by-one.\n\n\n9.4.2 Linearity\nFor a simple regression with one explanatory variable, we can see whether there is a linear relationship by plotting a scatter plot with the model line. For example, compare these two plots:\n\nset.seed(42)\nx1 &lt;- 1:20 + rnorm(20, 0, 0.5) \nx2 &lt;- 1:20 + rnorm(20, 0, 0.5)\nx &lt;- c(x1, x2)\ny &lt;- c(400*x1 + rnorm(20, 0, 100), x2^3 + rnorm(20, 0, 100))\nseries &lt;- c(rep(\"Linearity condition satisfied\", 20), rep(\"Linearity condition violated\", 20))\ntibble(x, y, series) %&gt;%\n  ggplot(aes(x,y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE) +\n  facet_wrap(~ series)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn the left-hand graph the points obviously fall on a straight line (a linear trend). However, in the right-hand side plot, there is an obvious non-linear trend, and that data violates this assumption of the linear model.\n\n\n\n\n9.4.3 Nearly normal residuals\nThe best linear model is the one that has the smallest sum of the squared residuals. We don’t need to worry too much about the mathematics behind this - however, you should be aware that the math does make the simplifying assumption that the residuals have a normal distribution (more or less).\nThe normal distribution is a bell-shaped distribution:\nTODO: add image of normal distribution\n\n\n\nWe can plot a histogram of the residuals and see if that has a (more or less) normal distribution:\nTODO: add image of histogram of normal distribution\n\n\n\nTo do that, we will need to:\n\nCalculate a new column in our dataframe holding the residuals.\nPlot the residuals.\n\nThis starter code calculates the residuals using your model and a useful function called add_residuals. The new dataframe (with an additional column called resid to hold the residuals), has been saved in a new variable called sim_model_df.\n\nUse this new sim_model_df dataframe to create histogram of the resid column (fill in the correct geom function and the resid column name in the appropriate blanks).\n\n\nsim_model_df &lt;- sim_df %&gt;%\n  add_residuals(sim_model)\n\nsim_model_df %&gt;%\n  ggplot() +\n  _______(mapping = aes(x = ________), bins = 5)\n\n\nsim_model_df &lt;- sim_df %&gt;%\n  add_residuals(sim_model)\n\nsim_model_df %&gt;%\n  ggplot() +\n  geom_histogram(mapping = aes(x = resid), bins = 5)\n\n\n# check code\ngradethis::grade_code()\n\nAs you can see, our histogram does not look like a perfectly normal distribution - however, the randomly simulated sim_df dataframe was created with normal residuals!\nThis is the challenge of working with random data: it will rarely look “perfect”. This is why we say that we only expect nearly normal residuals to satisfy this condition.\nIn your graph, there does appear to be a roughly normal distribution (of course, it’s easy to say that when we know it should be!):\n\nThere’s a single higher peak in the middle.\nThe heights of the bins decrease on either side.\nThere seem to be a similar number of residuals on either side of the central peak (i.e. the distribution is symmetric).\nAlthough the left-most bin is higher than we might expect, this can be explained as just a quirk of the data (remember that all data is a little bit random), especially as this dataset is small (such random variation tends to get “averaged out” as the number of data points increases).\n\n\n\n9.4.4 Constant variability of residuals\nThe residuals of a linear model are arranged above and below the line. As we saw in the previous section, this arrangement should look similar to a normal distribution, i.e.\n\nmost residuals will be small (close to the line)\na few residuals will be large\n\nThe linear model also assumes that this normal distribution is the same in all parts of the model (i.e. at both high and low value of \\(x\\)).\nFor example, in this model, we can clearly see that the variation in the residuals is not constant - this is a violation of this third assumption of the linear model:\n\nset.seed(46)\nx &lt;- 1:100 + rnorm(100)\ny &lt;- x + 100:1 * rnorm(100)\nggplot(mapping = aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Decreasing variation in points\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn the graph above, the points on the right lie much closer to the line: i.e. the residuals will be smaller.\nFor simple linear models like this (with only a single explanatory variable \\(x\\)), we can get a reasonable idea of the variability of the residuals just by looking at a scatter plot of the model, as above.\nFor comparison, here is the model you fit earlier:\n\nsim_df %&gt;%\n  ggplot(mapping = aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nquestion(\n  \"Does your model (as shown above) satisfy the linear model's assumption of constant variability of residuals?\",\n  answer(\"Yes, the model satisfies this assumption.\", correct = TRUE),\n  answer(\"No, the model violates this condition.\"),\n  allow_retry = TRUE\n)\n\n\n\n9.4.5 Independent observations\nThe fourth (and final) assumption of the linear model is that all the observations (data points) were generated independently.\nWe will not worry to much about this assumption in this course, as the datasets we give you will have independent observations (for the most part).\nBut what does it mean for observations to be non-independent?\nConsider the stock market: we could have a dataset that contains measurements of a company’s stock price at the end of each day. However these observations are not independent. A stock price will rise or fall, but this change will be from the previous observation. I.e. if the stock ends day 1 at 100, it will probably end day 2 pretty close to 100. Therefore, the data points are not independent.\nFor comparison, think about measuring the height of all the students in this class: one student’s height does not significantly affect an other’s, so these observations are all independent."
  },
  {
    "objectID": "src/book/10_inference_chapter.html#what-is-inference",
    "href": "src/book/10_inference_chapter.html#what-is-inference",
    "title": "10  Inference",
    "section": "10.1 What is inference?",
    "text": "10.1 What is inference?\n\nStatistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.\nSource: Wikipedia\n\nWhat does the above definition mean in simple terms?\nThe world around us is an uncertain, random place. Take the heights of the students in this class: you all have a wide range of heights - some taller, some shorter.\nWe could plot a histogram of your heights - we would call this graph a distribution of your heights, and we could infer various things about it, e.g. the mean.\nThis process of going from raw measurements (e.g. of heights) to properties of the distribution that those measurements came from is called statistical inference."
  },
  {
    "objectID": "src/book/10_inference_chapter.html#distributions",
    "href": "src/book/10_inference_chapter.html#distributions",
    "title": "10  Inference",
    "section": "10.2 Distributions",
    "text": "10.2 Distributions\n\n10.2.1 Data sampling\n\n\nSlides: PDF\n\n\n\n10.2.2 Quantifying data distributions\n\n\nSlides: PDF\n\n\nSlides: PDF\nFill in the ellipses (...) to create a boxplot of the Sepal.Length variable from the iris dataset.\n\n\n10.2.3 Probability mass functions\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/10_inference_chapter.html#hypothesis-testing",
    "href": "src/book/10_inference_chapter.html#hypothesis-testing",
    "title": "10  Inference",
    "section": "10.3 Hypothesis testing",
    "text": "10.3 Hypothesis testing\n\n10.3.1 Gender discrimination case study\n\n\nSlides: PDF\n\n\n10.3.2 A hypothesis test as a court trial\n\n\nSlides: PDF\n\n\n10.3.3 A manual simulation of the gender discrimination experiment\n\n\nSlides: PDF\n\n\n10.3.4 Simulating the gender discrimination experiment in R\n\n\nSlides: PDF\n\n\n10.3.5 One-sided hypothesis tests using infer\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#welcome",
    "href": "src/book/11_more_r_chapter.html#welcome",
    "title": "11  More about R",
    "section": "11.1 Welcome",
    "text": "11.1 Welcome\nThis chapter/tutorial will start with a refresher on basic programming in R (from Module 1: variables, data types, functions), and then introduce some additional programming concepts such as control flow and loops."
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#values-and-variables",
    "href": "src/book/11_more_r_chapter.html#values-and-variables",
    "title": "11  More about R",
    "section": "11.2 Values and variables",
    "text": "11.2 Values and variables\n\n11.2.1 A recap\nBy now you are familiar with the idea that in R we have values and variables. We can write something like:\nx &lt;- 2\nto store the value 2 in the variable x. We do so using the assignment operator, &lt;-.\nThis allows us to use a variable such as x in future lines of code. When that future code is run, the variable will be replaced with its value, e.g.\nx + 3\nbecomes\n2 + 3\n\n\n11.2.2 Expressions and statements\nAs we learn more about programming, it’s useful to distinguish between different types of code.\nIn programming, an expression is a piece of code that returns a value. For example:\n\n2 + 2 returns the value 4\n1 == 2 returns the value FALSE\n\nIf you run a line of R code that contains just an expression by itself, the expression will be evaluated, and the result will by displayed in the RStudio Console.\nHowever, the result of an expression is not saved by default. Instead it is just calculated and then discarded by the program. Otherwise, if you kept bits of data you don’t need, your computer would very quickly run out of memory!\nTherefore we almost never want an expression by itself - instead we want to do something with it, such as assigning it to a variable. Later in this tutorial we will learn of other things we can do with the results of expressions.\nA statement is a line of code that runs but does not return a value. If nothing is printed out in the Console when you run a line of code, then that line is a statement.\n\n\n11.2.3 How an R program runs\nWhat happens when you run an R program (or a code chunk in an RMarkdown document)? And why does this mean that 2 + by itself is not a valid expression?\nBut why will this code cause an error:\n2 +\nwhen this code will run successfully?\n2 +\n2\nWhen you run an R program, your code is sent, line by line to another program called the R interpreter (this is what is running in the RStudio Console). The interpreter converts your code into electrical signals that can be understood by your computer, and then takes the computer’s output and turns it back into a readable response that it displays on the screen.\nWe do not need to worry about how this happens, but the important thing to note is that this happens one line at a time. So this is why 2 + doesn’t work: it gets to the end of the line and has nothing to add to the first number.\nHowever… if a line ends in an incomplete expression, then before giving an error, R will first look to see if the next line could be the continuation of the first. This is why this code chunk works, even though the expression has been broken over multiple lines:\n2 +\n2\nAs programmers, we want to format our code to be read by humans not computers (R only cares if your code is correct, not it is easy to read). It sometimes helps readability to break long lines into multiple lines. For example, compare:\nsome_dataframe %&gt;% filter(col1==2) %&gt;% select(col2,col3) %&gt;% ggplot()+geom_point(mapping=aes(x=col3,y=col2))\nwith the same code broken over several lines:\nsome_dataframe %&gt;%\n  filter(col1 == 2) %&gt;% \n  select(col2, col3) %&gt;% \n  ggplot() + \n  geom_point(\n    mapping = aes(\n        x = col3,\n        y = col2\n      )\n    )\nAlso note that indentation aids readability by indicating what goes together:\n\nWhen we break an expression over multiple lines, you should indent every subsequent line of that expression by 2 spaces to indicate to a reader that it is part of the same expression.\nIn addition, if you insert a line break between a set of parentheses (...), you should also indent the contents of those parentheses by 2 spaces (as we have done with both the geom_point() and aes() functions above).\n\nLong lines of code in an RMarkdown code chunk will also overrun the right margin of a PDF after you knit, as lines in a code chunk are not automatically wrapped onto the next line, unlike regular text. Therefore you may have to break up a long line simply to fit it on the page.\nWhat about this code?\n2 + \n  y &lt;- TRUE\nWhat about this code chunk?\nsome_dataset %&gt;%\n  filter(col_A == \"some_value\")\n  mutate(\n    new_column = col_B * 100\n  )"
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#data-types",
    "href": "src/book/11_more_r_chapter.html#data-types",
    "title": "11  More about R",
    "section": "11.3 Data types",
    "text": "11.3 Data types\n\nIn the first interactive tutorial, An Introduction to Programming in R, you learned about basic data types such as numbers, character strings, and Boolean values.\nYou also learned about more complex data structures such as vectors and lists which can hold multiple values of those basic data types.\nIf you do not remember this, go back to the first interactive tutorial to refresh your memory.\nWe also have data structures that can hold a 2-dimensional table of data (with rows and columns): the dataframe.\nHowever, the dataframe has been in R since the language was first created, and as a result it has some odd behaviours that are counterintuitive and can lead to bugs.\nThe tidyverse collection of packages that we have been using add a new version of a dataframe that fix a lot of these problems called the tibble. We will use the names dataframe and tibble interchangeably in this course, and for the most part they are pretty similar, but you should be aware that they have some subtle differences and you should use tibbles when possible.\n\n11.3.1 Vectors\nYou can create a vector with the c() function. You can also create a vector of numbers using the : operator. For example, these two lines both create the same vector of the values 1 through 5:\n\nc(1,2,3,4,5)\n\n[1] 1 2 3 4 5\n\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n\n\nCreate a vector of all the integer numbers from 2 to 250.\n\n\n# hint text\n\"Try using the `:` operator\"\n\n\n2:250\n\n\n# check code\ngradethis::grade_code()\n\n\n\n11.3.2 Other vector tips\nThere are several other useful things to remember about vectors:\n\nYou can combine two vectors into a single vector with the c() function:\n::: {.cell}\nx &lt;- 1:3\ny &lt;- 4:6\n\nc(y, x)\n::: {.cell-output .cell-output-stdout} [1] 4 5 6 1 2 3 ::: :::\nYou can extract a single value from a vector by indexing into the vector with square brackets [...] and the position of item that you want:\n::: {.cell}\nx &lt;- 101:110\n\nx[5]\n::: {.cell-output .cell-output-stdout} [1] 105 ::: :::\nYou can change any value in a vector with square brackets and the assignment operator:\n::: {.cell}\nx &lt;- 1:5\nx[2] &lt;- 42\n\nx\n::: {.cell-output .cell-output-stdout} [1]  1 42  3  4  5 ::: :::\nYou can even extend a vector by one element by assigning to a position 1 higher than it’s current length:\n::: {.cell}\nx &lt;- 1:5\nx[6] &lt;- 42\n\nx\n::: {.cell-output .cell-output-stdout} [1]  1  2  3  4  5 42 ::: :::\n\n\n\n\n11.3.3 Tibbles\nYou can create a tibble using the tibble() function. The arguments of the function should be vectors that will form the columns of the table, for example:\n\nb &lt;- c(TRUE, FALSE, TRUE)\nz &lt;- c(\"Anna\", \"Bob\", \"Carlos\")\n\ntibble(\n  a = 1:3,\n  4:6,\n  student_name = z,\n  b\n)\n\n# A tibble: 3 × 4\n      a `4:6` student_name b    \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;lgl&gt;\n1     1     4 Anna         TRUE \n2     2     5 Bob          FALSE\n3     3     6 Carlos       TRUE \n\n\nNote that: * if the vector is stored in a variable, the variable name will be the column name (e.g. column b) * you can override the column name by using new_name = the_vector as we did for the 1st and 3rd columns * if you do not, you may get an unusual column name, e.g. the 2nd column\n\nFill in the blanks to create a tibble with the column names: student, grade, pass (in that order)\n\n\nstudent &lt;- c(\"Daphne\", \"Ed\", \"Frankie\")\ncol_3 &lt;- c(TRUE, TRUE, FALSE)\ng &lt;- c(\"A\", \"C\", \"F\")\n\ntibble(\n  ____,\n  ____,\n  ____\n  )\n\n\n# hint text\n\"\"\n\n\n# hint text\n\"\"\n\n\n# solution code\nstudent &lt;- c(\"Daphne\", \"Ed\", \"Frankie\")\ncol_3 &lt;- c(TRUE, TRUE, FALSE)\ng &lt;- c(\"A\", \"C\", \"F\")\n\ntibble(\n  student,\n  grade = g,\n  pass = col_3\n  )\n\n\n# check code\ngradethis::grade_code()"
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#functions",
    "href": "src/book/11_more_r_chapter.html#functions",
    "title": "11  More about R",
    "section": "11.4 Functions",
    "text": "11.4 Functions\nComing soon."
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#scope-and-the-environment",
    "href": "src/book/11_more_r_chapter.html#scope-and-the-environment",
    "title": "11  More about R",
    "section": "11.5 Scope and the environment",
    "text": "11.5 Scope and the environment\nComing soon."
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#control-flow",
    "href": "src/book/11_more_r_chapter.html#control-flow",
    "title": "11  More about R",
    "section": "11.6 Control Flow",
    "text": "11.6 Control Flow\nEarlier in this tutorial we talked about how R executes one line of code and then moves onto the next.\nBut what about if you don’t always want to go to the next line? Maybe want to jump to a different section of code or skip some lines entirely.\nIn the first interactive tutorial we mentioned functions, which are a way of saving particular lines of code so that they can be re-run. You learned how to define a function that had several input parameters, and then call it when you want to run it by passing values as arguments for each parameter. (Yes, there are a lot of words to learn for functions - you should review the first tutorial if this makes no sense!)\n\n11.6.1 If-statements\nThere are also methods of control flow for choosing whether to skip a line entirely. We can use an if statement to run one or more lines of code based on the result of a Boolean condition:\nif(some_boolean_condition_is_TRUE){\n  run the code inside these curly brackets\n}\n\nA Boolean condition is an expression that results in a Boolean value (TRUE or FALSE). If the expression evaluates to TRUE, then the code following the if statements inside the curly brackets {...} is run.\nYou are already familiar with these conditions, as you have used them in the filter() function to indicate which rows to pick from a dataframe.\n\n\nChange the value of the x variable in the first line so that the code in the if statement is run.\n\n\nx &lt;- \"ninety-nine\"\n\nif(x == 99){\n  print(\"You did it!\")\n}\n\nprint(\"This will always run.\")\n\n\n\nx &lt;- 99\n\nif(x == 99){\n  print(\"You did it!\")\n}\n\nprint(\"This will always run.\")\n\n\n# check code\ngradethis::grade_code()\n\n\n\n11.6.2 If/else-statements\nOften you may want to do one thing if a condition is TRUE, and something else if it’s not. In this case you can put an else{...} block after the if{...} block:\nif(some_boolean_condition_is_TRUE){\n  if TRUE, run the code inside these curly brackets\n} else {\n  otherwise, run this instead\n}\n\nChange the Boolean value in the if parentheses so that the else block is executed instead.\n\n\nif(TRUE){\n  print(\"the condition was TRUE\")\n} else{\n  print(\"the expression was FALSE\")\n}\n\n\n# hint text\n\"\"\n\n\n# hint text\n\"\"\n\n\nif(FALSE){\n  print(\"the condition was TRUE\")\n} else{\n  print(\"the expression was FALSE\")\n}\n\n\n# check code\ngradethis::grade_code()\n\nIn general you won’t want to put just TRUE or FALSE in the parentheses in real world code, because it will always execute one of the blocks or the other - however it is a valid Boolean expression!"
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#loops",
    "href": "src/book/11_more_r_chapter.html#loops",
    "title": "11  More about R",
    "section": "11.7 Loops",
    "text": "11.7 Loops\nSometimes you want to run a particular piece of code over and over again (after all, do simple things repeatedly is what computers are best at).\nTo do this we need to use a loop.\n\n11.7.1 For loops\nThe most common type of loop in R is the for loop.\nThe idea behind the for loop is that “for every thing in a group of things, run some code”.\nIn R code, we would write this as:\nfor(thing in many_things){\n  some code to run\n}\nFor example, you could loop through a vector of numbers, and print out each number:\n\nfor(x in 1:10){\n  print(x)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\nNote that on each iteration of the loop we update the variable x (or whatever you choose to call it) and that variable can be used in the code inside the for loop’s code block {...}\n\nFill in the blank in the if statement’s condition so that only even numbers are printed out. (Remember that an even number divided by 2 will have have a remainder of 0.)\n\n\nfor(x in 1:10){\n  if(_____){\n    print(x)\n  }\n}\n\n\n# hint text\n\"\"\n\n\n# hint text\n\"\"\n\n\n# solution code\nfor(x in 1:10){\n  if(x %% 2 == 0){\n    print(x)\n  }\n}\n\n\n# check code\ngradethis::grade_code()\n\n\n\n11.7.2 Vectorization\nWhen possible, we should try to work on entire vectors at once rather than looping over a vector with for loops.\nThis is because R will iterate through a loop one step at a time, whereas it can operate on multiple elements of a vector at the same time, so vectorized code is much faster.\nWe do not have to worry about this too much, as we are working on small datasets where small speed increases are not noticeable - however, much of the the tidyverse functions we have learned are already optimized to work on vectors.\nThere are also situations where we have to use loops rather than vectorized operations (i.e. if one calculation depends on the result of the previous). However, if you are working on large datasets in R in the future, and your code is running too slowly, see if there is a way to speed it up by applying an operation to entire vectors at a time (rather than looping through a vector)."
  },
  {
    "objectID": "src/book/11_more_r_chapter.html#installing-r-packages",
    "href": "src/book/11_more_r_chapter.html#installing-r-packages",
    "title": "11  More about R",
    "section": "11.8 Installing R packages",
    "text": "11.8 Installing R packages\nComing soon."
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#statistical-inference-review",
    "href": "src/book/17_inference_advanced_chapter.html#statistical-inference-review",
    "title": "13  More Statistics",
    "section": "13.1 Statistical Inference Review",
    "text": "13.1 Statistical Inference Review\n\n13.1.1 Video overview\nThis video reviews the inference material covered in the first module.\n\n\nSlides: PDF\n\n\n13.1.2 Test your understanding\n\n\nquestion(\n  \"You give sick patients either a drug or a placebo, and you measure whether they die or survive. This is stored in 2 variables, `treatment` and `outcome`, which are both categorical. What are your explanatory and response variables?\",\n  answer(\"response = `treatment`, explanatory = `outcome`\"),\n  answer(\"response = `outcome`, explanatory = `treatment`\", correct = TRUE),\n  answer('response = `\"survived\"`, explanatory = `\"drug\"`'),\n  answer('response = `\"survived\"`, explanatory = `\"placebo\"`'),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"In a particular hypothesis test, we generate 10,000 permutations of our original data. 200 of them have a test statistic more extreme than our observed statistic. What is the p-value?\",\n  answer(\"0\"),\n  answer(\"0.01\"),\n  answer(\"0.02\", correct = TRUE, message=\"The p-value is the probability that a value as or more extreme than the observed statistic came from the null distribution, which equivalent to the fraction of simulations more extreme than the observed statistic. This is 200/10,000 = 0.02\"),\n  answer(\"0.05\"),\n  answer(\"0.1\"),\n  answer(\"0.2\"),\n  answer(\"0.5\"),\n  answer(\"1\"),\n  answer(\"2\"),\n  answer(\"5\"),\n  answer(\"50\"),\n  allow_retry = TRUE\n)\n\n\n\n13.1.3 Supplementary reading\nThese two chapters provide a good review of the original statistical inference material that we covered in the first inference module. You should review them before starting this week’s assignment:\n\nStatistical Inference via Data Science by Chester Ismay and Albert Kim (available online at: https://moderndive.com/)\n\nChapter 7: Sampling - https://moderndive.com/7-sampling.html\nChapter 9: Hypothesis testing - https://moderndive.com/9-hypothesis-testing.html\n\n\nOn a lighter note, you can get an animated explanation of a hypothesis test with alpacas here: https://www.jwilber.me/permutationtest/\n\n\n13.1.4 Code review\nIn the first inference module, we used the infer package to simulate the null distribution.\nWe ran 10,000 random permutations to shuffle up the values of the response and explanatory variables. This random shuffling ensures that we simulate a distribution where there is no relationship between the two variables.\n&lt;DATAFRAME&gt; %&gt;%\n  specify(&lt;RESPONSE&gt; ~ &lt;EXPLANATORY&gt;) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(10000, type = \"permute\") %&gt;%\n  calculate(stat = \"...\", order = c(..., ...))\n\nThe specify function defines the response and explanatory variables.\nThe hypothesize function declares what the null hypothesis is between those variables. \"independence\" means that there is no relationship between them.\nThe generate function is where the simulations are actually run: we specify how many, and how to shuffle the data (in this case, using permutations).\nThe calculate functions takes the simulations and calculates a test statistic: i.e. a single value for each one (e.g. the difference in proportions, or the difference in means)."
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#one-sided-vs.-two-sided-tests",
    "href": "src/book/17_inference_advanced_chapter.html#one-sided-vs.-two-sided-tests",
    "title": "13  More Statistics",
    "section": "13.2 One-sided vs. two-sided tests",
    "text": "13.2 One-sided vs. two-sided tests\n\n13.2.1 Video overview\n\n\nSlides: PDF\n\n\n13.2.2 1 or 2?\n\n\nquestion(\n  \"You want to conduct a *two-sided* hypothesis test on data from an experiment comparing whether a patient survive a disease after receiving a drug or a placebo. What is your alternative hypothesis?\",\n  answer(\"The difference in proportions of patients who survive is significantly different between drug and placebo groups.\", correct = TRUE, message = \"In a two-sided test, the alternative hypothesis is that there is a difference, regardless of direction (i.e. whether the difference is positive or negative is unimportant).\"),\n  answer(\"The difference in means of patients who survive is significantly different between drug and placebo groups.\"),\n  answer(\"The difference in means of patients who survive is significantly greater in the drug group than the placebo group.\"),\n  answer(\"The difference in proportions of patients who survive is significantly greater in the drug group than the placebo group.\"),\n  allow_retry = TRUE\n)"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#effect-sizes",
    "href": "src/book/17_inference_advanced_chapter.html#effect-sizes",
    "title": "13  More Statistics",
    "section": "13.3 Effect sizes",
    "text": "13.3 Effect sizes\n\n13.3.1 Video overview\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#resampling-with-permutations-versus-bootstraps",
    "href": "src/book/17_inference_advanced_chapter.html#resampling-with-permutations-versus-bootstraps",
    "title": "13  More Statistics",
    "section": "13.4 Resampling with permutations versus bootstraps",
    "text": "13.4 Resampling with permutations versus bootstraps\nComing soon."
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#confidence-intervals",
    "href": "src/book/17_inference_advanced_chapter.html#confidence-intervals",
    "title": "13  More Statistics",
    "section": "13.5 Confidence intervals",
    "text": "13.5 Confidence intervals\n\n13.5.1 Video overview\n\n\nSlides: PDF\n\n\n13.5.2 Questions\n\n\nquestion(\n  \"You want to conduct a hypothesis test using the `infer` package in R. How should you shuffle the data to generate a null distribution?\",\n  answer(\"Bootstraps\"),\n  answer(\"Permutations\", correct = TRUE, message = \"A permutation shuffles up the values of the explanatory and response variables, and so simulates a world in which there is no relationship between the two (i.e. the null hypothesis).\"),\n  answer(\"It doesn't matter\"),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"You want to calculate the confidence interval around a statistic (e.g. the mean) using the `infer` package in R. How can you approximate resampling the original population?\",\n  answer(\"Bootstraps\", correct = TRUE, message = \"By taking random samples of rows from the original sample (*bootstrap resampling*), we can approximate resampling the original population.\"),\n  answer(\"It doesn't matter\"),\n  answer(\"Permutations\"),\n  allow_retry = TRUE\n)"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#further-reading",
    "href": "src/book/17_inference_advanced_chapter.html#further-reading",
    "title": "13  More Statistics",
    "section": "13.6 Further reading",
    "text": "13.6 Further reading\nIf you wish to do further reading on bootstrapping and confidence intervals, here are some online book chapters on the topic. (They cover the same material, so pick the one that makes the most sense to you.)\n\nStatistical Inference via Data Science by Chester Ismay and Albert Kim\n\nChapter 8: Bootstrapping and Confidence Intervals - https://moderndive.com/8-confidence-intervals.html\n\nStatistical Modeling by Daniel Kaplan\n\nChapter 5: Confidence Intervals - https://dtkaplan.github.io/SM2-bookdown/confidence-intervals.html"
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#models-in-more-dimensions",
    "href": "src/book/18_modeling_advanced_chapter.html#models-in-more-dimensions",
    "title": "14  More Models",
    "section": "14.1 Models in more dimensions",
    "text": "14.1 Models in more dimensions\n\n14.1.1 Multiple explanatory variables\nIn our previous module on linear models, we considered what is called simple linear regression: a linear model with just one explanatory variable, \\(x\\):\n\\(y = m \\times x + c\\)\nwhich we created using the lm function.\nHowever, we can technically have as many explanatory variables in our model as we like! (But we can only ever have a single response variable, \\(y\\)) For example, this model has two explanatory variables, called \\(x_1\\) and \\(x_2\\) to differentiate them:\n\\(y = m_1 \\times x_1 + m_2 \\times x_2 + c\\)\nNote:\n\neach explanatory variable gets its own coefficient (\\(m_1\\) and \\(m_2\\))\nthere is still just a single intercept, \\(c\\)\n\n\n\n14.1.2 But what does that look like?\nWe can pretty easily plot \\(y\\) vs a single explanatory variable \\(x\\). In this case, a linear model is just a best-fit line through the data:\n\nset.seed(42)\nx &lt;- 1:10 + rnorm(10, 0, 0.5)\ny &lt;- 2*x + rnorm(10, 0, 1)\nsim_df &lt;- tibble(x, y)\nsim_df %&gt;%\n  ggplot(aes(x,y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nBut what does a linear model look like in higher dimensions?\n\n# Code adapted from https://chart-studio.plotly.com/~diego898/122/_3d-plane-of-best-fit-through-generated-dummy-data/#code\ntrace1 &lt;- list(\n  uid = \"5c56cf7b-dc4c-4dfa-b691-728a6a9e1b6b\", \n  mode = \"markers\", \n  name = \"Data\", \n  type = \"scatter3d\", \n  x = c(3.882026172983832, -0.3627411659871381, -0.8540957393017248, 1.3263858966870303, -0.461584604814709, 1.9436211856492926, -1.6020576556067476, -0.4017809362082619, -0.977277879876411, 0.37816251960217356, -0.30230275057533557, -0.7474548114407578, -0.1871838500258336, 0.3187276529430212, -1.5362436862772237, 0.4001572083672233, -0.8264385386590144, -0.7421650204064419, 0.7065731681919482, 0.9008264869541871, -0.5788496647644155, -0.20829875557799488, 0.681594518281627, 0.8024563957963952, 0.7774903558319101, -1.5447710967776116, 0.9693967081580112, 0.6764332949464997, 1.7858704939058352, 1.5430145954067358, 0.05616534222974544, -0.2127402802139687, -0.3595531615405413, -1.2348258203536526, 0.15634896910398005, 0.41059850193837233, -0.1550100930908342, 1.2023798487844113, 0.40198936344470165, -1.0799315083634233, 0.9222066715665268, -0.6895497777502005, -0.1715463312222481, 0.0875512413851909, 1.8958891760305832, -0.6634782863621074, 1.8492637284793418, 0.33367432737426683, -1.3749512934180188, 1.469358769900285, 0.01747915902505673, -1.180632184122412, 0.26705086934918293, 1.117016288095853, 1.454273506962975, 2.240893199201458, -0.14963454032767076, 0.9060446582753853, -1.1680934977411974, -0.6510255933001469, -1.980796468223927, -0.4380743016111864, 0.8443629764015471, 1.2224450703824274, 0.6350314368921064, 0.386902497859262, -0.20515826376580087, 0.12898291075741067, 1.336527949436392, 1.9507753952317897, 1.4805147914344243, 0.3169426119248496, -2.2234031522244266, 1.7133427216493666, 0.3024718977397814, 0.12167501649282841, -1.4200179371789752, 0.4393917012645369, -0.17992483581235091, -0.6848100909403132, -1.0994007905841945, -0.1513572082976979, -0.43782004474443403, 0.05194539579613895, -1.0930615087305058, 0.40746183624111043, 0.5392491912918173, 1.9100649530990337, 0.42833187053041766, 0.6536185954403606, 0.9444794869904138, 0.03183055827435118, 1.9295320538169858, -1.4543656745987648, 0.4627822555257742, 0.8235041539637314, 0.9766390364837128, -1.3477590611424464, 0.052165079260974405, -1.7262826023316769), \n  y = c(3.882026172983832, -0.6343220936809636, 0.31306770165090136, 0.298238174206056, -1.3159074105115212, -1.17312340511416, 0.6252314510271875, 0.17742614225375283, 1.8675579901499675, 0.1549474256969163, -0.3873268174079523, -0.41361898075974735, 0.04575851730144607, 0.920858823780819, 0.46566243973045984, 1.764052345967664, -0.7447548220484399, 0.8644361988595057, 0.3563663971744019, -1.1651498407833565, -0.8707971491818818, 0.5765908166149409, -1.0342428417844647, -0.2680033709513804, -1.2527953600499262, -1.0002153473895647, -1.2704849984857336, -0.635846078378881, 0.010500020720820478, -0.7395629963913133, -0.31155253212737266, -1.6138978475579515, -0.672460447775951, 1.1394006845433007, -0.3479121493261526, -0.10321885179355784, 0.947251967773748, 1.2302906807277207, 0.12691209270361992, 1.126635922106507, 0.6140793703460803, -0.8034096641738411, 0.5232766605317537, 0.9494208069257608, 1.4882521937955997, -0.0984525244254323, -0.43515355172163744, 0.44386323274542566, -0.35399391125348395, 1.5327792143584575, -0.45553250351734315, -0.510805137568873, -1.2928569097234486, -0.9128222254441586, 0.144043571160878, 0.9787379841057392, -0.6945678597313655, 1.8675589604265699, -0.0392828182274956, 0.8568306119026912, -0.8877857476301128, -0.5096521817516535, -1.225435518830168, -0.40317694697317963, 0.16667349537252904, -0.8954665611936756, 1.4940790731576061, 0.7290905621775369, 2.16323594928069, -1.7062701906250126, 1.9229420264803847, 1.1880297923523018, -0.6436184028328905, -0.06824160532463124, 0.06651722238316789, 0.7610377251469934, -1.0485529650670926, -1.4912575927056055, 1.1787795711596507, 0.402341641177549, 0.37642553115562943, 0.9500884175255894, -1.1474686524111024, -0.9072983643832422, 0.3960067126616453, 0.6722947570124355, -0.7699160744453164, -0.8612256850547025, -0.028182228338654868, -2.5529898158340787, 2.383144774863942, -0.6743326606573761, -0.4980324506923049, 2.2697546239876076, -1.6301983469660446, 0.7717905512136674, 0.2082749780768603, 1.8831506970562544, -1.1043833394284506, -0.813146282044454), \n  z = c(0.6014148753050095, 52.76493680024886, 28.830181943943813, -52.751941211785535, -116.76325181356648, -185.65277987329165, 166.9266453026271, -32.74004418686453, 200.35475438264095, 84.07114839099326, -388.7351875153565, 44.1641313590487, 124.12766472786151, 11.33498950758262, 137.75319692929813, 69.93385864098357, 30.233468047331854, 165.09992349889487, 132.69190946366308, -206.11535605233962, 26.99732219416606, 365.6174266134817, -31.784900223469393, -165.8343459440985, -146.27731910939744, 10.210615390490545, -50.87361679224105, -110.51610451997985, -114.91059728659813, -9.211078415251677, -12.345496216569781, -150.20963346355904, -81.6021418991762, 62.37725640290973, 71.2915984442432, -79.86847898246879, 72.68455508109857, 75.95651283788784, -31.44557337935764, 282.6790798921676, -80.04878296553002, -34.87737844022634, 80.0106052206387, 70.59644155587, 36.88090048841791, -20.960391883243197, -215.60192693204198, 56.02926881751827, -39.22168581135089, 109.19498731579706, -92.38694595459536, -75.30341111686698, -130.22720888849, -185.3731571473548, 105.22665586124084, 14.144545758253031, 2.9242732489158243, 1.1669016364443507, 121.87838283834178, -46.94177432519599, -50.010839552593836, -26.167096300997564, -6.079239773173526, -84.7611095980838, 110.21636367129128, -224.15927360347007, 186.41050332336596, 7.169682592501054, 102.77209750471626, -21.581724138016796, 308.8768868115247, 229.41389106695559, -5.206358355322031, 83.9845870497286, -62.87852405647838, 5.928508882708712, -235.1797972236149, -258.92704556152404, 24.750468381961923, -112.63339759970111, 39.835715392615406, 143.7387638843515, -142.56710308804608, -60.880299394084815, 123.9687416050054, 76.25604817699059, -37.495883472786346, -162.4630155758125, -20.648920355074306, -280.80620838340803, 201.0298512682453, -72.25885831763277, 35.049556162643526, 270.53472727948645, -193.10382080579126, 114.09991346642917, -88.86871570838584, 282.071569269954, -79.07507692577363, -154.61086620064873), \n  marker = list(\n    size = 2, \n    opacity = 0.9\n  )\n)\ntrace2 &lt;- list(\n  uid = \"f606e49f-bb1b-4d74-acc9-a9d80027e663\", \n  name = \"Plane of Best Fit\", \n  type = \"surface\", \n  x = matrix(c(-3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3, -3, -2, -1, 0, 1, 2, 3), nrow=7, ncol=7), \n  y = matrix(c(-3, -3, -3, -3, -3, -3, -3, -2, -2, -2, -2, -2, -2, -2, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3), nrow=7, ncol=7), \n  z = matrix(c(-168.76593592211438, -198.5860701810799, -228.40620444004537, -258.2263386990109, -288.0464729579764, -317.8666072169419, -347.6867414759074, -82.74318014958601, -112.56331440855152, -142.38344866751703, -172.2035829264825, -202.02371718544802, -231.8438514444135, -261.663985703379, 3.279575622942346, -26.540558636023157, -56.36069289498866, -86.18082715395416, -116.00096141291965, -145.82109567188516, -175.64122993085067, 89.3023313954707, 59.4821971365052, 29.662062877539697, -0.15807138142580102, -29.9782056403913, -59.7983398993568, -89.6184741583223, 175.32508716799907, 145.50495290903356, 115.68481865006805, 85.86468439110256, 56.044550132137054, 26.22441587317156, -3.595718385793944, 261.3478429405274, 231.5277086815619, 201.7075744225964, 171.8874401636309, 142.0673059046654, 112.24717164569992, 82.42703738673441, 347.37059871305576, 317.5504644540903, 287.7303301951248, 257.91019593615925, 228.09006167719377, 198.2699274182283, 168.44979315926278), nrow=7, ncol=7), \n  opacity = 0.7, \n  showscale = FALSE, \n  colorscale = \"Greys\"\n)\ndata &lt;- list(trace1, trace2)\nlayout &lt;- list(\n  scene = list(\n    xaxis = list(title = list(text = \"x1\")), \n    yaxis = list(title = list(text = \"x2\")), \n    zaxis = list(title = list(text = \"y\"))\n  ), \n  title = list(text = \"3D Plane of Best Fit Through 2 Explanatory Variables\"), \n  margin = list(\n    b = 10, \n    l = 0, \n    r = 0, \n    t = 100\n  )\n)\np &lt;- plot_ly()\np &lt;- add_trace(p, uid=trace1$uid, mode=trace1$mode, name=trace1$name, type=trace1$type, x=trace1$x, y=trace1$y, z=trace1$z, marker=trace1$marker)\np &lt;- add_trace(p, uid=trace2$uid, name=trace2$name, type=trace2$type, x=trace2$x, y=trace2$y, z=trace2$z, opacity=trace2$opacity, showscale=trace2$showscale, colorscale=trace2$colorscale)\np &lt;- layout(p, scene=layout$scene, title=layout$title, margin=layout$margin)\n\np\n\nAs you can see, in three dimensions we have a plane of best fit rather than a line (note: you should be able to zoom and rotate the 3D graph).\nIn fact, we can have more than 2 explanatory variables. Although we can’t realistically visualize 4 or more dimensions, you can think of a high dimensional linear model as a high dimensional plane: a hyperplane.\nHowever, as we shall see, our inability to plot high dimensional linear models makes it impossible to assess the linear model’s assumptions by plotting the original data. Instead we will create alternative plots that reduce each assumption down to a 2D graph."
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#multiple-regression-in-r",
    "href": "src/book/18_modeling_advanced_chapter.html#multiple-regression-in-r",
    "title": "14  More Models",
    "section": "14.2 Multiple regression in R",
    "text": "14.2 Multiple regression in R\n\n14.2.1 lm() with multiple explanatory variables\nRecall that the first argument of the lm function was a formula. We create an R formula by:\n\nremoving the constant coefficients\nreplacing the \\(=\\) sign with ~ (~ means “is related to”)\n\nFor example,\n\\(y = m x + c\\)\nbecomes\ny ~ x\nThe same principle applies to multiple explanatory variables:\n\\(y = m_1 x_1 + m_2 x_2 + c\\)\nbecomes\ny ~ x1 + x2\n\nFor each of the following questions, identify the correct R formula (or original linear model equation). Some questions assume that the data is in a data frame called df.\n\n\n\nquestion(\n  \"What is the correct R formula for this equation: ${Price} = m_1 \\\\times {Bedrooms} + m_2 \\\\times {Sqft} + c$ ?\",\n  answer(\"`y ~ x1 + x2`\"),\n  answer(\"`Bedrooms + Sqft ~ Price`\"),\n  answer(\"`Price + Bedrooms + Sqft`\"),\n  answer(\"`Price ~ Bedrooms + Sqft`\", correct = TRUE),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"What is the equation behind this R formula: `cost ~ labor + goods` ?\",\n  answer(\"$y = m_1 x_1 + m_2 x_2 + c$\"),\n  answer(\"${cost} = m_1 \\\\times {labor} + m_2 \\\\times {goods} + c$\", correct = TRUE),\n  answer(\"$y = m_1 \\\\times {labor} + m_2 \\\\times {goods} + c$\"),\n  answer(\"`Cost ~ Labor + Goods`\"),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"How would you create this linear model in R? ${Price} = m_1 \\\\times {Bedrooms} + m_2 \\\\times {Sqft} + c$\",\n  answer(\"`lm(Price ~ Bedrooms + Sqft, data = df)`\", correct = TRUE),\n  answer(\"`lm(Price ~ Bedrooms + Sqft)`\"),\n  answer(\"`df %&gt;% lm(Price ~ Bedrooms + Sqft)`\"),\n  answer(\"`Price ~ Bedrooms + Sqft`\"),\n  allow_retry = TRUE\n)\n\n\n\n\n14.2.2 Create a multiple regression model\n\nLet’s create a linear model from the diamonds dataset.\nThe diamonds dataset is included as part of the ggplot2 package and contains observations about the properties of thousands of diamonds. We are going to use three variables from the dataset:\n\nthe price variable will be our response variable (i.e. y)\nthe carat and depth variables will be our explanatory variables\n\n\nFill in the blanks to create this linear model.\n\n\ndiamonds_model &lt;- lm(_____ ~ _______ + _______, data = _______)\n\n\ndiamonds_model &lt;- lm(price ~ carat + depth, data = diamonds)\n\n\n# check code\ngradethis::grade_code()"
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#interaction-terms",
    "href": "src/book/18_modeling_advanced_chapter.html#interaction-terms",
    "title": "14  More Models",
    "section": "14.3 Interaction terms",
    "text": "14.3 Interaction terms\n\n14.3.1 Independence and interaction\nSo far we have assumed that each term is independent: i.e. each one contributes to the response variable \\(y\\) a separate amount proportional to its coefficient.\nHowever, it is possible for variables to interact with each other:\n\\(y = m_1 x_1 + m_2 x_2 + m_3 x_1 x_2 + c\\)\nThis linear model has an interaction term: \\(x_1 x_2\\). What this means is that the contribution of either variable depends somewhat on the value of the other!\nFor example, consider an experiment where we are looking at the effect of water and sunlight on plant height. Both variables may increase a plant’s height - however they are also dependent on each other. If we keep a plant in complete darkness, then no amount of water will help it grow (and vice versa). In otherwords, the effect of either sunlight or water on plant height depends on how much of the other variable the plant is receiving.\nIt is very easy to add interaction terms to a linear model. For example, the formula above would be written something like:\ny ~ x1 + x2 + x1 * x2\nAs you can see, we just add an asterisk * between variables that we wish to have an interaction between."
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#testing-assumptions",
    "href": "src/book/18_modeling_advanced_chapter.html#testing-assumptions",
    "title": "14  More Models",
    "section": "14.4 Testing assumptions",
    "text": "14.4 Testing assumptions\n\n14.4.1 Linearity\nBy definition, a linear model’s predictions (for the response variable) will all fall on a flat line/plane/hyperplane. Therefore, if the observed (i.e. true) values of the response variable are linearly related to the explanatory variables, they will also be linearly related to the predicted response variables.\nIn other words, no matter how many dimensions in our original model, we can always create a 2D plot to examine the assumption of linearity! We just have to create a scatter plot of the observed response variable vs the predicted response variable.\n\nWe call this an observed vs predicted plot (observed on the y axis, predicted on the x). Because these values are essentially the same number (or would be if the model went through every data point exactly), we should expect these variables to fall on a line with a slope of 1 and an intercept of 0. We will add such a reference line to our plot for comparison.\nBefore we can do so, we need to calculate the predicted values. We can easily create a column of predicted values using the add_predictions function from the broom package, which adds a column called pred to a dataframe using a model (such as the diamonds_model that you created earlier with the lm function):\n\ndiamonds_model &lt;- lm(price ~ carat + depth, data = diamonds)\n\n\ndiamonds_plus_preds &lt;- diamonds %&gt;%\n  add_predictions(diamonds_model)\n\n\nUsing the new diamonds_plus_preds data frame, fill in the 3 blanks to create an observed vs predicted plot by:\n\nplotting the true (observed) response variable, price, on the y axis, and the model’s predictions, pred, on the x axis\nadding a reference line using the geom_abline function with a slope of 1 and an intercept of 0.\n\n\n\ndiamonds_model &lt;- lm(price ~ carat + depth, data = diamonds)\ndiamonds_plus_preds &lt;- diamonds %&gt;%\n  add_predictions(diamonds_model)\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = _____, y = _____)) +\n  geom_abline(slope = 1, intercept = ___, color = \"red\")\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = pred, y = price)) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\")\n\n\n# check code\ngradethis::grade_code()\n\nWe can see from this plot that while there is definitely a relationship, it is slightly non-linear. This is a violation of the linearity assumption.\n\n\n14.4.2 Constant variability\nTo assess constant variability of the residuals, we can just plot the residuals (on the y-axis) vs the predicted response variable (x-axis): a residuals vs predicted plot.\nAgain, this graph will always have just two dimensions, regardless of how many explanatory variables are in our model, and so it is ideal for assessing this assumption.\nIn this graph, our reference line should be a horizontal line plotted at \\(y = 0\\), because the residuals should be distributed around zero in all parts of the model.\n\nFill in the 3 blanks to create a residuals vs. predicted plot (remember that these are stored in the resid and pred columns). Set the geom_hline to intercept at \\(y = 0\\).\n\n\ndiamonds_model &lt;- lm(price ~ carat + depth, data = diamonds)\ndiamonds_plus_preds &lt;- diamonds %&gt;%\n  add_predictions(diamonds_model) %&gt;%\n  add_residuals(diamonds_model)\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = ______, y = ______)) +\n  geom_hline(yintercept = ____, color = \"white\", size = 2)\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = pred, y = resid)) +\n  geom_hline(yintercept = 0, color = \"white\", size = 2)\n\n\n# check code\ngradethis::grade_code()\n\nWe can clearly see that the variability of residuals is not constant above and below the reference line.\nFor comparison, here are three example residual vs predicted plots (with purple guide lines showing the variability):\n\nthe left-most plot shows constant variability of the residuals above and below the line\nthe middle graph violates this assumption because the variability of the residuals increases as the predicted values increase.\nthe third graph also violates this assumption, because the variation above and below the line is not consistent (this pattern arises because there is not a linear relationship between the response and explanatory variables - the residual vs predicted plot can also be used to examine the first assumption of linearity).\n\n\nknitr::include_graphics(\"../img/resid-vs-pred.png\")\n\n\n\n\n\n\n\n14.4.3 Nearly-normal residuals\nWe could plot a histogram of the residuals to see if they are normally distributed.\nHowever, it is often hard to interpret the “normality” of a histogram. Instead, a much better method is to create a Quantile-Quantile (Q-Q) plot. This video from Stat Quest gives an excellent overview of what a Q-Q plot is:\n\n\nQ-Q plots are so useful that ggplot already contains geom functions for creating them.\nFirst however, we need to add a column of residuals to our dataframe using the add_residuals function that you are already familiar with (we’re also going to re-add the pred column to show you how to do both at the same time, and because we’ll need both columns in the next section):\n\ndiamonds_plus_preds &lt;- diamonds %&gt;%\n  add_predictions(diamonds_model) %&gt;%\n  add_residuals(diamonds_model)\n\nNow we can use the diamonds_plus data frame to create a Q-Q plot (with both points and a reference line):\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_qq(mapping = aes(sample = resid))+\n  geom_qq_line(mapping = aes(sample = resid))\n\n\n\n\n\nquestion(\n  \"Does this model satisfy the assumption of nearly-normal residuals?\",\n  answer(\"Yes\"),\n  answer(\"No\", correct = TRUE),\n  answer(\"Can't say one way or the other\"),\n  incorrect = \"To satisfy this assumption, the majority of the points should fall very close to the line.\"\n  )\n\n\n\n14.4.4 Independent Observations\nComing soon."
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#hypothesis-tests-for-models",
    "href": "src/book/18_modeling_advanced_chapter.html#hypothesis-tests-for-models",
    "title": "14  More Models",
    "section": "14.5 Hypothesis tests for models",
    "text": "14.5 Hypothesis tests for models\n\n14.5.1 Linear models and hypothesis tests\nYou may remember from our first modeling module that the tidy function reported a p-value for the model and for each explanatory variable. See the p.value columns in both of these examples:\n\nsim_model &lt;- lm(y ~ x, data=sim_df)\n\n\nsim_model %&gt;%\n  glance() %&gt;%\n  select(r.squared, p.value)\n\n# A tibble: 1 × 2\n  r.squared    p.value\n      &lt;dbl&gt;      &lt;dbl&gt;\n1     0.935 0.00000512\n\n\nAs you will remember from our inference modules, the p-value is the probability that the actual data was generated in a world in which the null hypothesis is true.\nNull hypothesis?! But we haven’t specified any hypotheses for our linear model, have we?\nIn fact, there is a null hypothesis for every linear model, although we have never formally written it down. The null hypothesis for a linear model is that there is no relationship between the response and explanatory variables. In other words, our null hypothesis is that the line of best fit is a horizontally flat line (e.g. the blue line in this plot):\n\nsim_df %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x, y)) +\n  geom_abline(slope = 0, intercept = 0, color = \"blue\") +\n  ylim(-5,25)\n\n\n\n\nIn other words, the p-value is the probability that the data came from this line. Just by looking at it, we can see that this is not very likely, and in fact the p-value is 0.000005 (or 0.0005%), i.e. statistically very unlikely.\nWe continue to use our regular significance threshold, \\(\\alpha = 0.05\\). In this case, we can reject our null hypothesis (that there is no relationship).\nHowever, hypothesis testing only tells us the probability that there is a relationship, or not. If there is a relationship, it doesn’t tell us how good it is, i.e. how well our model fits the data. For this, we need to continue to use the \\(R^2\\) value, as well as our graphs to check the 3 main assumptions of the linear model.\n\n\n14.5.2 p-values of individual variables\nThe tidy() function also reports a p-value for each variable in the model.\n\nsim_model %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)     1.27     1.06       1.20 0.264     \n2 x               1.75     0.164     10.7  0.00000512\n\n\nFor individual variables, the null hypothesis is that each individual explanatory variable has no relationship with the response variable. In other words, our null hypothesis is that the intercept and slope are 0.\nAs we can see for this example, because the p-values of the slope is below 0.05, so we reject our null hypothesis. We find that the non-zero slope of the x variable is statistically significant. However, the intercept has a p-value of 0.26. This is greater than 0.05, so we cannot reject our null hypothesis of a zero intercept for this particular dataset.\n(This dataset was simulated with an intercept of zero, so that makes sense!)\n\nA note on scientific notation\nIn the previous examples, our p-values have been written as strange numbers, such as: 2.637853e-01\nThis is a convenient way of writing numbers that are very large or very small. We can write 1.2 million (1,200,000) as:\n\\(1.2 \\times 10^6\\)\ni.e. 1.2 times 1 million, or\n1.2e6\nwhich uses e to represent “10 to the power of”.\nFor numbers that are less that one, we can do the same thing but use a negative power of ten. For example, we can write 0.263 as:\n\\(2.63 \\times 10^{-1}\\)\nor\n2.63e-1\nbecause \\(10^{-1}\\) is 0.1"
  },
  {
    "objectID": "src/book/32_databases_chapter.html#relational-data",
    "href": "src/book/32_databases_chapter.html#relational-data",
    "title": "15  Databases",
    "section": "15.1 Relational data",
    "text": "15.1 Relational data\nA database is simply an organized way of storing data.\nThere are obviously many ways that one could choose to organize data, but a particularly common form of database is the relational database, in which data is broken up into multiple tables. In general the goal of a relational database is to avoid repeating the same piece of data in multiple rows.\nFor example, in the dataset of hobbits and addresses in the previous section, we can avoid repeating the same address for Bilbo and Frodo by splitting the addresses into a separate table. Then every hobbit who lives at a particular address gets a link to that address row in the separate address table (instead of repeatedly writing out the same address for each hobbit).\nIn addition, relational databases follow the same rules that tidy datasets have to follow:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nRelational databases are extremely common (for example, almost every website you visit has a relational database that stores the website’s information), and learning how to work with these databases is a vital skill for every data scientist.\n\n15.1.1 Joining matching rows\nIn such cases, there are often columns in each table that link the two tables back together.\nFor example, consider these two tables:\n\n\n# A tibble: 3 × 3\n  names   age hobbit_hole\n  &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Bilbo   111           1\n2 Frodo    50           1\n3 Sam      38           2\n\n\n# A tibble: 2 × 2\n     id address                \n  &lt;dbl&gt; &lt;chr&gt;                  \n1     1 Bag End, Hobbiton      \n2     2 3 Bagshot Row, Hobbiton\n\n\nThe first table contains three hobbits. However, instead of listing out their addresses, we have instead recorded a number that corresponds to the id column of the second table.\nBy matching the hobbit_hole column with the id column, we can see that Bilbo and Frodo both live at Bag End whereas Sam lives at 3 Bagshot Row.\nSome terminology:\n\nIn the mathematical theory of databases, each of these tables would be called a “relation”, hence the name relational data for data stored across multiple tables in this fashion.\nThe id column in the table of addresses is an example of a key. A key is any column (or columns) that provide a unique way of identifying every row in a table.\nThe hobbit_hole column in the table of hobbits is a foreign key. This links each hobbit to the identifying key in a different (i.e. “foreign”) table.\n\nOf course, we could just have recorded this data as a single table! For example:\n\n\n# A tibble: 3 × 3\n  names   age hobbit_hole            \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;                  \n1 Bilbo   111 Bag End, Hobbiton      \n2 Frodo    50 Bag End, Hobbiton      \n3 Sam      38 3 Bagshot Row, Hobbiton\n\n\nWhat are the advantages to splitting data over multiple tables?\n\nOne reason is to avoid repeating certain values. For example, we have written the address Bag End twice. If we split addresses into a separate table then we only need to write it once. This means that:\n\nWe need less space to store our data.\nWe can reduce errors and inconsistencies (for example, we won’t end up with two different spellings of Bag End by accident (Bag End vs. Bag-End)).\nUpdating the dataset is easier (if we later decided that Bag End did need to be spelled with a hyphen then we would only need to update it in one place - otherwise we might easily update it for one hobbit and forget to do so for another).\n\n\nOf course, sometimes we need to combine data from multiple tables into a single dataframe for analysis.\nTo do this we need to join the separate datasets back together, typically using by using columns that link to connected rows in different tables (such as keys and foreign keys).\nThere are several different ways to join tables, which will link rows in different ways.\nTo illustrate these different joins let us imagine that we are ecologists in a similar world to that inhabited by the hobbits from the previous section. We have gathered the following two tables of data about different species inhabiting this unusual world.\n\nspecies &lt;- tibble(\n  species = c(\"Giant Eagle\", \"Goblin\", \"Goblin\", \"Giant Spider\", \"Balrog\"),\n  location = c(2, 2, 3, 1, NA)\n)\n\nsightings &lt;- tibble(\n  location_id = c(1,2,3,4),\n  name = c(\"Mirkwood Forest\", \"Misty Mountains\", \"Mordor\", \"The Shire\")\n)\n\nOne important thing to note is that there are some rows in each table that do not have a match in the other table.\n\nThe default type of join is an inner join (if somebody ever says just “join” then they are usually referring to an inner join).\nAn inner join matches rows that have the same values in some column(s). Any rows in either table that do not have a match are omitted.\nWe will use the inner_join() function from the dplyr package which has the following signature:\ninner_join(x, y, by=NULL)\n\nx and y are two dataframes that we wish to join.\nThe by parameter specifies which columns to use for matching rows. If we do not specify it then R will automatically match any columns with the same names. It is a good idea to always specify it to make sure that we are only joining on the columns that we want to match!\n\nNote that: * We are piping in the species tibble to the inner_join() function, but we could have written it inside the function instead. * The argument to by is a named vector specifying the names of the columns. * \"location_id\" is the only value in this 1 item vector, but we have also named this item as \"location\". * R will look for the column \"location\" in the first dataframe (which is species in this example), and location_id in the second dataframe (sightings).\n\nspecies %&gt;%\n  inner_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 4 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n\n\nIn our output we only have rows that had a matching row in the other dataframe (i.e. we dropped the Balrog row from the species dataframe and The Shire from the sightings table).\nAlso, note that the Misty Mountains now appear twice in the output, even though there is only one Misty Mountains row in the sightings table."
  },
  {
    "objectID": "src/book/32_databases_chapter.html#database-software-and-sql",
    "href": "src/book/32_databases_chapter.html#database-software-and-sql",
    "title": "15  Databases",
    "section": "15.2 Database software and SQL",
    "text": "15.2 Database software and SQL\nYou could create your own simple version of a relational database by storing each of your tables in a separate file on your computer’s harddrive. For simple applications this is a valid solution - however in more complex projects you start to run into problems with ensuring that tables contain the corret data, or that different programs can simultaneously access the data (and don’t accidentally overwrite an update to a file created by another computer program), or simply with efficiently wrangling large amounts of data.\nWe can solve all of these problems by using a dedicated database program that takes of care of storing our data for us. In addition to storing the data for us, these database management systems (DMS) will also allow us to query the data (i.e. access it) and will run those queries in an efficient manner.\nRelational databases all use a special programming language for accessing their data. This programming language is called SQL.\n\nSQL stands for Structured Query Language. It is properly pronounced using by spelling out the acronym: “ess-kew-ell”. However, some people pronounce it as “sequel”.\n\nFortunately we do not need to learn SQL for this class (although you should learn SQL at some point if you wish to do more work with data in the future, since most of it will be stored in databases). Instead we can interact with relational databases using the data wrangling functions that we have already learned.\nI.e. we can use functions like select(), filter(), and inner_join() directly on a relational database and R will translate these into the SQL programming language for us!\n\n15.2.1 Connecting to a database.\nSince a database management system is a separate software program, we need first need to set up a connection to that database software from R.\nTo do this we can use an R package called DBI. In these examples we will connect to a database program called SQLite. (There are many other relational database systems (MySQL, PostgreSQL, etc.) but we could connect to those with very similar code.)\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"my_database_name.sqlite\")\nWhat is this code doing?\n\nThe dbConnect() function from the DBI package sets up the connection to the database.\n\nNote that we have used the :: operator in the general form package_name::function to access the function from a particular package. This way we do not have to load the package first with the library() function.\n\nThen we use the SQLite() function from the `RSQLite package to specify that this database uses the SQLite dabase management system.\nFinally we have to specify the location of the database. SQLite stores databases as a file on your computer, so here we have just written the name of such a file: \"my_database_name.sqlite\".\n\n\n\n15.2.2 Listing tables\nWe can get a list of all the tables in a database by running this DBI package function on our new connection:\nDBI::dbListTables(con)\n\n\n15.2.3 Connecting to a database table\nTo access a table in our database with our standard data wrangling functions, we need to create a new R variable that points to that table. We do this with the tbl() function from the dbplyr package:\nsome_table &lt;- tbl(con, \"table_name\")\nThis code creates a new variable called some_table that allows us to access a particular database table.\nYou will need to create a new variable in the same way for every database table that you wish to access from R.\n\nNote that the dbplyr package is different to dplyr:\n\ndplyr contains the data wrangling functions that we have used to interact with R dataframes (e.g. filter(), select(), etc.).\ndbplyr allows us to use these data wrangling functions on databases by translating the functions into SQL, and then converting the database output back into an R dataframe.\n\n\n\n\n15.2.4 An example: selecting columns\nWe can now use our data wrangling functions on the database table just as if it was an R dataframe.\nFor example, to select() columns:\nexample_query &lt;- some_table %&gt;%\n  select(example_column_1, example_column_2)\nOne difference, however, is that this code will not create a new dataframe. Instead it creates a SQL query that can be run on the database (in the future) to create a new dataframe.\nWe have stored this query in a new variable called example_query. If we wish, we can view what the SQL query looks like by running the show_query() function:\nexample_query %&gt;%\n  show_query()\nTo run the query and return a dataframe, you need to run the collect() function on the query:\nexample_df &lt;- example_query %&gt;%\n  collect()\nThis will retrieve the data from the database and store it as an R dataframe in a new variable called example_df."
  },
  {
    "objectID": "src/book/32_databases_chapter.html#joining-data-from-relational-database-tables",
    "href": "src/book/32_databases_chapter.html#joining-data-from-relational-database-tables",
    "title": "15  Databases",
    "section": "15.3 Joining data from relational database tables",
    "text": "15.3 Joining data from relational database tables\nIn the More Data Wrangling chapter, we learned that we could join separate tables of data together by matching rows with\n\n15.3.1 Outer joins\nSometimes we want to preserve a row from one (or both) of the tables we are joining even if there is no match with the other table. These are called outer joins, and there are three types depending on which table(s) we want to keep unmatched rows from:\n\nA left outer join (or just “left join”) keeps all rows from the table on the left (i.e. the first dataframe we name).\nA right outer join (aka “right join”) keeps the rows in the table on the right instead (the second dataframe we name).\nA full outer join (“full join”) keeps all rows from both tables.\n\nAny rows that are kept but not matched to a row in the other table will just have missing data in the other table’s columns.\nThe process for doing these joins is very similar to an inner join.\n\n15.3.1.1 Left outer joins\n\nspecies %&gt;%\n  left_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 5 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 Balrog             NA &lt;NA&gt;           \n\n\n\n\n15.3.1.2 Right outer joins\n\nspecies %&gt;%\n  right_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 5 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 &lt;NA&gt;                4 The Shire      \n\n\n\n\n15.3.1.3 Full outer joins\n\nspecies %&gt;%\n  full_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 6 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 Balrog             NA &lt;NA&gt;           \n6 &lt;NA&gt;                4 The Shire"
  },
  {
    "objectID": "src/book/32_databases_chapter.html#non-relational-databases",
    "href": "src/book/32_databases_chapter.html#non-relational-databases",
    "title": "15  Databases",
    "section": "15.4 Non-relational databases",
    "text": "15.4 Non-relational databases"
  },
  {
    "objectID": "src/book/references.html",
    "href": "src/book/references.html",
    "title": "References",
    "section": "",
    "text": "Baker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a."
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#rstudio",
    "href": "src/book/A_faqs_appendix.html#rstudio",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.1 RStudio",
    "text": "A.1 RStudio\n\nA.1.1 RStudio error: Error occurred during transmission\nIf you get this error message when logging into the GMU RStudio Server:\n\n\n\nRStudio error message: Error occurred during transmission\n\n\nSolution\n\nGo to https://rstudio.cos.gmu.edu/home\nClick the “Quit” button (near the top of the page)\nThen you should be able to reopen a project."
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#cannot-push-to-github-authentication-failed",
    "href": "src/book/A_faqs_appendix.html#cannot-push-to-github-authentication-failed",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.2 Cannot push to GitHub: “Authentication failed”",
    "text": "A.2 Cannot push to GitHub: “Authentication failed”\nYou might find yourself with an error like this when you try to clone a repository from GitHub into RStudio, or push commits back to GitHub:\nremote: Support for password authentication was removed on August 13, 2021.\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\nfatal: Authentication failed for 'https://github.com/mason-cds101/final-project-dominicwhite/'\nThis usually means that either:\n\nYou have not correctly created and set your GitHub token (as per the instructions in step 4 Section Section 2.4.1). If this is the first time you are trying to clone from or push to GitHub, this is probably the reason.\nYour previous token has expired, and you will need to create a new one and store it in RStudio. If your token used to allow you to clone from and push to GitHub but has suddenly stopped working, then this is probably the reason.\n\nIn both cases, repeat Step 4 in Section Section 2.4.1."
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#packages",
    "href": "src/book/A_faqs_appendix.html#packages",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.3 Packages",
    "text": "A.3 Packages\n\nA.3.1 Installing an older version of a package\nTo install an older version of a package (e.g. the lmvar package), you can run these two lines of code, making sure to replace the name of the package and the version that you want to install:\nrequire(remotes)\ninstall_version(\"lmvar\", version = \"1.5.2\", repos = \"http://cran.us.r-project.org\")"
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-create-new-rstudio-project",
    "href": "src/book/B_additional_setup_appendix.html#sec-create-new-rstudio-project",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.1 How to clone a GitHub repository into RStudio",
    "text": "B.1 How to clone a GitHub repository into RStudio\n\nB.1.1 In RStudio Cloud\n\nFrom your homepage, click on the New Project button in the top right of the screen (Figure B.1)\n\n\n\n\nFigure B.1: The New Project Button in RStudio Cloud\n\n\n\nIn the drop-down menu, click on the option that says “New Project from Git Repository” (Figure B.2).\n\n\n\n\nFigure B.2: The menu of different ways to create a new project.\n\n\n\nIn the pop-up window, paste in the URL (web address) of the GitHub repository that you wish to open in RStudio (Figure B.3).\n\n\n\n\nFigure B.3: An example of the kind of GitHub repository URL (web address) you need to type in.\n\n\n\n\nB.1.2 In RStudio Desktop\n\nClick on the New Project option in the Files dropdown menu).\nIn the New Project wizard that pops up, click on the option that says “Version Control” (Figure B.4)\n\n\n\n\nFigure B.4: Step 1 of RStudio’s New Project wizard.\n\n\n\nOn the next page of the wizard, click on the “Git” option (Figure B.5)\n\n\n\n\nFigure B.5: Step 2 of RStudio’s New Project wizard.\n\n\n\nOn the final page of the wizard (Figure B.6), fill in the details for the GitHub repository you wish to “clone” (i.e. download), and where to download it to:\n\n\n\n\nFigure B.6: Step 3 of RStudio’s New Project wizard.\n\n\n\nIn the first field (“Repository URL”) copy-and-paste the web address of the GitHub repository’s homepage.\nThe second field will be the name of the folder created on your computer to hold all the files you are downloading from the GitHub repository. It may auto-fill with the repository’s name - you can also type something in, or change it to a different folder name if you prefer.\nThe third field is the name of the parent folder that will hold the folder above. I would recommend organizing related projects (e.g. the projects from this book) in a single parent folder so that they are easy to find. If you click “Browse” you can choose or create a parent folder to hold all your project folders.\n\nThen click “Create Project”."
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-rstudio-github-linux-connection",
    "href": "src/book/B_additional_setup_appendix.html#sec-rstudio-github-linux-connection",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.2 Connecting RStudio to GitHub on Linux",
    "text": "B.2 Connecting RStudio to GitHub on Linux\n\nFirst, go to RStudio and open the tab in the left hand pane called Terminal. If you do not see a Terminal tab, then you can create one from the top menu of RStudio Desktop by going to “Tools &gt; Terminal &gt; New Terminal”.\nIn this terminal, set your GitHub username by running this line, making sure to replace your name inside the quotation marks:\ngit config --global user.name \"Your Name Here\"\nThen run this commend, again making sure to replace the email inside the quotation marks with the same email you used to sign up for GitHub:\ngit config --global user.email \"you@emailHost.com\"\nI would also recommend running one final line in the Terminal (this will enable your computer to store your GitHub login details - otherwise you will be typing them in a lot).\ngit config --global credential.helper store\nThen go to the Console tab (which should be next to the Terminal), and copy and paste these lines of R code one at a time:\ninstall.packages(\"usethis\",\"gitcreds\")\nthen this line (which will open a GitHub web page - see below for what to fill in)\nusethis::create_github_token()\n\nOn the token webpage that appears, make sure that you are creating a “Classic token”, and not a “Fine-grained” token. Then you will need to set the following options:\n\nIn the Note field, write something that indicates where this token will be used, e.g. RStudio.\nFor the expiration date, pick a date about in the future after which you will no longer need the token. E.g. if you are following these instructions for a class, pick a date after the end of the semester.\nYou should not select no expiration date - that is a security risk.\n\nYou can leave all the checked scopes as the defaults, and then scroll down to the green Generate token button at the bottom of the webpage and click it.\nThe next page that appears will display a token, a random series of letters and numbers that is basically a temporary password that you can use to authorize a restricted set of activities on your GitHub account (without having to share your master password with RStudio). You will never see this token again after you leave this page, so don’t close the webpage until you have finished this section, or you will have to create an entirely new token.\nReturn to the Console tab in RStudio, and run this line:\ngitcreds::gitcreds_set()\nAt the prompt, copy and paste the token from GitHub and click enter.\n(If you ever need to replace the token, just run gitcreds::gitcreds_set() in the RStudio Console again.)"
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-vscode-setup-appendix",
    "href": "src/book/B_additional_setup_appendix.html#sec-vscode-setup-appendix",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.3 VS Code IDE Set-up",
    "text": "B.3 VS Code IDE Set-up"
  }
]