[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Computing and Data for Scientists",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "src/book/01_intro.html#what-are-the-computational-and-data-sciences",
    "href": "src/book/01_intro.html#what-are-the-computational-and-data-sciences",
    "title": "1  Introduction",
    "section": "1.1 What are the computational and data sciences?",
    "text": "1.1 What are the computational and data sciences?\nIn this book we are concerned with two closely related fields:\n\nComputational science (also called scientific computing) combines computer science (i.e. the theory of how computers work) with applied problems.\nData science similarly combines statistics (i.e. the mathematical theory of analyzing data) with applied problems.\n\nBasically we want to know how to apply computers and data to investigate scientific questions. In this book we will focus on questions from the natural and social sciences (i.e. physics, biology, economics, psychology, etc.), but the techniques and methods can be applied outside of science, too."
  },
  {
    "objectID": "src/book/01_intro.html#why-this-book",
    "href": "src/book/01_intro.html#why-this-book",
    "title": "1  Introduction",
    "section": "1.2 Why this book?",
    "text": "1.2 Why this book?\nMany books on these topics (particularly data science) are interested in business problems. Businesses are typically concerned with learning about the future (typically so that they can increase their profits).\nThere is nothing inherently wrong with this, and indeed scientists are sometimes interested in predicting the future as well. However scientists are more generally interested in the broader goal of understanding how the world works.\nAlthough scientists and business people have to worry about many of the same problems when dealing with computers and data, our different end goals mean that we put more emphasis on different parts of the process.\nAs scientists, we want to learn the underlying truths of the universe.\n\nWe want to make sure that any discoveries we make are real (we don’t want to fool ourselves, a pit that is surprisingly easy to fall into). We therefore care about making sure that our experiments are reproducible. (I.e. if we have discovered something real, any other scientist should be able to follow our steps to get to the same result.)\nSince we care about understanding over prediction, we prefer simpler mathematical approaches that are easier to interpret.\nAs scientists we should try to focus on simplicity and clarity over glamour!\n\nThat’s not to say that a data scientist at a company shouldn’t care about reproducibility, simplicity, and clarity. But if a company can make a million dollars tomorrow with a sexy but complicated and hard-to-reproduce analysis, then they should do so! But this would be bad practice for a scientist."
  },
  {
    "objectID": "src/book/01_intro.html#the-scientific-method",
    "href": "src/book/01_intro.html#the-scientific-method",
    "title": "1  Introduction",
    "section": "1.3 The Scientific Method",
    "text": "1.3 The Scientific Method\nWhy don’t we still believe that the sun is the flaming chariot wheel of the ancient Greek god Apollo as he drives across the sky each day? Why do we now have effective medicines and treatments for many illnesses that were a death sentence for our ancestors?\nThe answer, in case you haven’t guessed, is science. Science is the reason that human progress has exploded since the scientific revolution of the 16th and 17th centuries. And science offers our best path forwards for making future progress and confronting the challenges that we will inevitably face in the coming centuries.\nWe can think of science as a game: humanity versus the unknown. Each field of science (biology, physics, psychology, etc.) is like a team in this great game. And just like regular sports teams, each scientific field uses slightly different tactics to play the game, because the type of thing that a scientist studies influences how they can go about experimenting on it.1\nDespite the range of techniques used by different scientists, and the conventions followed by different scientific fields, all scientists agree on the rules of the game itself. This shared set of rules is called the scientific method.\nActually, that’s a simplification. There is not actually a single universally agreed upon definition of what the scientific method actually is, but broadly speaking it follows these principles:\n\nA scientific theory or hypothesis must make testable predictions about how the world behaves.\nWe test each theory by doing experiments that have the ability to disprove that theory.\nWe can only choose between theories based on the outcomes of these experiments.2\n\nWhat this looks like in practice can vary widely: a biologist could be out in the field observing living organisms or in the laboratory looking at cells; a physicist might be coming up with precise mathematical theories or building a telescope to go into space to observe distant galaxies; an economist might be collecting data about people’s behavior or the growth of economies.\nIn the 21st century, a unifying feature of all of these scientists is that they are using computers to analyze their data or run experiments. The scientific method tells us nothing about how we should use computers, only that whatever we do needs to adhere to the principles outlined above.\n\nThe scientific method tells us very little about how we should go about using computers, so let’s consider how we might do so.\n\n1.3.1 Reproducibility and replicability\nAs scientists we are trying to discover truths about the universe. These truths are universal. If I experience gravity, so should you (and everyone else). And if two scientists conduct the same experiment independently, they should both get evidence that is consistent with that universal truth.\nThere are two separate concerns when it comes to the repeatability of science.\nThe first is that if you and I analyze the same data (e.g. we share the same dataset), in the same way, we should arrive at the same result. This is generally called reproducibility.\nIt’s also possible for us to obtain our own datasets (e.g. we each run our own experiment, sometimes completely different experiments) with a view to investigating that universal truth. I might measure the Earth’s gravitational field by dropping rubber balls out of a second floor window, whereas you might use the speed of a pendulum’s swing. However, we should get results that are consistent with each other. This is called replicability.\nWe reproduce analyses; we replicate experiments.3\n\n\n1.3.2 How reproducible is modern science?\nThe rest of this book is focused on how to use computers and data in ways that are good scientific practice. However, let us take a moment to talk more about the importance of reproducibility.\nUnfortunately, scientists are not rewarded for making sure that their analyses are reproducible, or for checking that they can reproduce the work of other scientists.\nIn a recent survey of 1500 scientists (Baker 2016), a shockingly high percentage said that they were unable to reproduce another scientist’s work:\n\n\n\n\n\nFigure 1.1: Data from (Baker 2016).\n\n\n\n\nConsider how the process of modern science works:\n\nThe scientist needs to get money to pay for their future work, so they apply for grants. In the USA, a lot of grant money is distributed by the federal government; other grants may come from private organizations. However, there is never enough money for every scientist who applies, and so the grant distributors have to pick their favorites. And what makes a proposal likely to be funded? Doing exciting, new research (not routine reproduction of other scientists existing work).\nEven if you decide to reproduce an experiment, its not always simple to do so. Scientists are supposed to describe the steps they followed in their experiment, but they do not always provide enough details for somebody else to replicate their work exactly. Sometimes, scientists can’t even reproduce their own work (this is not uncommon in laboratory experiments in chemistry or biology, where you are working with minuscule substances that you hope are in your test tubes, but which might be subtly different from one day to the next).\nIn the computational and data sciences, there are often subtle choices we can make in terms of parameter values, or choice of algorithm, or simply the way that you write your code, which can significantly alter the output of your computer program. Thus you may also need the original computer code that was used for the analysis!\nIf the experiment is the analysis of an existing dataset, then anyone wishing to reproduce the work will need access to the same data and code. Unfortunately, scientists do not always make their data or code available when they publish their results.\n\nSometimes this is for good reasons (e.g. protecting the privacy of the people in the dataset, or not publishing the genetic sequence of a dangerous virus).\nOther times the reasons might be more self-serving. For example, if you were able to get a large grant of money to conduct a complex experiment, you might want to publish several different analyses of the results. Unfortunately there are no prizes for second place in science, and the first person to make a discovery gets the glory. If somebody else publishes a discovery that you were about to publish yourself, then scientists call this getting “scooped”. To avoid getting scooped, you might decide to restrict access to your data until you have published all the analyses of it that you want to.\nAnd sometimes scientists don’t have a good reason for not publishing their data/code, except that it wasn’t required of them.4\n\n\n\n\n1.3.3 What we can do\nFortunately science has recognized its reproducibility problem over the past decade, partly due to several major scandals. Since the root cause of the problem is due to misaligned incentives, science has adapted to change these incentives.\nFor example:\n\nIn the USA, scientists funded by the federal government are typically required to make data available when they publish their results.\nTo guarantee publication of your data while preventing another scientist from “scooping” your work, you can restrict data access in responsible ways, typically by publishing the data at the same time as your study but placing an embargo on it. This delays the release of the data, typically by several months or years, until you have had a chance to publish your other analyses, but ensures that it will eventually become available to any other scientists who want to check your work or extend upon it.\nNew initiatives like the Center for Open Science are promoting the sharing of data and code, for example by creating the Open Science Framework for new studies.\nThere are now various websites that will permanently archive code and data so that it can be easily shared with other researchers, such as DataDryad and FigShare.\n\nSince this is a book on analyzing data rather than conducting experiments, we will focus on reproducibility rather than replicability.\n\n\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a."
  },
  {
    "objectID": "src/book/01_intro.html#footnotes",
    "href": "src/book/01_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "For example, a microbiologist can grow bacteria in a lab and do pretty much whatever they want experimentally, whereas a psychologist studying human behavior is much more constrained by what is ethically acceptable to do to other humans.↩︎\nIf all of your experiments suggest that a theory is wrong, you can’t decide to stick with it just because you like it for some other reason. For example, let’s say that you hypothesize that all cows are blue. When you start finding non-blue cows in the real world, you can’t claim that your theory is still correct because blue is your favorite color.↩︎\nExcept in the minority of scientific fields that define these terms in different, or even opposite, ways.↩︎\nThis is a problem of both the scientists, as well as science as a discpline for not holding ourselves to higher standards.↩︎"
  },
  {
    "objectID": "src/book/02_setup.html#code-editors",
    "href": "src/book/02_setup.html#code-editors",
    "title": "2  Setup",
    "section": "2.1 Code editors",
    "text": "2.1 Code editors\nProgramming code is typically written in plain text files (but instead of the file extension .txt they will typically use a different extension that indicates the programming language being used). Traditionally, code in the R programming language was written in R script files with the .R file extension.\nBecause these code files are just text, all you need to open them is a text editor, like Windows’s Notepad or Mac’s TextEdit. However, you can also use a dedicated code editing program, which will add a lot of useful features that makes writing code much easier. These editing programs are often called Interactive Development Environments, but because that is a mouthful, we usually just refer to them using their acronym: IDEs.\nThe most popular IDE for the R programming language is called RStudio, and there are two easy ways to use it.\n\nDownload and install the free RStudio Desktop version to your computer.\nUse the online Posit Cloud1 version through a web browser (no installation required, but only free for a certain number of hours each month2).\n\nThese two versions are functionally equivalent, so the only things you really have to decide is how averse you are to paying money and whether you can install software on the computer you are using.\n\nHow to pick the best RStudio editor for you.\n\n\n\n\n\n\n\n\nDon’t want to pay\nCan pay if necessary\n\n\n\n\nCan install software\nInstall RStudio Desktop.\nEither works.3\n\n\nCan’t install software\nUse Posit Cloud online, and don’t exceed the monthly free quota.\nUse Posit Cloud.\n\n\n\nLater in this chapter I will give instructions for getting each of these options up and running.\nThere are also other IDEs that you can use to write R code. A popular general IDE is VS Code, which can be used to write in any different programming language. The downside is that it lacks a lot of R specific features that you can find in RStudio. However, instructions to get set-up with VS Code are provided in an Appendix: Section B.3."
  },
  {
    "objectID": "src/book/02_setup.html#version-control",
    "href": "src/book/02_setup.html#version-control",
    "title": "2  Setup",
    "section": "2.2 Version control",
    "text": "2.2 Version control\nMost programming is not actually that difficult, once you learn how to think like a computer. The hard part of programming is writing large complicated programs with other programmers.\nOne of the reasons this is challenging is because we need a way to collaboratively edit the same set of files that contain our code.\nThis is not just a problem for programmers - if you and a friend were writing a report together in Microsoft Word, you might find yourself emailing the Word document back-and-forth. In fact, Microsoft Word has helpful a feature called Track Changes that allows you to see who has edited different parts of a file.\nUnfortunately, you can’t edit the file while your friend is, otherwise you will end up with different versions of the document, and the only way to recombine them will be to compare them side-by-side and manually copy over any differences.\nIn these modern times, you could instead use an online collaborative program like Google Docs - but while that is fine for text, programs need to be run, and that usually needs you and your friend to be working on separate computers so that your versions of the code don’t interfere with each other when running.\nSoftware engineers have come up with solutions to these problems, which they call version control (because it enables you to control the version of the program that you are running). The dominant version control software used today is called Git, and it is so popular that IDE’s like RStudio automatically include integrations to work with it.\nWe will talk more about how Git works in Chapter 12 but for now you can think of it as like the save points in a video game. When you reach a significant point (like adding an important software feature, or defeating a boss in a video game) you can save your progress at that point. This allows you rewind your progress back to that point if you make a mistake in the future.\nGit can also figure out how to automatically combine different versions of a project (i.e. your version and your friend’s version), and it will keep track of all the changes you record in a save point as well as who made them.\nWe can also use websites like GitHub.com to share projects that are managed with Git. This is useful for a number of reasons, but from a reproducibility perspective it allows other scientists to download our code and run it for themselves."
  },
  {
    "objectID": "src/book/02_setup.html#step-1-create-a-github-account",
    "href": "src/book/02_setup.html#step-1-create-a-github-account",
    "title": "2  Setup",
    "section": "2.3 Step 1: Create a GitHub account",
    "text": "2.3 Step 1: Create a GitHub account\nIf you already have a GitHub account, then proceed to step 2.\nIf not, then go to https://github.com and create a free account. A few suggestions:\n\nIf you have any interest in working in tech in the future, then pick something vaguely professional as your GitHub username. You don’t want to have to explain to a future employer why your GitHub username is squeaky_boi. Think of your GitHub profile as the the programming equivalent of your LinkedIn profile.\nGitHub will ask if you want to upgrade to a fancy paid account when you register, but you should stick with the free account which has everything we need.\nGitHub will also ask you a bunch of questions when you sign up about what you want to use it for. It really doesn’t matter what you respond to these questions, so feel free to skip through them."
  },
  {
    "objectID": "src/book/02_setup.html#step-2-set-up-rstudio",
    "href": "src/book/02_setup.html#step-2-set-up-rstudio",
    "title": "2  Setup",
    "section": "2.4 Step 2: Set up RStudio",
    "text": "2.4 Step 2: Set up RStudio\n\n2.4.1 Option 1: Install RStudio Desktop on your computer\nIf you are installing RStudio Desktop on your computer, then you should follow these steps:\n\nFirst, you should first install Git by following the appropriate installation instructions for your operating system on the Git website: https://git-scm.com/downloads\n(Note that Git is almost certainly already installed if you are using Linux.)\nNext you should install R by going to the appropriate page for your operating system:\n\nWindows:\n\nDownload and install R from the .exe installer on this page: https://cran.rstudio.com/bin/windows/base/\nThen also install RTools from this page (make sure the version number of RTools matches the version number of R that you just installed, e.g. if you installed R v4.2.3 then you will need to install RTools v4.2 [i.e. the same first two digits]): https://cran.rstudio.com/bin/windows/Rtools/\n\nMac: download and install R from the appropriate .pkg installer for your version of macOS on this page: https://cran.rstudio.com/bin/macosx/\nLinux: follow the instructions for your flavor of Linux: https://cran.rstudio.com/\n\nFinally we are can install RStudio Desktop. Download the installer for your operating system here and then install from it: https://posit.co/download/rstudio-desktop/\nNote that RStudio Desktop is free, but Posit (the company that created RStudio) also offers several paid versions, so make sure you get the free RStudio Desktop version.\nAfter you have installed RStudio, you should be able to start the program, which should look like this:\n\nNext we need to install a program called LaTeX (pronounced “lay-tek” - this will turn our files containing R code into nicely formatted PDFs).\nTo do this, open RStudio. There should be a pane on the left called “Console”. In this Console, copy and paste the following two lines and hit enter to run them:\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOnce you have done this, proceed to Section 2.5 to connect GitHub to RStudio.\n\n\n2.4.2 Option 2: Create an online RStudio account at Posit Cloud\nGot to https://posit.cloud/ and create an account. You can sign up for the free plan, which at the time of writing includes 25 hours of online RStudio access per month before they ask you to pay.\nTo connect your GitHub account, click on your profile name/icon in the top right of the posit.cloud homepage, and then on the Authentication page.\nFind the line on this page where it says GitHub, and slowly click any unchecked checkboxes (wait a couple of seconds between each checkbox in case a prompt appears).\nSome of the checkboxes may open up new webpages taking you to GitHub, which will ask you to verify that you want to authorize Posit to access your GitHub account. Make sure you agree all of these, otherwise you might not be able to edit code on GitHub."
  },
  {
    "objectID": "src/book/02_setup.html#sec-rstudio-github-connection",
    "href": "src/book/02_setup.html#sec-rstudio-github-connection",
    "title": "2  Setup",
    "section": "2.5 Step 3: Connect RStudio to GitHub",
    "text": "2.5 Step 3: Connect RStudio to GitHub\n\n\nFirst, go to RStudio (or launch a new empty RStudio project if you are using the online RStudio at posit.cloud) and open the tab in the left hand pane called Terminal. If you do not see a Terminal tab, then you can create one from the top menu of RStudio Desktop by going to “Tools &gt; Terminal &gt; New Terminal”.\nIn this terminal, set your GitHub username by running this line, making sure to replace your name inside the quotation marks:\ngit config --global user.name \"Your Name Here\"\nThen run this commend, again making sure to replace the email inside the quotation marks with the same email you used to sign up for GitHub:\ngit config --global user.email \"you@emailHost.com\"\nI would also recommend running one final line in the Terminal (this will enable your computer to store your GitHub login details - otherwise you will be typing them in a lot).\ngit config --global credential.helper store\nThen go to the Console tab (which should be next to the Terminal), and copy and paste these lines of R code one at a time:\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nthen this line (which will open a GitHub web page - see below for what to fill in)\nusethis::create_github_token()\n\nOn the token webpage that appears, make sure that you are creating a “Classic token”, and not a “Fine-grained” token. Then you will need to set the following options:\n\nIn the Note field, write something that indicates where this token will be used, e.g. RStudio.\nFor the expiration date, pick a date about in the future after which you will no longer need the token. E.g. if you are following these instructions for a class, pick a date after the end of the semester.\nYou should not select no expiration date - that is a security risk.\n\nYou can leave all the checked scopes as the defaults (you need the first set of repo scopes), and then scroll down to the green Generate token button at the bottom of the webpage and click it.\nThe next page that appears will display a token, a random series of letters and numbers that is basically a temporary password that you can use to authorize a restricted set of activities on your GitHub account (without having to share your master password with RStudio). You will never see this token again after you leave this page, so don’t close the webpage until you have finished this section, or you will have to create an entirely new token.\n\nReturn to the Console tab in RStudio, and run this line:\ngitcreds::gitcreds_set()\nAt the prompt, copy and paste the token from GitHub and click enter.\n(If you ever need to replace the token, just run gitcreds::gitcreds_set() in the RStudio Console again.)"
  },
  {
    "objectID": "src/book/02_setup.html#footnotes",
    "href": "src/book/02_setup.html#footnotes",
    "title": "2  Setup",
    "section": "",
    "text": "Formerly known as RStudio Cloud.↩︎\nAt the time of writing, you get 25 hours per month for free, after which you have to pay.↩︎\nBear in mind that RStudio Cloud can only be used if you are connected to the internet, so if you want to work somewhere without internet then you will need to install RStudio Desktop. Also, RStudio Desktop will ultimately provide you with more flexibility.↩︎"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#your-first-program",
    "href": "src/book/03_r_programming_chapter.html#your-first-program",
    "title": "3  Introduction to R",
    "section": "3.1 Your First Program",
    "text": "3.1 Your First Program\nIt is a long-standing tradition that your first program in any new programming language should simply display the message “Hello, World!”.\nIn R we can use the print() function to display a piece of data (such as the text Hello, World!).\nFor example, we can run this line of code in R:\nprint(\"Hello, World!\")\nAnd R will return this result:\n\n\n[1] \"Hello, World!\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhat’s happening here?\nWe will learn more about the different parts of this code in this chapter, but briefly:\n\nprint(...) is a function: it is a recipe that takes some input ingredients (which we list between the parentheses after the function’s name), and does something with them.\n\"Hello, World!\" is a string of characters (and other symbols, such as !). A character string is a type of data that computers can work with. Here we are supplying it as the input argument to the print() recipe.\n\nWhen you hit Enter, R figures out how to covert this code into a series of electrical currents that your computer’s processor can understand. The result comes back as another series of electrical currents, which R then figures out how to convert back into something that a human can understand.\n\n\n\n\n\n\n\n\n\nExercise: Try it yourself: Open up RStudio and copy or type the line of code into the RStudio Console pane (see Fig. Figure 3.1).\n\n\n\nFigure 3.1: How to find the RStudio Console\n\n\nNote that you will need to write the code after the prompt, which is the &gt; symbol that starts the lowest line in the Console.\nAfter you have written all the code, press the  key on your keyboard. This will send your code to the R Console, which will run it and return the output. You should get back the same output as we saw earlier (Fig. Figure 3.2).\n\n\n\nFigure 3.2: Running the Hello World program in the RStudio Console\n\n\nCongratulations! You just ran your first program in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Try to edit the line in the Console containing the print(\"Hello, World!\") command that you just ran. Can you change it to print out something else?\n\n\n\n\n\n\nTip\n\n\n\nOnce you have pressed  to run a line of code in the Console, you cannot go back and change it. To modify it, you will need to re-type out the code at the empty prompt at the bottom of the Console (below the output of the last line of code that you ran). You cannot edit a line of code in the Console that has already been run.\n\n\nIt can get tedious to rewrite the code you have already run in the Console. However, click at the new (empty) Console prompt, and press the Up arrow on your keyboard. RStudio should autofill the prompt with the previous line of code from your history.\nModify this line to print out a different sentence, and rerun your new code."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#data",
    "href": "src/book/03_r_programming_chapter.html#data",
    "title": "3  Introduction to R",
    "section": "3.2 Data",
    "text": "3.2 Data\nThe central component of everything we will be doing in R this semester is data. Even non-data science programs revolve around data.\nAt a very basic level, a computer is just a fancy calculator that adds and subtracts numbers. Even things like words and pictures are stored inside a computer as numbers.\nHowever, we often want to work work with data that is not numbers. For example, in the last section we were able to get R to print out the sentence Hello, World! To your computer that was just numbers flowing down wires as electrical signals. But the R programming language took care of converting our instruction into something your computer could understand.\nThis is the magic of programming languages! They allow us to write commands in (relatively) human-readable instructions, and then take care of translating that into the very unreadable numbers that computers work with.\nR allows us to work with several “higher-level” types of data. These data types include:\n\nthe numeric data type holds numbers such as 42 or -12.5 or 0. Unlike text, numbers are written without quotation marks around them. R sometimes refers to numbers as “integers” (if they are whole numbers) or “doubles” (if it is storing a number in a way that could handle decimals, since this (historically) takes up twice the amount of computer memory as an integer).\nthe character data type holds text (i.e. letters, symbols, and the characters that represent numbers). We need to put the text inside quotation marks so that R knows where the text starts and ends: \"this is character data\".\n\nNote: in other programming languages this datatype is sometimes known as a “character string” or just a “string”.\n\nthe Boolean data type holds a value that is either TRUE or FALSE. (This is sometimes also referred to as the “logical” data type.)\n\n\n\n\n\n\n\n\nExercise: What data type is \"Introduction to Computing and Data for Scientists\"? numericcharacterBoolean\n\n\n\n\n\n\n\n\n\nExercise: What data type is 2? numericcharacterBoolean\n\n\n\n\n\n\n\n\n\nExercise: What data type is FALSE? numericcharacterBoolean\nFollow-up: In the RStudio Console, type in typeof(FALSE), and hit the  key to run this line of code. What do you get back?\n\n\n\n\n\n\nTip\n\n\n\ntypeof() is another function. Whereas the print() function displayed simply displayed its own input, typeof() returns the data type of its input.\n\n\n\n\n\n\n\n\n\n\n\nExercise: What data type is \"\"5\"? numericcharacterBoolean\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nAs far as R is concerned, 5 and \"5\" are different types of data! 5 is a number whereas \"5\" is the character of the number."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#operators",
    "href": "src/book/03_r_programming_chapter.html#operators",
    "title": "3  Introduction to R",
    "section": "3.3 Operators",
    "text": "3.3 Operators\n\n3.3.1 Combining data with operators\n\nOkay, now we know about data.\nBut data by itself is not especially useful. It just sits there until you do something to it. There are many ways of doing things to data, but some of the simplest are operators.\nOperators operate on data. You may not have heard the name operator before, but you are already familiar with many common mathematical operators, such as + and - for adding and subtracting numbers.\n\n\n\n\n\n\n\nExercise (Try it yourself): Try entering a number after the &gt; in the Console (e.g. 1), then Enter, and see what happens.\n\n\n\nWhen you hit enter, the R interpreter reads in the line, evaluates it, and returns the answer. In this case, you entered 1, so the computer thinks ‘Hey, it’s a 1! Wow, a one! The result of 1 is… drum roll, please… 1!’ and returns the result of this expression, which is a one.\nCool! But not, I confess, particularly useful. Let’s fix that: next we’ll add two numbers together.\n\n\n\n\n\n\nAt the prompt, enter two numbers separated by a plus sign, +\nFor example:\n&gt; 1 + 1\nWhat do you get?\n\n\n\n(Note that I’ve left the Console’s &gt; prompt in the example code above, but I will leave it out in future examples.)\n\n\n\nGreat! Let’s move on and investigate operators in more depth…\n\n\n3.3.2 Operating on numbers\nHeart surgeons operate on hearts, brain surgeons operate on brains. You will be operating on numbers… does that make you a data surgeon?\nHere are some of the operators available to us in R:\n\n\n\nOperator\nExample\nResult\n\n\n\n\n+\n5 + 2\n7\n\n\n-\n5 - 2\n3\n\n\n*\n5 * 2\n10\n\n\n/\n5 / 2\n2.5\n\n\n^\n5 ^ 2\n25\n\n\n%%\n5 %% 2\n1\n\n\n\nSome of these might seem obvious, while others might be unfamiliar. In this section’s exercises we will go through them all and figure out what they do.\n\n\n\n\n\n\n\nExercise (the - operator): In the R Console, type 5 - 2 and hit enter to run the line of code. (You probably have a good idea of what - does, but try changing the numbers just to make sure!)\nWhat does - do?\n\n Adds two numbers together. Subtracts one number from another. Multiplies two numbers together. Divides one number by another.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (the * operator): In the R Console, type 3 * 2 and hit enter to run the line of code.\nWhat does * do?\n\n Adds two numbers together. Subtracts one number from another. Multiplies two numbers together. Divides one number by another.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (The / operator): In the R Console, type 3 / 2 and hit enter to run the line of code.\nWhat does / do? (Just to be sure, try some other numbers.)\n\n Adds two numbers together. Subtracts one number from another. Multiplies two numbers together. Divides one number by another.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise (The ^ operator):In the R Console, type 3 ^ 2 and hit enter to run the line of code.\nWhat does ^ do? (Hint: Try some other numbers, like 2 ^ 3 or 16 ^ 0.5)\n\n Returns the modulus (the remainder after dividing one number by another). Calculates an exponential (raises one number to the power of the second). Evaluates an equality (whether one number is equal to another). Evaluates an inequality (whether one number is bigger or smaller than another).\n\n\n\n\n\n\n\n3.3.3 The %% operator\n\n\n\n\n\n\n\nExercise (The % operator): Next up, a slightly trickier one. Type 3 %% 2 and hit enter to run the line of code.\nWhat does %% do? You will probably have to try some other numbers to figure this one out.\nIf you have difficulty, try also dividing the same numbers. E.g. try both 9 %% 4 and 9 / 4.\n\n Returns the modulus (the integer remainder after dividing one number by another). Calculates an exponential (raises one number to the power of the second). Evaluates an equality (whether one number is equal to another). Evaluates an inequality (whether one number is bigger or smaller than another).\n\n\n\n\n\n\n\n3.3.4 Which operator goes first?\n \nJust like in normal math, we can do sums in R with multiple operators:\n3 + 5 / 5 * 3 ^ 2\nIn such a case, which operation do we do first?\nAgain, just like in regular math, some operations are always done before others. For example, all multiplication and division will be done before any addition or subtraction.\n\nF.Y.I.\nThe order in which operators are calculated is known as operator precedence, and you can find the precedence of any operator here: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html\n\nWe can change the order of operations with parentheses: ( and ). For example\n2 + 2 * 5 = 12\nwhereas\n(2 + 2) * 5 = 20\n\n\n\n\n\n\n\nExercise: Modify this R code\n3 + 5 / 5 * 2 ^ 2\nso that it performs the calculation \\(\\frac{3 + 5}{(5 \\times 2) ^ 2}\\).\nAnswer:  :::{.callout-tip collapse=true title=‘Expand for hint’}\nWhen correct, you should get the answer 0.08.\n\n\n\n::::"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#sec-r-programming-variables",
    "href": "src/book/03_r_programming_chapter.html#sec-r-programming-variables",
    "title": "3  Introduction to R",
    "section": "3.4 Storing and reusing results with variables",
    "text": "3.4 Storing and reusing results with variables\n\nSo far we have learnt how to combine data to get different results.\nWe can do multiple separate calculations by putting each one on a separate line. When R reads your code, it treats everything on one line as a single expression that is separate from other lines:\n2 + 2\n5 * 5\nThis program will have two separate outputs: 4 and 25\nHowever, after these results are shown to us, they are thrown away! All that effort just discarded…\nWhat if we wish to save the result of a calculation so that we can reuse it in a subsequent line?\nIn this case, we need to store the result in a variable.\n\n\n\nRun these two lines of code in the RStudio Console and see what result you get.\na &lt;- 2 + 2\n5 * a\nThen take a look at the Environment tab in the top-right pane of RStudio. Do you see a variable called a? Does it hold the value calculated in the first line of code or the second?\n\n\n\n3.4.1 The “result” of the assignment operator\nWe store the result of an expression in a variable using the assignment operator: &lt;-\nvariable_name &lt;- value_to_be_stored\n\n\nRun these two lines of code in the RStudio Console and see what result you get.\n2 + 3\nb &lt;- 2 + 4\nIf you take another look at the Environment tab in the top-right pane of RStudio, you should see another variable called b. What value does it hold?\n\n\n\n\n2 + 3\n\n\nb &lt;- 2 + 4\n\n\nThe second line of code should not print out any output when it runs. This is because assigning the result of an expression to a variable has no “result” in itself. For example, in math \\(2+2\\) is \\(4\\), but the expression \\(b = 2 + 2\\) does not return \\(4\\) directly (but somebody somewhere is hopefully keeping track of the fact that \\(b\\) is now equivalent to \\(4\\)).\nIf you want to see the data that is stored in a variable, you can put the name of the variable on a line by itself:\nsome_variable\nR will evaluate this line: it will ask itself “What is the result of some_variable”, which is just whatever value is stored in that variable.\nFor example,\n\nc &lt;- 3\n\nc\n\n[1] 3\n\n\nThe other implication of this is that if you calculate something in R and do not assign the result to a variable then it will be printed out and then forgotten. So remember: if you calculate something important in R that you will need in the future, make sure that you store that result in a variable.\n\n\nType the name of one of the variables in your Environment tab in the RStudio Console (e.g. a), and hit Enter to run it.\nDoes this return the data that you think is stored in that variable?\n\n\n\n\n\n\n3.4.2 Variables are… variable\nVariables get their name because their value can vary. We have created the variable b that holds the value 6, but we can change the value of b and store a completely different value in it!\n\n\nTry assigning the value 7 to the variable b using the assignment operator &lt;-.\nSince we already created b in an earlier exercise, you should see that its value in the Environment tab updates.\n\n\n\n\nb &lt;- 7\n\n\n\n\n\n3.4.3 When does assignment happen\n\n&lt;- is an operator, just like + or *. As such, it has a precedence: it will happen before some operators but after others.\nHowever, it turns out that the &lt;- precedence is extremely low - i.e. it will happen after the result of all the other operators on that line of code have been calculated.\nSo, when you write:\na &lt;- 2 + 4\n…you are essentially doing this:\na &lt;- (2 + 4)\n\nYou can name a variable anything you choose… with certain restrictions. R does not allow variables to begin with a number, such as 100, or contain spaces within the variable, such as one hundred.\nTechnically you can get around this restriction by putting an invalid variable name inside backticks, for example: `100` or `one hundred`.\nHowever, it is generally advisable to avoid naming variables with invalid names, since it will make your life a lot easier if you don’t constantly have to include extra backticks. For example, we might replace spaces with underscores: one_hundred."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#how-r-works",
    "href": "src/book/03_r_programming_chapter.html#how-r-works",
    "title": "3  Introduction to R",
    "section": "3.5 How R works",
    "text": "3.5 How R works\nR is an interpreted programming language.\nThat is a fancy way of saying that R runs (i.e. “interprets”) every line of code one at a time.\nSo far we have written a line of code and then run it. In a couple of exercises you may have run multiple lines of code where one line depended on a result from a previous line. However, R completely finished running the first line before moving onto the next one.\nWhen R interprets a line of code, it figures out how to convert your human-readable code into computer-readable instructions (which are a series of 0s and 1s, since a computer is basically a bunch of wires that can either have an electrical current flowing down them (which we denote as 1) or not (0)).\nBecause R is interpreted line-by-line, it is an ideal programming language for exploring and analyzing scientific data, where we typically figure out what to do next as we go along!\n\nWe will soon be learning how to write multiple lines of R code in a file and then run them from the file. However, even when R runs code from a file, it still figures out how to run it one line at a time.\n\n\n\n\n\n\nTip\n\n\n\nCompiled Programming Languages\nNot every programming language is interpreted like R. Some are compiled.\nThis means that you write all your code in a file, and then turn all of it into computer-readable instructions at once. This step is called compilation and can take a long time (up to hours for large programs in some languages!). It is typically slower to write programs in a compiled language because of this extra step.\nThe main advantage of a compiled programming language is that your computer can figure out how to optimize all the lines of code so that they run extremely fast.\nFamous examples of compiled programming languages are Java and C++."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#boolean-data",
    "href": "src/book/03_r_programming_chapter.html#boolean-data",
    "title": "3  Introduction to R",
    "section": "3.6 Boolean data",
    "text": "3.6 Boolean data\n\n3.6.1 Boolean data revisted\nAt the start of this chapter we mentioned that there is a type of data in R, called Boolean data, that can have one of two values: TRUE or FALSE.\nWe can ask R questions that have a true or false answer, for example: “Does the variable x hold the number 3?” or “Is 10 greater than 9?”\nWe do this with Boolean operators:\n\n\n\nOperator\nExample\nResult\n\n\n\n\n&lt;\n10 &lt; 9\nFALSE\n\n\n&gt;\n10 &gt; 9\nTRUE\n\n\n==\nx == 3\nFALSE\n\n\n\nFor example:\n\n10 &lt; 9\n\n[1] FALSE\n\n\nHere R returns the value FALSE when it evaluates this expression, because 10 is obviously not less than 9.\n\nCombining comparisons\nSometimes we want to know if one datum is greater than or equal to another. You can use the Boolean operators &gt;= for such a comparison, or &lt;= to see if something is less than or equal to another.\n\nJust as with numeric data, we can store a Boolean value in a variable, e.g. d &lt;- FALSE or e &lt;- 10 &lt; 9. (Remember that this assignment to a variable always happens last, after we have evaluated the expression on the right-hand side of the &lt;- operator.)\n\n\n3.6.2 The &lt; and &gt; operators\n\n\nExercise: Try it yourself: What do you get if you run 10 &lt; 9 in the RStudio Console?\nCan you change one of the numbers so that this expression returns TRUE?\nThen change the &gt; to a &lt; operator (i.e. reverse its direction). What is the result now?\n\n\n\n\n3.6.3 The == operators\n\n\nLet’s try another Boolean operator. What do you get if you run 8 == 10? What about 8 == 8?\nWhat do you think the == operator does?\n\n\n\n\nCombining comparisons\nSometimes we want to know if one datum is greater than or equal to another. You can use the Boolean operators &gt;= for such a comparison, or &lt;= to see if something is less than or equal to another.\n\n\n\nAssign the value TRUE to a variable called d.\n\n\n\n\n d &lt;- TRUE"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#vectors",
    "href": "src/book/03_r_programming_chapter.html#vectors",
    "title": "3  Introduction to R",
    "section": "3.7 Vectors",
    "text": "3.7 Vectors\n\nSo far we have looked at pieces of data by themselves:\n\na &lt;- 1\nb &lt;- 2\nc &lt;- 3\nprint(a)\n\n[1] 1\n\nprint(b)\n\n[1] 2\n\nprint(c)\n\n[1] 3\n\n\nBut what about if we want to combine multiple pieces of data together?\nR includes several types of container that can hold multiple pieces of data. We can then refer to that container by a single variable. For example, instead of the three variables above, we can create a vector that holds all three values. We create a vector with c(...), putting the objects we want to combine inside the parentheses (and separated by commas):\n\nc(1,2,3)\n\n[1] 1 2 3\n\n\nAll the data in a vector must be the same type of data. For example, a vector could contain all numbers, or all characters, but not a mix of the two.\n\n\n\nCreate a vector holding 3 character strings (in this order): “This”, “is a”, “vector!”\n\n\n\n\nc(\"This\", \"is a\", \"vector!\")\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou might be wondering what the numbers in square brackets at the start of each line in the output mean? E.g. [1]\nThese tell us whereabouts in the vector we are. The number indicates the position in the vector of the first element displayed on that line.\nFor example, the [1] at the start of the line (before “Introduction”) shows that “Introduction” is the first element in this vector.\n\n\n\n3.7.0.1 Operations on vectors\nWe can use operators on more complicated data structures just as we did on the simpler data types. For example, we can add 2 vectors together:\n\nv1 &lt;- c(1,2,3)\nv2 &lt;- c(4,5,6)\nv1 + v2\n\n[1] 5 7 9\n\n\nAs you can see, the individual elements are added together.\n\n\n\nWhat happens if you add two vectors of different lengths? For example, run this code and see what happens:\n\n\nv3 &lt;- c(10, 20, 30, 40, 50)\nv4 &lt;- c(1, 2)\nv3 + v4\n\nWhat happens when you add two v3 and v4?\nFirstly, we get a warning \"longer object length is not a multiple of shorter object length\" because v3 is longer than v4. However, a warning doesn’t stop the code running - it merely tells us that something unexpected might be happening.\nIn this particular case, R will do something called recycling which repeats the shorter vector over and over until it is the same length as the longer vector. I.e. v4 will be repeated 2.5 times to become (1,2,1,2,1) before adding it to v3.\nR warns you that this is happening because this may not be what you wanted, especially if you hadn’t realized that the vecotrs were different lengths.\n\n\n\nv3 &lt;- c(10, 20, 30, 40, 50)\nv4 &lt;- c(1, 2)\nv3 + v4"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#sec-intro-r-dataframes",
    "href": "src/book/03_r_programming_chapter.html#sec-intro-r-dataframes",
    "title": "3  Introduction to R",
    "section": "3.8 Dataframes",
    "text": "3.8 Dataframes\nIn our we often want to work with tables of data you probably encountered the concept of the table before. Typically each column in the table represents some type of measurement, known as a variable (note that the variable represented by a column is different to the R variables we learned about back in Section 3.41). Each row represents a thing that those measurements were taken from.\nFor example here’s a simple table of data that we might want to analyze in R:\n\n\n\nID\nMeasurement A\nMeasurement B\n\n\n\n\nS1\n5.2\n8.1\n\n\nS2\n6.4\n7.9\n\n\n\nJust like R has a vectors to store a single series of values, it also contains a structure called a dataframe to hold a table of data. In fact, R uses vectors to create dataframes: behind the scenes, each column of a dataframe is stored within a vector.\nDataframes can also be assigned to R variables so that we can store and retrieve them. For the purposes of this section, suppose that the R variable df contains a dataframe of the table above. If we run the variable by itself, we will print out the dataframe:\n\ndf\n\n  ID Measurement A Measurement B\n1 S1           5.2           8.1\n2 S2           6.4           7.9\n\n\nR’s use of vectors for each column of the dataframe means that each column can only contain one type of data. For example, a column might contain just character strings, or just numbers, but not a mix of the two.\nYou might ask, how do we retrive the a single column from a dataframe? We will talk more about this in Chapter 6, but for now you should know that there is a very basic method to get a column using the $ operator. (Once we learn better methods of wrangling dataframes, we will rarely use this, but it’s helpful to know.)\nIn code, we would write the dataframe’s variable, followed by a $, followed by the column name. There are no spaces between these three things. For example, we could retrieve the ID column from df as follows:\n\ndf$ID\n\n[1] \"S1\" \"S2\"\n\n\nYou’ll note that this returns a vector of the values in that column.\nOne last thing to remember about dataframes is that although its columns are not technically R variables in their own right, they do behave a lot like R variables in some ways.\nOne such similarity is how we have to write the column name in code. We run into the same restrictions on column names that apply to R variable names, e.g. they cannot contain spaces, or start with a number. However, there is no restriction on creating dataframes with column names that violate these rules, so you will often encounter dataframes with column names that cannot be referenced as R variables.\nFor example, df contains two columns that have a space in their column name: Measurement A and Measurement B. Just as with variables, if we need to reference these columns then we have to wrap the column name in backticks, e.g. `Measurement A`.\nFor example, if we wanted to extract the Measurement B column as a vector using the $ operator, we would need to write:\n\ndf$`Measurement B`\n\n[1] 8.1 7.9\n\n\nIf you’re creating your own R dataframes, it’s generally advisable to name the columns in such a way that they don’t require violate variable naming rules and require backticks, because this will make your life easier. For example, you could use underscores instead of spaces in the column name."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#sec-intro-r-functions",
    "href": "src/book/03_r_programming_chapter.html#sec-intro-r-functions",
    "title": "3  Introduction to R",
    "section": "3.9 Functions",
    "text": "3.9 Functions\nPerhaps, keen mathematician that you are, you want to calculate the length of the hypotenuse of a triangle. Dredging up memories of early math classes, you will doubtless recall Pythagoras’s theorem that the hypotenuse (the long side) of right-angled triangle is given by:\n\\[c = \\sqrt{a^2 + b^2}\\]\n(\\(c\\) is the hypotenuse [long side] and \\(a\\) and \\(b\\) are the short sides.)\n\n\nLet’s say we have a triangle where the shorter sides (a & b) are 3 and 4 units long. Can you calculate the length of side c in R using just the operators from the first section?\nHint #1: The square root is equal to the 0.5 power of a number: 4 ^ 0.5 = 2\nHint #2: Just like in regular math equations, R will calculate some operators before others. For example it will do all multiplications before any additions. However, just like in regular math, we can change the order of operations by wrapping parts of our calculation in parentheses: (...)\n\nDid you get the answer 5? Fantastic!\n\n\n\n\n3.9.1 Re-useable code = functions\nWhat’s that? Another complaint? You have to write out this long expression every time you need the hypotenuse of a triangle? (No doubt this is a frequent chore in your day-to-day life.)\nAgain, there is a solution! R allows us to save pieces of code in variables. Yes, you heard that right: variables don’t just have to store data, they can also store code!\nThese stored, reusable sections of code are called functions.\nFor example, you could create a function to calculate the sum of two numbers:\nadder &lt;- function(number1, number2) {\n    result &lt;- number1 + number2\n    return(result)\n}\nEntering these 4 lines at the console prompt will be slow and error-prone, so let’s try something different.\nClick on the “File” menu at the top of RStudio. Select “New File” and then “R Script”. A blank editor window should appear in a new pane above the console.\nCopy the adder function from the previous page into this empty script. Then press “Control + Alt + R” on your keyboard (simultaneously). This will run the contents of your script all at once.\nIf successful, you should see that adder appears in the Environment pane under a new section called Functions.\nHow do we use our adder function? Go back to the console, and type something like this:\n\nadder(3, 5)\n\nIf your function is working correctly you should get the result of the 2 numbers that you entered inside the braces.\nLet’s take another look at the adder function to understand what’s going on:\nadder &lt;- function(number1, number2) {\n    result &lt;- number1 + number2\n    return(result)\n}\nLine 1: The first line creates a new function with the function keyword and saves it to the name adder using the assignment operator &lt;-, just as we did for variables.\nAfter function are a pair of parentheses. Inside these, we put a list of the parameters that the function can take, separated by commas. In this case, our adder function has two paramters (the numbers to add together). We are going to give these numbers the temporary names number1 and number2 (creative, I know). We will use these parameter names inside the function to refer to these two numbers.\nWe end the line with an opening curly bracket { to indicate that the code that follows is part of the function.\nLine 2: This is the meat of our adder function. We add our two number paramters together and store them in a variable called result. Its important to note that result only exists inside the curly brackets of the adder function (i.e. it vanishes after the function has finished).\nLine 3: Here we specify what the function is should return: in this case we want to return the result variable.\nLine 4: We signal the end of the function with a closing curly bracket (matching the one from the end of line 1).\nYou might object (and not without reason) that our adder function is a very trivial example. Wouldn’t it just be easier to use the + operator?\nYes, it would! So let’s look at a more complicated function.\nWe can create a function to calculate the hypotenuse like this:\n\nhypotenuse &lt;- function(a, b) {\n  c &lt;- (a^2 + b^2)^0.5\n  return(c)\n}\n\nThen we can use this hypotenuse function as many times as we like. For example calculate the hypotenuse of a triangle with sides of length 3 and 4, we would run:\n\nhypotenuse(3, 4)\n\n[1] 5\n\n\n\n\nUse the hypotenuse() function to calculate the area of a triangle with sides of length 3 and 4.\nHint: Try changing the numbers inside the parentheses after hypotenuse.\n\nDid you get the answer 5? Fantastic!\n\n\n\nhypotenuse &lt;- function(a, b) {\n  c &lt;- (a^2 + b^2)^0.5\n  return(c)\n}\n\n\n\n\n3.9.2 How our hypotenuse() function works\nThere are a few things to note about this code:\n\n\nWe tell R that we are creating a reusable function using the function keyword.\nfunction is followed by parentheses (...) that contain parameters. Parameters are the names that we give to the input data to the function.\n\nFor example, above we created two parameters: a and b\nYou can have as many parameters as you want in a function, from zero on up. They must be separated by commas.\n\nThe reusable code goes inside a pair of curly brackets {...}\n\nWe can now use the function’s parameters in this code (e.g. a and b). Essentially we temporarily create new variables with the parameter names (but these are)\n\nAt the end of the function we can return a particular result with return(...) - just replace the dots with a value or\nWe store the function in a name with the assignment operator &lt;- (just like we did with variables)\nWhen we want to run the code, we write the function name followed by parentheses, with any arguments inside the parentheses (separated by commas)\n\n\n\n\nReplace the blanks to create a function to calculate the area of a triangle instead. Save this function as triangle_area.\n_______ &lt;- function(a, b) { area &lt;- _______ return(area) }\nHint: The area of a triangle is \\(0.5 imes a imes b\\)."
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#packages",
    "href": "src/book/03_r_programming_chapter.html#packages",
    "title": "3  Introduction to R",
    "section": "3.10 Packages",
    "text": "3.10 Packages\nFunctions are clearly useful - we can save a lot of time and effort by writing our code once as a function, and then just calling that function whenever we need to do that thing.\nOf course, we can save even more time by not writing the function ourselves but instead using a function that somebody else has written which does what we want.\nIn R (as in many other programming languages) we can import collections of functions (and other useful things, such as datasets) that other people have written. These collections are called packages.\n\n3.10.1 Installing packages\nBy default R will come with several useful packages installed. You can which ones are currently installed by going to the Packages tab of the bottom right pane in RStudio.\nTo install a new package, either:\n\nClick on the Install button in the Packages tab, and type the name of the package you want into to the pop-up that appears.\nGo to the RStudio Console and type in (making sure to replace the name of the package you want inside the quotes!):\ninstall.packages(\"some_package_name\")\nFor example, to install a package called the tidyverse (which we will be using for much of this book), you would run:\ninstall.packages(\"tidyverse\")\nHaving gone through this chapter, this code should hopefully make some sense! install.packages() is a function (built-in to the core R programming language), and \"tidyverse\" is a character string that we are passing as the argument to that function.\n\nNote that it can take some time to install a package (e.g. the tidyverse package can take 10-15 minutes to install!), so it’s worth checking to see if it is already installed before you waste a lot of time.\n\n\n3.10.2 Loading packages\nA package only needs to be installed to your computer once.\nHowever, you need to load the functions and other objects from that package in every R session that you wish to use them (because they will not automatically be available to R even after you have installed them).\nTo load a package we use the library() function. For example, to load the tidyverse package, you would run:\nlibrary(tidyverse)"
  },
  {
    "objectID": "src/book/03_r_programming_chapter.html#footnotes",
    "href": "src/book/03_r_programming_chapter.html#footnotes",
    "title": "3  Introduction to R",
    "section": "",
    "text": "For now I shall try to explicitly indicate whether I am talking about a column variable or a R variable, but as the book progresses I will phase out this distinction and expect the meaning of the word variable to be clear from context.↩︎"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html",
    "href": "src/book/04_rmarkdown_chapter.html",
    "title": "4  Literate Programming",
    "section": "",
    "text": "Heading"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#rmarkdown-and-quarto-files",
    "href": "src/book/04_rmarkdown_chapter.html#rmarkdown-and-quarto-files",
    "title": "4  Literate Programming",
    "section": "4.1 RMarkdown and Quarto files",
    "text": "4.1 RMarkdown and Quarto files\nRMarkdown files are text files that end in the file extension .Rmd. For example, a file called my_homework.Rmd in an RMarkdown file. Quarto files are similar, but end with the extension .qmd.\nWe can create both types of files from within RStudio. Click on the File menu in the top left of RStudio1, then pick the New File option, and then select either:\n\nRMarkdown… to create an RMarkdown file, or\nQuarto document… to create a Quarto file.\n\nWhen you create new files in this way, RStudio will include some default contents to give an example of how each type of file works. (We will also see how to create empty files, with no default contents.)\nSince the results are slightly different, let’s look at each filetype in turn.\n\n4.1.0.1 New RMarkdown files\nWhen you create a new RMarkdown file, you will be presented with the following wizard:\n\nFigure: RMarkdown creation wizard.\nYou can accept the defaults in the wizard as shown above, and modify the rest later. Or you can enter a title, author name, and/or date now if you prefer. Click OK to create the file (which will contain some default contents).\n\nFigure: Example default RMarkdown file.\nTo create an empty file without any default contents, click the Create Empty Document button instead.\nThe file will automatically be opened in the Editor pane of RStudio.\n\n\n4.1.0.2 New Quarto files\nQuarto files come with a similar set-up wizard:\n\nFigure: Quarto creation wizard.\nAgain, it is fine to just click OK with the defaults, but you can put in a title and author name if you like.\nI also like to deselect the Editor checkbox, but you can easily turn the visual editor on or off once you have created the file (Section 4.8).\nIf you would like an empty Quarto document without any default contents, click the Create Empty Document button instead of OK.\nThe file will automatically be opened in the Editor pane of RStudio.\n\nFigure: Side-by-side examples of default RMarkdown & Quarto files, with key features highlighted."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#rendering-to-an-output",
    "href": "src/book/04_rmarkdown_chapter.html#rendering-to-an-output",
    "title": "4  Literate Programming",
    "section": "4.2 Rendering to an output",
    "text": "4.2 Rendering to an output\nNote that the files you create are not saved when you create them. To save them, make sure the file is the current one open in the editor pane and either press the Save icon in the top toolbar, or File &gt; Save in the top menu, or click anywhere in the open file and then press the Ctrl-S keys at the same time.\nTo render the saved RMarkdown/Quarto files to an output:\n\nIn RMarkdown, click the Knit button at the top of the open file in the Editor pane.\nIn Quarto, click the Render button.\n\nIn each case, the default output filetype will be created, such as a PDF or a HTML file (a webpage). Later on we’ll see change the type of output file you render to in Section 4.5.\nThe rendering process will be shown in the bottom-right pane of RStudio while it happens. Sometimes a problem will occur which prevents the file from rendering. Common reasons for this are covered in the Frequently Encountered Problems section: ?sec-common-render-issues.\nOnce rendering is finished, the file should open up automatically. A PDF will typically open in your computer’s default PDF viewer. An HTML output file will open up in your browser. If the output file doesn’t open automatically, you can open it manually by clicking on the file in the Files tab in the bottom-right pane of RStudio."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#text",
    "href": "src/book/04_rmarkdown_chapter.html#text",
    "title": "4  Literate Programming",
    "section": "4.3 Text",
    "text": "4.3 Text\nIf you look at Fig. X (rmd/qmd side-by-side), you can see that both Quarto and RMarkdown files are divided into two sections. At the top of each file is a header section, which begins and ends with a line of three dashes: ---. This is where we can specify settings for the file, which we will learn more about in Section 4.5.\nBelow that is the main section of the document. Here we can add text that will appear in our output when we render.\nWe can try adding text to a document to see what effect it will have on the output. On the left of Fig. X, we can see an RMarkdown and Quarto files with a brief header section (the same contents should work just as well in a Quarto file. Below that is a line of text that says Hello, world!. On the right are the outputs that we get when we render these files to HTML.\nIn the outputs, the text Hello, world! has been formatted as a paragraph of text.\n\nFigure: Side-by-side examples of RMarkdown and output, and Quarto and output (hello world).\nWhat about if we want to format contents as something other than regular paragraph text? For example, you might want section headings (like this book contains), or italics or bold text. In a program like Microsoft Word or Google Docs, you can click a button to apply these formatting styles.\nRMarkdown and Quarto files cannot contain this type of embedded formatting, because they are just text documents (i.e. the same thing as a file with a .txt extension). But we can specify that we want their output to contain formatting. We do this by including symbols in the input file that will be interpreted as formatting instructions when we render.\nFor example, we can create italicized text by putting the words inside a pair of asterisks *italicized text* or a pair of underscores _italicized_text_. The asterisks or underscores will not appear in the output, but the output will instead have the formatting you have specified.\nThis method of applying formatting in a text document is called Markdown. You may have encountered something similar online, as websites like Reddit and Discord allow you to use symbols like this to apply formatting. It is also where RMarkdown files got their name from.\nThere are a lot of different symbols we can use to format text with Markdown.\nAs mentioned above, we can use a pair of asterisks or a pair of underscores to italicize text. In the same way, we can use double asterisks or underscores to get bold text, and three to make text both italicized and bold.\n\n\n\nWrite this in your file\nTo get this when you render\n\n\n\n\n*italics* or _italics_\nitalics or italics\n\n\n\n\n\n\nWrite this in your file\nTo get this when you render\n\n\n\n\n\nRegular output\n\n\n\n\n\n*italics* or _italics_\n\n**bold** or __bold__\n\n_**bold and italicized**_\n\nitalics or italics\nbold or bold\nbold and italicized\n\n\nWe create section headings by putting the hashtag/pound symbol # at the start of a line that contains the text of a heading. Two hashtags create a sub-heading (e.g. for a subsection), three hashtags create a sub-sub-heading, and so on. Note that there should be no spaces before the hashtags (it should be the first character on a line), but there should be a space between the hashtags and the first character of the text of the heading.\n\n\n# Heading\n\n\n\n\n\n\n\n\n## Sub-heading\n\n\nSub-heading\n\n\n\n\n\n### Sub-sub-heading\n\n\nSub-sub-heading\n\n\n\nClickable links to webpages are created by putting the text of the link (what you will see) in square brackets, immediately followed by the URL (web address) of the webpage in parentheses. For example, here is a link to the Google homepage:\n\n\n[Link text](https://www.google.com)\n\nLink text\n\n\nWe can also create two types of list: unordered (i.e. bullet points) and ordered.\nAn unordered bullet point list is created by starting a line with an asterisk symbol * and then one or more spaces before the text of the item.\nAn ordered list is created by starting the line with a number followed by a period, and then one or more spaces before the text. You can also use roman numerals or letters of the alphabet to use different ordering symbols.\n\n\n* Bullet point item\n* Second item\n  * Nested list, indented by spaces\n  * Another item\n    - Another nested list, \n      with a different marker\nbecomes\n\nBullet point item\nSecond item\n\nNested list, indented by spaces\nAnother item\n\nAnother nested list, using a hyphen marker\n\n\n\n\n1. An ordered list\n2. Second item\n   i. A nested list that uses \n       roman numerals\n      a. A nested-nested list\n   ii. Back to the roman numerals\n   \n       Second paragraph of item (ii).\nbecomes\n\nAn ordered list\nSecond item\n\nA nested list that uses roman numerals\n\nA nested-nested list\n\nBack to the roman numerals.\nSecond paragraph of item (ii).\n\n\n\n\nOne important thing to note is that the number of spaces at the start of a list item matters.\nNested list items need to be preceded by the number of spaces to make them flush with the first character of the parent item’s main text.\nIf a list item has multiple paragraphs, then (1) these paragraphs need to be separated by a blank line, and (2) all paragraphs after the first need to be indented with enough spaces so that their first character is flush with the first character of the first paragraph’s main text.\n\n\n\n\n\n\nThe importance of lines and spaces\n\n\n\nIn general, text on consecutive lines will be bundled up into one line when you render Markdown into another format. For example,\nThis paragraph is written on two consecutive lines.\nIt will be compiled into a single paragraph because there is no blank line between them.\n\nThis will be a second paragraph, since there is a blank link between it and the previous text.\nThis can cause problems if you forget to put a blank line where you need it, e.g. before a heading line. For example,\nsome regular text\n### Important Heading\nbecomes\nsome regular text ### Important Heading {.unnumbered .unlisted}\nSpaces can also affect formatting.\nIf you forget a space between a heading’s hashtags and its text, you will render it as normal text (with hashtags).\n###Heading Text\nbecomes\n###Heading Text\nThe same thing can happen if you forget a space between a list symbol and the text of that list item.\nPutting four spaces at the start of a line will cause it to be rendered like code:\n    Indented by four spaces.\nbecomes\nIndented by four spaces.\nThis has its uses if you need to render something like code, but is not appropriate for regular text. It is not only harder to read than properly formatted text, but it also will not be wrapped if the line is long (i.e. a long line will just run off the right side of the page instead of continuing onto the next line.)\n\n\nThis brings us to the most important rule of writing and rendering Markdown based files:\n\n\n\n\n\n\nWarning\n\n\n\nAlways proofread the rendered output and check that the formatting is correct!\nYou can’t always tell how the rendered output will look just from the contents of your RMarkdown/Quarto file. It’s also very easy to make typos that will cause unintended formatting."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#sub-heading",
    "href": "src/book/04_rmarkdown_chapter.html#sub-heading",
    "title": "4  Literate Programming",
    "section": "Sub-heading",
    "text": "Sub-heading"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#code-chunks",
    "href": "src/book/04_rmarkdown_chapter.html#code-chunks",
    "title": "4  Literate Programming",
    "section": "4.4 Code chunks",
    "text": "4.4 Code chunks\nThe great power of RMarkdown and Quarto documents is that we can also include sections of code within the document. These sections are called code chunks.\nA code chunk begins and ends with a line of three backticks: ```. Note that a backtick is not a single quotation mark. On a US style keyboard it is the sumbol from the key in the top-left part of the keyboard.\nFor example:\n```\ncode goes here\n```\nBackticks alone will format the code chunk like code, but you will not be able to run the code.\nTo indicate the programming language and thus to enable the code to be run, we need to include curly braces around the programming languages symbol at the end of the opening line of backticks. For example, this is an R code chunk:\n```{r}\ncode goes here\n\nA major reason for using RStudio to write Rmd/Qmd documents is that you can run code chunks from within the file editor tab. Either click on the green \"Play\" triangle in the top right, or click within the code chunk so that the cursor is flashing on a line of code and then simulatenously press the following keys: Ctrl + Shift + R (Windows/Linux) or Cmd + Option + R (Mac).\n\nWhen you run a code chunk interactively from within the document, the code is sent to the Console to be executed, but any output is then displayed back below the code chunk. This allows us to preview what the output of a code chunk will look like without having to knit the entire document (which is a lot slower).\n\n::: {.cell hash='04_rmarkdown_chapter_cache/html/unnamed-chunk-10_6c65329346bf4a80133ca06a594aa16f'}\n\n:::\n\nBecause code in code chunks is sent to the Console, it runs in the same place (called an *Environment*) as non-code chunk code that you type directly into the Console itself. This also means that all code chunks share this same environment, and the results of one code chunk are available to another.\n\nFor example, if you create a R variable in one code chunk, a different code chunk can then use that variable.\n\nThere's just one problem: order matters. You have to run the code chunks in order. I.e. you would need to run the code chunk that creates the variable before you run the code chunk that references that variable. Otherwise the variable doesn't exist yet in the Environment and you will get an error.\n\nThere's a few ways that new R programmers run into errors due to this code chunk execution in the shared environment:\n\n1. You mix up the order of the code chunks within your file. You can run them interactively out of order, and so avoid the error, but when you try to knit you get an error. (This is because the knitting process runs the code chunks from scratch (in a new Environment) in order that they exist in the file.)\n\n2. You delete a code chunk that creates something another (not deleted) code chunk needs. That thing will stick around in your RStudio Environment, and so be availabe to any code chunks running interactively, but when you knit you get the same error as in the previous point (for the same reason).\n\n3. You restart RStudio after a break and your most recently written code chunk doesn't work. This is because anything R creates in the Environment is temporary. It will only last for as long as R or RStudio is running on the computer. When you restart RStudio and reopen a Rmd/Qmd file, you will need to re-run any code chunks whose output you need. (It's often easiest to just rerun all the code chunks, which you can do by clicking the \"Run\" button in the top right of the file editor pane and then selecting the last option to \"Run All\".)\n\n## Inline code\n\nWe can also include code within paragraphs of text. Let's say you calculated some important number in your analyses, and you want to discuss that number. You could type in the number by hand, but this is prone to errors where the calculated number changes for some reason (maybe you adjust the analysis's code, for example) but then you forget to update the number in the text.\n\nWe can instead just include the variable's value directly in the text using *inline code*, for example:\n\n\n\n````default\nThe answer to life, the universe, and everything is `r variable_name`.\nHere, we use a single pair of backticks around the code to indicate where it starts an ends, and an r immediately after the first backtick to indicate that this is R code. When the document is rendered, the value assigned to the variable called variable_name will be rendered within the sentence instead of the code. For example (if variable_name = 42),\nMy results showed that the answer to life, the universe, and everything is 42.\nWe can put more complicated expressions within the inline code backticks. For example, you could put a calculation to get the same result:\nMy results showed that the answer to life, the universe, and everything is `r 40 + 2`.\nor we could get the date at the time that the document is rendered by using the Sys.Date() function that is part of the R programming language:\nThis document was created on `r Sys.Date()`.\nwhich would be rendered as:\nThis document was created on 2025-03-11."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#sec-yaml-header-section",
    "href": "src/book/04_rmarkdown_chapter.html#sec-yaml-header-section",
    "title": "4  Literate Programming",
    "section": "4.5 The header section",
    "text": "4.5 The header section\nWe may wish to customize how our input document is rendered to an output file, instead of using the defaults. For example, you may wish to change the font size, or make all website links blue.\nIn both RMarkdown and Quarto documents we do this in the header section. The general structure of a header section is similar in both cases, but the details can vary.\nThe header section occurs before the actual contents of the document (i.e. before the text and code), and begins and ends with a line of three dashes: ---. You may have already seen a header section if you created a Rmd or Qmd in RStudio, because the example files that RStudio create come with simple header sections.\nA simple example of a header section might be:\n---\ntitle: \"Computing and Data for Scientists\"\nauthor: \"Dominic White\"\n---\nThe header section is written in another markup language called YAML.2 YAML uses a different format to the Markdown that we used for writing regular text. YAML is much better suited to writing a list of options, such as the rendering options you want to apply to your output file.\nThere are a few things to note about header sections:\n\nThe example header section above would work in either an RMarkdown or Quarto file, because both have a title and author option. However, many of the options are different, either because they only exist in one of the two file types or because they are called different things!\nFor example, in both document types we can specifif the type of output file we want to render to.\nThis is how we would could modify the example header section above to render to a PDF from an RMarkdown file:\n---\ntitle: \"Computing and Data for Scientists\"\nauthor: \"Dominic White\"\noutput: pdf_document\n---\nAnd this is how we would write the same thing in a Quarto file:\n---\ntitle: \"Computing and Data for Scientists\"\nauthor: \"Dominic White\"\nformat: pdf\n---\nSome options will be shared between all output file types (like title and author`), but other options are specific to a particular type of output file.\nFor simplicity, let’s consider just RMarkdown. We can specify multiple output formats with their own options (the first format will be the default), e.g.\n---\ntitle: \"Computing and Data for Scientists\"\nauthor: \"Dominic White\"\noutput: \n  pdf_document:\n    keep_tex: true\n  html_document:\n    theme: cosmo\n    code_folding: hide\n---\nWhat does all this mean?\nFirst we have specified options for rendering to both PDF (pdf_document) and HTML (i.e. web pages, with html_document).\nkeep_tex is a PDF specific option. A .tex is an intermediate filetype that is created from the RMarkdown file, which can then be rendered as a PDF. By default, that intermediate file is deleted once the PDF is created. Here we’ve set an option to keep it instead.\nWe’ve also set two html specific options. theme allows us to pick a style for our webpage (e.g. colors, font, etc.) from several pre-existing options. code_folding will hide the code of any code chunks, but create a button that can be used to expand the code (this is not something we could do in a PDF, since PDFs are static).\n\nThese examples also highlight a bit of YAML formatting syntax (applicable in both Rmd and Qmd files).\n\nAll of our options above have been specified in the format of key: value where key is the name of the option and value is what we set it to.\nWe can nest options under other options as we did with the format specfic options. In that case, we need to indent those nested lines two more spaces that the parent:\nkey: \n  value:\n    nested_key: nested_value\nSometimes we want to provide a list of values to an option. For example, in RMarkdown header sections, we can provide multiple authors as a list to the author option.\nWe either enclose them in square brackets:\nauthor: [\"Joe Bloggs\", \"Anna Smith\"]\nor list them on separate lines (indented 2 spaces, and starting with a hyphen):\nauthor: \n  - \"Joe Bloggs\"\n  - \"Anna Smith\"\n\nA detailed list of header sections options for RMarkdown can be found in the book RMarkdown: The Definite Guide (available at https://bookdown.org/yihui/rmarkdown-cookbook). For Quarto, the official documentation at https://quarto.org provides a similar list."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#images-figures",
    "href": "src/book/04_rmarkdown_chapter.html#images-figures",
    "title": "4  Literate Programming",
    "section": "4.6 Images & figures",
    "text": "4.6 Images & figures\nIf you have an image in a separate file that you wish to include in the rendered output, then you can do so using the following Markdown syntax:\n![](image_file_name.jpg)\nYou will note that this is very similar to the syntax for a web link (square brackets then parentheses), but with an exclaimation mark at the front.\nWe can add alternative text within the square brackets like this:\n![description of image](image_file_name.jpg)\nAlternative text is a description of the image that will be displayed if the image can’t be loaded. It’s a good idea to include alternative text even if you don’t think that’s a problem, because alternative text is used by blind or visually impaired users to understand what’s in an image.3\nWe can also include images using R code in a code chunk. This makes use of the knitr package:\n```{r}\nknitr::include_graphics(\"image_file_name.jpg\")\n```"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#tables",
    "href": "src/book/04_rmarkdown_chapter.html#tables",
    "title": "4  Literate Programming",
    "section": "4.7 Tables",
    "text": "4.7 Tables\nYou can create Markdown tables using the following syntax:\n| column 1 heading | column 2 heading  |\n| ---------------- | ----------------- |\n| row 1 content    | other content     |\n| row 2 content    | other row 2 stuff |\nwhich creates a table like this:\n\n\n\ncolumn 1 heading\ncolumn 2 heading\n\n\n\n\nrow 1 content\nother content\n\n\nrow 2 stuff\nother row 2 stuff\n\n\n\nA few things to note:\n\nVertical bars | divide columns. The hyphens --- on the second line of the table divide the headings row from the data rows.\nNice formatting is not required by Markdown, but it helps anyone reading the source file understand the text. However, you are not required to align the column separators | on each line, nor do you need more than one hyphen below each heading.\nFor example, this is a valid way of writing the same table as before:\n| column 1 heading | column 2 heading |\n|-|-|\n| row 1 content | other content |\n| row 2 stuff | other row 2 stuff |\n(Although you would hopefully agree that it’s much harder to understand!)\nYou can adjust the alignment of content in each column (aligning it either to the left, the right, or centered) using colons : in the line of dashes.\nFor example:\n| column 1     | column 2 | column 3      |\n| :----------- | :------: | ------------: |\n| left aligned | centered | right aligned |\nThe contents of a table cell have to be on a single line. You cannot put a linebreak in the source file to make it easier to read.\n\nTables can also be created by code chunks, typically by including a line of code that returns a dataframe, e.g.:\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nHere we are using the head() function to retrieve just the first six rows of the iris dataframe.\nWe could have included a line of code that was just the dataframe name:\n```{r}\niris\n\nHowever this would print out the entire dataframe, which has 150 rows! Since large dataframes make output documentshard to read, you generally don't want to display the original data.\n\n\n## Math \n\nWe often want to include mathematical equations or other notation in scientific documents. Markdown does not include support for this, but RMarkdown and Quarto both allow us to include maths written in Latex notation.\n\nTo do so, we put the math between dollar symbols: `$`.\n\nA single pair of dollar symbols allow us to write *inline* math, within a paragraph of regular text, for example:\n\n\n````default\nThe formula for the area of a circle is $A = \\pi r^2$.\nwhich becomes\nThe formula for the area of a circle is \\(A = \\pi r^2\\).\nYou may also want include a block of math as it’s own entity, separate from the rest of the text. To do so, we use a pair of double dollar symbols, e.g.\n$$A = \\pi r^2$$\nwhich becomes\n\\[A = \\pi r^2\\]\nNote how the math block is centered.\nHere are some useful mathematical symbols for various operations in Latex, and what they render as in the output document.\n\n\n\nOperation\nLatex symbols\nExample\n\n\n\n\nArithmetic\n+, -, =\n$4 + 2 - 1 = 5$\n\n\nBasic division\n/\n$1/2$\n\n\nFancy division\n\\frac{...}{...}\n$\\frac{1}{2}$\n\n\nGreek letters\n, \n$\\alpha \\pi$"
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#sec-visual-editor",
    "href": "src/book/04_rmarkdown_chapter.html#sec-visual-editor",
    "title": "4  Literate Programming",
    "section": "4.8 Visual editor",
    "text": "4.8 Visual editor\nSo far we have written the contents of our input file in plain text, with markup symbols to indicate what the rendered output should look like. This can take some getting used to, especially if you are more familiar with word processing software like Microsoft Word or Google Docs where you can see the formatting as you edit the document. It can also sometimes be fiddly to edit markup symbols.\nTo help, RStudio includes a visual editor in which you can get a live preview of what your document will look like as you write it.\nTo switch to the visual editor when you have a source file open in RStudio, click the Settings dropdown (the white cog wheel) at the top of the file editor pane and select the first option (Use Visual Editor).\nThe visual editor can be useful if you find it easier to work with a visual representation of the output document. However, you should remember a few things when working with the visual editor:\n\nThe underlying file is still a RMarkdown or Quarto file.\nThe visual editor view is what the output will look like if rendered to HTML (e.g. to a webpage). If you render to a different output once you are finished (e.g. to a PDF), the output might look slightly different.\nThere may be times when you will need to switch back to the Source mode to edit the underlying source file, e.g. if you are trying to add some sort of advanced content that is not supported by the visual editor."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#rmarkdown-versus-quarto",
    "href": "src/book/04_rmarkdown_chapter.html#rmarkdown-versus-quarto",
    "title": "4  Literate Programming",
    "section": "4.9 RMarkdown versus Quarto",
    "text": "4.9 RMarkdown versus Quarto\nAside from the differences covered in the previous sections:\n\nRMarkdown has an extensive ecosystem of extensions developed over its long history. Quarto can be extended, but seems to be designed to contain most features within the main system (and because it’s newer, has a smaller (albeit growing) set of extensions).\nBoth can support multi-file projects (e.g. if you wanted to write a book, with a separate source file for each chapter), but Quarto knits each source file separately, whereas RMarkdown make the code and results of each source file available to later files in the rendering process.\nFor example, if you were writing a multi-file book and you create a variable in the Chapter 1 file, you can then access it in a separate Chapter 2 file if you are using RMarkdown. In Quarto, you would need to repeat all the code to recreate the same variable in difference chapter files.\nPersonally I have found Quarto to be a little less flexible and more prescriptive, but not deal-breakingly so. It has some nice new features that are not standard in RMarkdown, and is being actively developed and improved (whereas RMarkdown is relatively stable at this point).\n\nThis book is written principally in Quarto, but due to the similarities, it’s easy to switch between them, or even include RMarkdown files within Quarto files (which this book does).\n\n\n\n\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "src/book/04_rmarkdown_chapter.html#footnotes",
    "href": "src/book/04_rmarkdown_chapter.html#footnotes",
    "title": "4  Literate Programming",
    "section": "",
    "text": "or click the New File icon in the top toolbar↩︎\nIf you are wondering how many markup languages this book is going to require you to learn, you may be dismayed to learn that YAML stands for Yet Another Markup Language. Also, two. And you don’t need to learn much YAML.↩︎\nSuch users may read your document with screen reading software that reads the contents of the file out loud, including the alternative text that accompanies any images.↩︎"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#the-good-the-bad-and-the-ugly",
    "href": "src/book/05_visualization_chapter.html#the-good-the-bad-and-the-ugly",
    "title": "5  Visualizing Data",
    "section": "5.1 The Good, the Bad, and the Ugly",
    "text": "5.1 The Good, the Bad, and the Ugly\nThis video will introduce you to the importance of visualization in data science, and how it can be used effectively (or ineffectively).\n\n\nSlides: PDF\nThere are many ways that we can visualize the same data. These pictures of data are called “graphs”. Some of these ways may be good, but many of them will be bad.\n\nA picture is worth thousand words…\n\n…and a good graph is clearer than a table of data.\nIn pre-modern times, our ancestors did not have to worry about tables of data (or words). But they did have to interpret patterns in what they saw.\nWe are stuck with the same brains, and so while a column of numbers doesn’t mean much, if we can transform that column into a picture then patterns can become much clearer.\nWhy create graphs?\n\nTo quickly understand patterns in the data.\nTo spot problems in the data (outliers, or checking that data makes sense with your expectations)\n\n\n5.1.1 Example: Challenger Disaster\nIn 1985 the space shuttle Challenger exploded shortly after launch, killing all the astronauts on-board. An investigation found that this was probably because a rubber seal had become too cold and brittle, due to cold weather on the launch day.\nThe investigation also found that although the managers and engineers knew that this was a potential issue, they had decided to go ahead with the launch anyway. But the engineers presented their concerns to their managers in the form of tables of numbers like this. Would you have looked at this and thought “Clearly we should not launch today?”\n\n\n\nTable of O-ring damage presented to NASA by Morton Thiokol’s engineers\n\n\nVisualization expert Edward Tufte suggested that clearer methods of presenting the data, such as a graph like this one, could have made a stronger case for delaying the launch:"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#data-in-r",
    "href": "src/book/05_visualization_chapter.html#data-in-r",
    "title": "5  Visualizing Data",
    "section": "5.2 Data in R",
    "text": "5.2 Data in R\nBefore we begin to make our own graphs, we need to learn some terminology for describing the underlying data.\n\n\n\n\nSlides: PDF\n\nWe usually want to store numerical data in tables, just like you might do with spreadsheet software like Microsoft Excel or Google Sheets. For example, instead of the separate x_coords and y_coords vectors from the graph in the previous sections, we would often be working with these as separate columns of a single table:\n\n\nTable 5.1: Table of our 2 points\n\n\nx_coords\ny_coords\n\n\n\n\n1\n2\n\n\n3\n4\n\n\n\n\nBecause the R programming language is designed for analyzing data, it comes with a built-in data structure for storing tables of data: the dataframe.\nWe will learn how to create dataframes later in this book. For now, I will just give you pre-existing dataframes that we can analyze. If you want to follow along with these examples in RStudio, you can run this line of code in the Console:\nThat creates an R variable called df that holds the table of our two points. If we enter the name of the variable at the Console, R will print out the dataframe stored in the variable:\n\ndf\n\n  x_coords y_coords\n1        1        2\n2        3        4\n\n\nAs you can see, this contains the same data as the table above, albeit not formatted quite as nicely (the default formatting of dataframes in R is not particularly pretty, but that is another thing we will learn to do in the future!).\nOur dataframe is broken down into various parts:\n\nEach row represents an observation of some thing. For example, the first row represents measurements taken from one thing, and the second row represents measurements taken from a different thing.\nEach column represents a different variable, or quantity that we are measuring.\n\nNote that we are reusing the word variable here. We have already encountered one type of variable in the R programming language, as a way to store and reference objects and data in our computer.\nNow we have encountered a new use: variables as measurements from the real world, which exist as a columns in a table.\nThese two concepts of a variable are different, even though we use the same word, “variable” to refer to each of them. I will be using “variable” for both of them throughout this book, and you will need to deduce from the context whether I am talking about a programming variable in R or a column variable that measures something in the real world. (If it could be ambiguous, I might refer to an “R variable” or a “column” to prevent confusion.)\nThe columns of an R dataframe are actually vectors! I.e. a dataframe is essentially just a bunch of vectors of the same length. This means that the rules that apply to a vector also apply to a dataframe column. For example, a dataframe column can only contain one type of data (e.g. just numbers, or just character strings), just like a vector.\nWe can get a single column from a dataframe using the $ operator after the dataframe’s variable. For example, to refer to the x_coords column from our df dataframe, we could write:\n\ndf$x_coords\n\n[1] 1 3\n\n\nAnd we get back a vector of the two values from that column!\n\n\nWe have already talked about the specific types of data that the R programming language is aware of, such as numbers, character strings, and Boolean values.\nBut there are also more abstract concepts of data that a column variable can contain. For example:\n\nContinuous variables: we say that a column variable is continuous if it can feasibly be any number. For example, measurements of the height of different humans is continuous, because you could be 200cm tall, or 201cm tall, or 200.5cm tall, or 200.75cm tall (etc.). The values between any two values are also valid values for a continuous variable to take on.\nDiscrete variables: we say that a variable of numbers is discrete if it can only contain particular values. For example, the year is discrete, because it could be 2023 or 2024, but it cannot be 2023.5.\n\nCount variables are a particular sub-type of discrete variable. As you might guess from the name, this is when the variable indicates how many there are of something.\n\nCategorical variables contain labels, text, or something other than numbers. For example, colors (red, blue, green, etc.) would be categorical values.\n\nCategorical variables can be unordered, such as the color example, or…\nThey can be ordered, in which case we refer to them as ordinal variables. For example, if a variable of ratings can contain the values “good”, “average”, or “bad”, then this would be considered an ordinal variable since there is an ordering to these values.\n\n\nHere is how these different abstract types of variables could be concretely recorded in R in the columns of a dataframe (note that we have not yet encountered all of these R data types):\n\n\n\nType of variable\nType of R data\n\n\n\n\nContinuous\nNumerical\n\n\nDiscrete/count\nNumerical, integer (p. ??)\n\n\nCategorical\nNumerical, character, factor (p. ??)"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#one-numerical-variable-histograms",
    "href": "src/book/05_visualization_chapter.html#one-numerical-variable-histograms",
    "title": "5  Visualizing Data",
    "section": "5.3 One numerical variable: histograms",
    "text": "5.3 One numerical variable: histograms\nHow can we visualize a single column of data by itself? And why would we want to do this?\n\nLet’s use a dataset that is included in the tidyverse R package which contains information about different characters from the Star Wars franchise.\n\nYou can load the dataset in RStudio by loading the tidyverse package (which you installed in Section 3.10.1) by running library(tidyverse). The dataset will then be available to you as a dataframe stored in an R variable called starwars.\nHere is an example of the data stored in the starwars dataframe: ::: {.cell hash=‘05_visualization_chapter_cache/html/unnamed-chunk-5_069e7b90eee93c4d5945aac111f22bcd’}\nstarwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 &lt;NA&gt;    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 &lt;NA&gt;    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 &lt;NA&gt;    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n:::\nWe might be interested in visualizing the heights of different Star Wars characters. Maybe we want to know if there are lots of tall characters and not many short ones.\nA graph of a single variable shows us the distribution of that variable. A good type of graph to visualize the distibution of a single continuous variable (like the height column) is a histogram.\nHere is how we can create a histogram of the height column with the plot() function:\n\nhist(x = starwars$height)\n\n\n\n\nNote how:\n\nWe used the $ operator to select the height column as a vector of numbers to go on the x-axis.\nThe x-axis of heights has been broken up into a series of bars which we call bins.\nThe length of each bin is the number of observations (the “frequency”) that fall into that bin (i.e. the number of Star Wars characters with a height in the range of that bin).\n\nFor example, the tallest bin catches the characters with a height between about 160-180 centimeters (cm). There seem to be about 28 characters in the dataset with a height in that range. (If you prefer the old Imperial measurement system, then 2.5cm is about 1 inch, so 100cm = 3’4”, 150cm = 5’, 200cm = 6’8”, etc.)\n\n\nR has guessed the number of bins for us. Sometimes you will want to customize this, and we can override the default by setting the breaks parameter of the hist() function. For example, we can force the histogram to contain just 5 bins:\n\nhist(x = starwars$height, breaks = 5)\n\n\n\n\nR will recalculate the range of the new bins, and recount how many observations fall into each bin.\n\nExercise: Find the tallest bin in the 5-bin histogram. What is the range of heights of Star Wars characters in this bin? Approximately how many characters are in this bin? Recall that one of the purposes of a graph is to see if the data fits our expectations - does this make sense as the most common height range to find in the Star Wars movies?\nWe can also create a histogram with a lot of bins, e.g. 100 bins:\n\nhist(x = starwars$height, breaks = 100)\n\n\n\n\nPicking the right number of bins is a bit like Goldilocks tasting porridge: there’s a sweet spot in the middle, but it’s a bit subjective.\n\nIf we have too few bins then we destroy any patterns in the variable.\nIf we have two many bins then we see a jagged pattern caused by random variation rather than the true shape of the distribution.\n\nCompare these three histograms with different numbers of bins:\n\n\n\n\n\n\nThe middle histogram shows us that while humans (and similar-sized creatures) are predominant even a long time ago in galaxies far, far away, there is also a clump of shorter creatures with heights below 100cm.\nHowever, that shape is obscured in the plot on the left with just three bins. With too few bins, we destroy any fine grained shape in the distribution.\nIt is equally hard to see the overall shape in the right-hand histogram, because there are so many bins that our eyes are distracted by the sharp transitions between neighboring bins. However, these patterns are caused by random variation, i.e. by accidentally having a few more Star Wars characters in one bin."
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#one-categorical-variable-bar-charts",
    "href": "src/book/05_visualization_chapter.html#one-categorical-variable-bar-charts",
    "title": "5  Visualizing Data",
    "section": "5.4 One categorical variable: bar charts",
    "text": "5.4 One categorical variable: bar charts\n\nWe can also visualize the distribution of a categorical variable. We gain use a graph with bars, but whereas in a histogram we had to divide the x-axis into bins, we now have existing categories in the variable.\nWe can visualize the distribution by showing the number of values in each category (the frequency of each category).\nTo see how, let’s pick the sex variable from the starwars dataframe. Here’s what the first few rows looked like:\n\n\n\n\n\nname\nsex\n\n\n\n\nLuke Skywalker\nmale\n\n\nC-3PO\nnone\n\n\nR2-D2\nnone\n\n\nDarth Vader\nmale\n\n\nLeia Organa\nfemale\n\n\nOwen Lars\nmale\n\n\n\n\n\nThe first thing we need to do is count the number of values of each sex in the sex column. We can do this with the table() function:\n\ntable(starwars$sex)\n\n\n        female hermaphroditic           male           none \n            16              1             60              6 \n\n\nWe can then use the barplot() function to make a bar graph of those frequencies:\n\nh &lt;- table(starwars$sex)\nbarplot(h)\n\n\n\n\nNote that saving the table to a variable (which we have called h) is optional - we could instead nest the table() function inside the barplot() function like this:\nbarplot(table(starwars$sex))\n\n\n\n\n\n\nTip\n\n\n\n\n\n5.4.1 Nesting vs sequential functions\nWhen should you nest functions and when should you run them sequentially?\nIn general, nested functions can be harder to read. It’s fine if you only have two or three, but nesting more than that results in a lot of parentheses and arguments that are hard to sort out.\nWe can instead run each function one at a time, and save the output in a new variable. However, if we only want to use that variable once (in the next function), then we end up creating a lot of variables that we don’t really need.\nIn the next chapter we will learn about a new operator (called a pipe) that will allow us to run functions sequentially and avoid creating lots of intermediate variables."
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#two-variables-scatter-plots",
    "href": "src/book/05_visualization_chapter.html#two-variables-scatter-plots",
    "title": "5  Visualizing Data",
    "section": "5.5 Two variables: scatter plots",
    "text": "5.5 Two variables: scatter plots\nWhen we have two or more variables, we often want to see how these variables covary, i.e. how does the distribution of one variable differ at different values of another variable.\nIf we have two continuous variables, then we usually visualize their covariation with a scatter plot. You have probably seen graphs like tis before, in which we draw dots at meeting points of \\(x\\) and \\(y\\) values.\nIn R, we can create simple scatter plots with the plot() function, such as this graph of mass vs. weight of Star Wars characters:\n\nplot(x = starwars$height, y = starwars$mass)\n\n\n\n\n\nNote the two parameters that we pass arguments to, x and y, for the columns of data that we want to plot on the x and y axes respectively.\nWhen we talk about a scatter plot of “A vs. B”, by convention A is the variable that we are plotting on the y-axis. I.e. we would say that the graph above shows “mass vs height”, not “height vs. mass”.\nA final convention is that the x-axis contains the variable which we think probably explains the other one. I.e. I think it makes sense to think as height causing changes in mass, not mass causing changing in height, since an increase in height tends to make you weigh more. However, as middle age has taught me, an increase in mass does not necessarily lead to a change in height…"
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#trends-line-graphs",
    "href": "src/book/05_visualization_chapter.html#trends-line-graphs",
    "title": "5  Visualizing Data",
    "section": "5.6 Trends: line graphs",
    "text": "5.6 Trends: line graphs\nAnother type of graph that you may encounter for two continuous variables is the line graph.\nHowever, it is important to note that a line graph is not appropriate for many datasets.\nHere’s why. When we connect points with a line, we are implying that there is some connection between those points. Typically we connect points in the direction of the x-axis, going from left to right. By drawing a line between, for example, the points (1,1) and (2,3) we are implying that a particular thing had the y-axis attribute equal to 1 when x=1, and then when \\(x=2\\) that same thing had changed its y-axis attribute to a value of 3.\nIn other words, we should only draw a line graph to connect points if those points represent sequential observations of the same thing. A line graph shows us changes in values, whereas a scatter plot typically shows independently observed values of the same thing.\nIn would be bad, for example, to use a line graph to show the mass vs. height of Star Wars characters, because there is no reason to connect the point for Jabba the Hutt to the point for C-3PO1. These rows in the dataset are completely separate things, not two observations of the same thing. If we compare such a line graph to the previous scatter plot that we made, we can clearly see that the line graph looks awful:\n\n\n\n\n\nWe typically use line plots when we are measuring something repeated over time. Time is then the sequential value on the x-axis (because time in graphs always goes from left to right). A dataset that contains repeated observations of the same thing over time is called a time series.\nFor example, here is a scatter plot and a line graph showing the body temperature of a beaver over time. All the body temperature measurements are from the same beaver, so this is a time series and it makes sense to connect the points. In fact, it is actually easier to understand the line graph than a scatter plot of the same data, because it is hard for our brain to mentally draw the connecting lines in the scatter plot.\n\nbeaver_1_plus &lt;- beaver1 %&gt;%\n  mutate(\n    hour = time %/% 100,\n    minutes = time %% 100,\n    day_and_time = ddays(day-346) + dhours(hour) + dminutes(minutes)\n  )\npar(mfrow = c(1, 2))\nplot(\n  beaver_1_plus$day_and_time, \n  beaver_1_plus$temp, \n  xlab = \"Time (seconds)\", \n  ylab = \"Body temperature (Celsius)\"\n  )\nplot(\n  beaver_1_plus$day_and_time, \n  beaver_1_plus$temp, \n  xlab = \"Time (seconds)\", \n  ylab = \"Body temperature (Celsius)\", \n  type = \"l\"\n  )\n\n\n\npar(mfrow = c(1, 1)) # Back to the original graphics device\n\nWe can create a line graph using the plot() function with the additional argument type = \"l\", for example:\nplot(x = column_1, y = column_2, type = \"l\")\nWe supply the character \"l\" to the type parameter to indicate that we want lines in this graph."
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#sec-visualization-best-practices",
    "href": "src/book/05_visualization_chapter.html#sec-visualization-best-practices",
    "title": "5  Visualizing Data",
    "section": "5.7 Best practices for graphs",
    "text": "5.7 Best practices for graphs\nTODO: show some good and bad graphs to illustrate these points.\n\nCommunicate as much information as possible with as little ink as possible\n\n\nWhite space is good\nExcessive use of color is bad.\nFun and artistic infographics often obscure the actual data.\n\n\nCreate graphs that meet the expectations of the viewer, and be explicit when you do something different.\n\n\nAxis ranges (start at 0, or be clear if not. Don’t start elsewhere to exaggerate a small difference.)\nDirection of data on axis. (left to right, and bottom to top)\nTime on x-axis\nScales on subplots\n\n\nMake comparisons easy.\n\n\nLengths, not areas.\nSide-by-side comparisons - don’t make people move things mentally to see how they are different.\n\n\nLabel your graphs.\nAvoid 3D graphs like the plague."
  },
  {
    "objectID": "src/book/05_visualization_chapter.html#footnotes",
    "href": "src/book/05_visualization_chapter.html#footnotes",
    "title": "5  Visualizing Data",
    "section": "",
    "text": "Nobody suggest this to JJ Abrams…↩︎"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#wrangling-overview",
    "href": "src/book/06_wrangling_chapter.html#wrangling-overview",
    "title": "6  Wrangling Data",
    "section": "6.1 Wrangling overview",
    "text": "6.1 Wrangling overview\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#the-dplyr-package-and-the-tidverse",
    "href": "src/book/06_wrangling_chapter.html#the-dplyr-package-and-the-tidverse",
    "title": "6  Wrangling Data",
    "section": "6.2 The dplyr package and the tidverse",
    "text": "6.2 The dplyr package and the tidverse\n\nTBA: packages and loading with library()"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#the-presidential-dataset",
    "href": "src/book/06_wrangling_chapter.html#the-presidential-dataset",
    "title": "6  Wrangling Data",
    "section": "6.3 The presidential Dataset",
    "text": "6.3 The presidential Dataset\n\n6.3.1 Examining the data\n\nFor the first part of this chapter we will be using a dataset of US presidents. This dataset is stored in a variable called presidential.\n\n\n\nUse head() function to check some of the contents of the presidential dataframe. (The head() function prints the first six rows of a dataframe.)\nRun head(presidential) in the R Console and examine the output."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#picking-columns-with-select",
    "href": "src/book/06_wrangling_chapter.html#picking-columns-with-select",
    "title": "6  Wrangling Data",
    "section": "6.4 Picking columns with select()",
    "text": "6.4 Picking columns with select()\n\n6.4.1 The select function\nThe select() function can be used to pick certain columns of a dataset. The output of select() is a new dataframe containing just the columns that you specified.\nIgnore the video’s instruction to follow along in RStudio: you will try this function out on the next page of this tutorial instead.\n\n\nSlides: PDF\n\n\n6.4.2 A simple select()\n\n\n\nExercise TBM\n\n\n\n\n\n# Replace the blank with the name of the column\nselect(presidential, _____)\n\n\nselect(presidential, name)\n\n\ngrade_code(\"Nice work!\")\n\n\n\n\n6.4.3 Selecting multiple columns\nWe can put as many column names as we want into the select function, separating each by a comma.\n\n\n\nExercise TBM\n\n\n\n\n\n\n\n6.4.4 Selecting a range\nIn R, the range operator, : (the colon punctuation symbol) indicates a range.\nFor example, this code create a vector of all the integer numbers in the range of 1-10:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n(We could also have written c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) as you learned in the first week, but the range operator is a convenient shorthand in this scenario.)\nWe can also use the range operator to indicate a range of columns inside dplyr functions such as select:\nselect(presidential, name:end)\nThis selects all sequential columns from name to end (which in this case is name, start, and end).\n\n\n\nExercise TBM\n\n\n\n\n\n\nNote:\nYou combine ranges and individual column selections by separating them by commas, e.g.\nselect(presidential, name:start, party)"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#sorting-with-arrange",
    "href": "src/book/06_wrangling_chapter.html#sorting-with-arrange",
    "title": "6  Wrangling Data",
    "section": "6.5 Sorting with arrange()",
    "text": "6.5 Sorting with arrange()\n\n6.5.1 The arrange function\n\n\nSlides: PDF\n\n\n6.5.2 Arrange practice\n\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#piping-data-between-functions",
    "href": "src/book/06_wrangling_chapter.html#piping-data-between-functions",
    "title": "6  Wrangling Data",
    "section": "6.6 Piping data between functions",
    "text": "6.6 Piping data between functions\n\n6.6.1 The pipe %&gt;% operator\n\n\nSlides: PDF\nAs described in the video, the pipe operator takes the value on its left and inserts it as the first argument of the function on the right.\nIn other words:\nsome_data %&gt;% someFunction()\nis equivalent to:\nsomeFunction(some_data)\nIf there are other arguments supplied to the function, they get “pushed back” so that the data piped in can claim the “first argument” spot:\nsome_dataframe %&gt;% someFunction(this_is_really_argument_2)\nis the same as:\nsomeFunction(some_data, this_is_really_argument_2)\n\n\n6.6.2 Piping practice\n\n\n\nExercise TBM\n\n\n\n\n(Note that we can put a new line after the %&gt;% operator as above - R knows that there must be a right-hand side, so it treats both lines as the same line.)\n\nWhy does this matter?\nThe first argument of functions in the tidyverse is the dataframe. However, many functions also output a dataframe (as does a variable holding a dataframe, such as presidential). So we can just pipe from one function to another and build up a long chain of functions: i.e. a pipe:\nsome_dataframe %&gt;%\n  select(some_columns) %&gt;%\n  some_other_function() %&gt;%\n  a_third_function(different_argument)\n\nNote that I have invented some made-up functions and variable names in the code above - this is called pseudocode."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#boolean-logic",
    "href": "src/book/06_wrangling_chapter.html#boolean-logic",
    "title": "6  Wrangling Data",
    "section": "6.7 Boolean logic",
    "text": "6.7 Boolean logic\n\n6.7.1 Review of Boolean logic\n\n\nSlides: PDF\n\n\n6.7.2 Boolean logic quiz\nYou are already a little familiar with Boolean operators from the Introduction to R tutorial, so let’s refresh our memories with a quick quiz.\n\n\n\nQuiz TBA"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#picking-rows-with-filter",
    "href": "src/book/06_wrangling_chapter.html#picking-rows-with-filter",
    "title": "6  Wrangling Data",
    "section": "6.8 Picking rows with filter()",
    "text": "6.8 Picking rows with filter()\n\n6.8.1 The filter function\n\n\nSlides: PDF\n\n\n6.8.2 Some practice\n\n\n\nExercise TBM\n\n\n\n\n\n\n6.8.3 Filtering with multiple conditions\n\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#creating-columns-with-mutate",
    "href": "src/book/06_wrangling_chapter.html#creating-columns-with-mutate",
    "title": "6  Wrangling Data",
    "section": "6.9 Creating columns with mutate()",
    "text": "6.9 Creating columns with mutate()\n\n6.9.1 The mutate function\n\n\nSlides: PDF\n\n\n6.9.2 End year\n\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#sec-wrangling-groupby-summarize",
    "href": "src/book/06_wrangling_chapter.html#sec-wrangling-groupby-summarize",
    "title": "6  Wrangling Data",
    "section": "6.10 Grouping and summarizing",
    "text": "6.10 Grouping and summarizing\n\n6.10.1 The group_by and summarize functions\n\n\nSlides: PDF\n\n\n6.10.2 Practice\n\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#what-is-tidy-data",
    "href": "src/book/06_wrangling_chapter.html#what-is-tidy-data",
    "title": "6  Wrangling Data",
    "section": "6.11 What is tidy data?",
    "text": "6.11 What is tidy data?\nSo far we have looked at relatively simple data wrangling operations that return subsets of the data. However, we often want to reshape the dataframe to turn it into a format called tidy data. This video will introduce you to what tidy data looks like."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#converting-columns-to-rows",
    "href": "src/book/06_wrangling_chapter.html#converting-columns-to-rows",
    "title": "6  Wrangling Data",
    "section": "6.12 Converting columns to rows",
    "text": "6.12 Converting columns to rows\n\n6.12.1 The pivot_longer() function\n\n\n\n\n\n\n6.12.2 Practice\nLet’s try the pivot_longer() function on the presidential dataset. We will reshape this dataset to convert the two data columns (start and end) into rows, with a names column that indicates the name of the original column, and a values column that holds the dates.\nIn other words, we want to convert the presidential dataframe:\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\ninto this:\n\n\n\nname\ntype_of_date\ndate\nparty\n\n\n\n\nEisenhower\nstart\n1953-01-20\nRepublican\n\n\nEisenhower\nend\n1961-01-20\nRepublican\n\n\nKennedy\nstart\n1961-01-20\nDemocratic\n\n\nKennedy\nend\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\nThe data in both dataframes is the same, but we have changed the shape of the dataframe by converting columns into rows.\n\n\n\nExercise TBM"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#turning-rows-to-columns",
    "href": "src/book/06_wrangling_chapter.html#turning-rows-to-columns",
    "title": "6  Wrangling Data",
    "section": "6.13 Turning rows to columns",
    "text": "6.13 Turning rows to columns\n\n6.13.1 The pivot_wider() function\n\n\n\n\n\n\n6.13.2 Practice\nLet’s use the pivot_wider() function to undo the transformation we did earlier with the pivot_longer() function.\nI.e. we want to turn this:\n\n\n\nname\ntype_of_date\ndate\nparty\n\n\n\n\nEisenhower\nstart\n1953-01-20\nRepublican\n\n\nEisenhower\nend\n1961-01-20\nRepublican\n\n\nKennedy\nstart\n1961-01-20\nDemocratic\n\n\nKennedy\nend\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\nback into this:\n\n\n\nname\nstart\nend\nparty\n\n\n\n\nEisenhower\n1953-01-20\n1961-01-20\nRepublican\n\n\nKennedy\n1961-01-20\n1963-11-22\nDemocratic\n\n\n…\n…\n…\n…\n\n\n\n\n\n\nExercise TBM\n\n\n\n\n\n\n\n6.13.3 What happened to those dates?\nWhen we tried to reverse the transformation, we were not able to retrieve the start and end date columns back in the same format as we originally started with in the presidential dataframe.\nInstead of actual dates in the column, you should see values such as &lt;date [1]&gt;. This indicate that each cell of the table holds a list of dates instead of just a single date.\nYou code will also have generated a warning about this: Warning: Values are not uniquely identified; output will contain list-cols.\nWhat are these non-unique values that we are being warned about? If you look through the table created by pivot_wider(), you will notice that one president’s date list is longer than the others (see if you can find which president this is).\n\nIn fact, there were two US presidents with this same surname during the period of this dataset, and consequently, when we tried to widen the table and identify unique rows, we identified 2 start dates and 2 end dates for this presidential surname.\nTo avoid these problems when using pivot_wider(), we always need at least one column in the remaining non-widened columns that is unique for each row we wish to generate in our output. Here we fail that requirement, because one of the presidential surnames in our dataset is used by two seperate observations (presidents).\nBy default, pivot_wider() assumes that all remaining columns are unique for all widened rows. However, if there are only one or a few unique columns, these can be specified by supplying those column names to the id_cols argument of the pivot_wider() function."
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#splitting-and-combining",
    "href": "src/book/06_wrangling_chapter.html#splitting-and-combining",
    "title": "6  Wrangling Data",
    "section": "6.14 Splitting and combining",
    "text": "6.14 Splitting and combining\n\n6.14.1 The separate function\n\n\nSlides: PDF\n\n\n\n6.14.2 The unite function\n\n\nSlides: PDF\nThat’s it for this tutorial!"
  },
  {
    "objectID": "src/book/06_wrangling_chapter.html#other-data-wrangling-functions",
    "href": "src/book/06_wrangling_chapter.html#other-data-wrangling-functions",
    "title": "6  Wrangling Data",
    "section": "6.15 Other data wrangling functions",
    "text": "6.15 Other data wrangling functions\n\n6.15.1 Other helpful dplyr verbs\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#from-plot-to-ggplot-...",
    "href": "src/book/07_ggplot_chapter.html#from-plot-to-ggplot-...",
    "title": "7  Graphs with ggplot",
    "section": "7.1 From plot() to ggplot() + ...",
    "text": "7.1 From plot() to ggplot() + ...\nSo far we have created graphs with base R1 functions like plot() and hist().\nHowever the graphs created via these functions are not very visually appealing. It also becomes difficult to create more complex graphs, and the ways to do so are not very intuitive.\nIn this chapter we will learn how to use alternative graphing functions from the ggplot2 package. This is a core part of the tidyverse, just like the dplyr and tidyr packages that we learned about for data wrangling.\nFor example, instead of using hist() to create a histogram:\n\nhist(x = starwars$height, breaks = 10, data = starwars)\n\n\n\n\n…we will use an alternative pair of functions, ggplot() and geom_histogram(), like this:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height),\n    bins = 10\n  )\n\n\n\n\nAs you can see, the shape of the histograms are very similar. However, we’ve avoided this way of creating graphs up until now, because we’ve been trying to keep our code as simple as possible.\nNow we need to make the trade of slightly more complicated code in exchange for much better graphs. So let’s jump in and see how and why this works."
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#sec-ggplot-histogram-layer",
    "href": "src/book/07_ggplot_chapter.html#sec-ggplot-histogram-layer",
    "title": "7  Graphs with ggplot",
    "section": "7.2 Graphs as layers and transformations",
    "text": "7.2 Graphs as layers and transformations\nThe ggplot2 package gets its name because it’s based on a concept called the Grammar of Graphics (hence ggplot…). This is a fascinating topic that we will learn more about later in this book, but the key idea is that all data visualizations can be described using a common set of terms and ideas (i.e. a “grammar”). The terms of this grammar correspond to the functions and their parameters in the ggplot2 package.\nOne of the most important parts of this grammar are the layers that we can add to a graph.\nFor example, the ggplot() function by itself just creates a blank canvas, i.e. the base layer of the graph:\n\nggplot(data = starwars)\n\n\n\n\nWe can then add layers (created by other functions) by combining them with the + operator.\nTo get some kind of graph layer we need to take the data and map it to a geometric shape.\nFor example, if we wanted to map the height column of the starwars dataset to the geometric shape of a histogram’s bars, we could use this code:\ngeom_histogram(mapping = aes(x = height))\nA few things to note:\n\nThe geom_histogram() function is an example of what we call a geom function (each geom function specifies a different type of geometric shape).\nGeom functions have a parameter called mapping which tells R how to convert a column of data into a layer.\nThe argument that we pass to the mapping parameter is a function called aes() (this is short for “aesthetic”). Inside the aes() function we need to specify any parts of the histogram’s appearance that are determined by a column in the dataset.\n\nIn our example the x-axis of the histogram needs to show the height variable.\n\n\nWe then add this layer to the canvas created by ggplot() with the addition operator, +.\nAnd so our final code looks like this:\n\nggplot(data = starwars) +\n  geom_histogram(mapping = aes(x = height))"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#aesthetic-mappings-and-other-parameters",
    "href": "src/book/07_ggplot_chapter.html#aesthetic-mappings-and-other-parameters",
    "title": "7  Graphs with ggplot",
    "section": "7.3 Aesthetic mappings and other parameters",
    "text": "7.3 Aesthetic mappings and other parameters\nWe refer to this combination of the mapping parameter and its aes() argument as an aesthetic mapping, which is a fancy way of saying how we convert columns of the dataset into some visual representation.\nBut not every part of a graph is determined by data in the dataset. For example, there is no column in the starwars dataset that tells us how many bins this histogram should have. Instead we need to specify that number ourselves, with another piece of data (i.e. the number of bins).\nSince this number is not in the dataset, it is not an aesthetic mapping. Therefore we specify the number of bins as an argument of the geom_histogram() function, and not of the aes() function, like so:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height),\n    bins = 30\n  )\n\n\n\n\nDo you see how the geom_histogram() function has two parameters, mapping and bins, whereas the aes() function has a single parameter (x)?\n(We have written the geom function’s two arguments on separate lines to improve the readability of this code.)"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#sec-ggplot-scatter-plots",
    "href": "src/book/07_ggplot_chapter.html#sec-ggplot-scatter-plots",
    "title": "7  Graphs with ggplot",
    "section": "7.4 Scatter plots with ggplot()",
    "text": "7.4 Scatter plots with ggplot()\nHopefully you are beginning to see that the ggplot() syntax uses many of the same parameters as qplot(), just in different places.\nWe can create a scatter plot by switching to a geom function called geom_point() and passing an additional column to the aes() function to go on the y axis:\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = aes(x = height, y = mass)\n  )"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#color-and-fill",
    "href": "src/book/07_ggplot_chapter.html#color-and-fill",
    "title": "7  Graphs with ggplot",
    "section": "7.5 Color and fill",
    "text": "7.5 Color and fill\n\nSometimes we wish to highlight different categories of data in a graph. There are different ways that qwe could do this, but a common solution is to use color.\nFor example, let’s say that we want to investigate the relationship between mass versus height again, but also look at how it varies between genders.\nThe starwars dataframe conveniently contains a categorical gender column that tells us this information:\n\nstarwars |&gt;\n  select(name, mass, height, gender) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  name            mass height gender   \n  &lt;chr&gt;          &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;    \n1 Luke Skywalker    77    172 masculine\n2 C-3PO             75    167 masculine\n3 R2-D2             32     96 masculine\n4 Darth Vader      136    202 masculine\n5 Leia Organa       49    150 feminine \n6 Owen Lars        120    178 masculine\n\n\nWe can add an additional aesthetic mapping to our scatter plot from the previous section, which will map the gender column to the color aesthetic of the scatter point layer:\n\nggplot(data = starwars) +\n  geom_point(\n    mapping = aes(x = height, y = mass, color = gender)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly we can also color histograms - however the color parameter colors the outside of shapes:\n\nggplot(data = starwars) +\n  geom_histogram(mapping = aes(x = height, color = gender))\n\n\n\n\nThis graph is hard to interpret. It would be easier if we could color inside the shapes - fortunately we can do this using the fill parameter instead of color:\n\nggplot(data = starwars) +\n  geom_histogram(mapping = aes(x = height, fill = gender))\n\n\n\n\nNote that this still goes inside the aes() function, because we are still mapping the gender column, just now to the fill aesthetic.\nOne other thing to note is that, by default, ggplot2 will stack bars of different colors on top of each other. However, this makes it hard to compare the relative heights of the bars, in direct contravention of the graphing guidelines that we learned in Section 5.7.\nThus what we need to do is to change the position of the bars so that each bin’s bars overlap. We will also need to make them transparent so that we can see through to the bars behind. We can do this with the position and alpha parameters:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height, fill = gender),\n    position = \"identity\",\n    alpha = 0.4\n    )\n\n\n\n\nA few things to note:\n\nWe set position = \"identity\" to cause the bars to all start at the x-axis.\nWe set alpha = 0.4 to make the bars 40% transparent. You can set alpha anywhere from 1 (completely opaque) to 0 (completely transparent, i.e. invisible). Usually a value between 0.2-0.4 works well.\nBoth \"identity\" and 0.4 are values, not columns of the dataframe, so we need to supply these two new parameters to the geom function instead of the aes() function (i.e. we are not creating any aesthetic mapping to the data in starwars with these settings).\n\nIt is actually getting a little hard to interpret some of the overlapping bars in the plot above, because when too many colors overlap we get an indistinguishable grey mess. Coloring histograms woks best when we have only two (or maybe three) categories. More categories than that and we need to start considering whether a different type of graph might communicate the patterns in the data more clearly."
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#piping-to-ggplot",
    "href": "src/book/07_ggplot_chapter.html#piping-to-ggplot",
    "title": "7  Graphs with ggplot",
    "section": "7.6 Piping to ggplot()",
    "text": "7.6 Piping to ggplot()\nOne advantage of ggplot() over plot() is that we can pipe a dataframe to ggplot() because the first parameter of ggplot() is the dataset to be used.\nSo instead of explicitly passing the starwars dataframe to the data parameter:\nggplot(data = starwars) + ...\n…we could write this:\nstarwars %&gt;%\n  ggplot() + ...\nThis might not seem like much, but it does mean that we can put the graphing functions at the end of a series of piped functions that transform and wrangle our dataset, for example:\nsome_dataset %&gt;%\n  mutate(...) %&gt;%\n  filter(...) %&gt;%\n  ggplot() + geom_FUNCTION(...)\nNote how we use pipes to connect a series of sequential data wrangling steps leading up to ggplot(), but then after ggplot() we have to switch to the + operator because we are then adding layers together. Don’t try to pipe the output of ggplot() on to a geom function because it won’t work (because we are trying to combine things in the graph, not work on the output of the previous function)."
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#labeling-graphs",
    "href": "src/book/07_ggplot_chapter.html#labeling-graphs",
    "title": "7  Graphs with ggplot",
    "section": "7.7 Labeling graphs",
    "text": "7.7 Labeling graphs\n\nIt is good practice to label all the graphs we create. We can do this by adding the labs() function to a graph:\n\nstarwars %&gt;%\n  ggplot() +\n  geom_point(\n    mapping = aes(x = height, y = mass)\n  ) +\n  labs(\n    title = \"Mass vs. height of Star Wars characters\", \n    y = \"mass (kg)\", \n    x = \"height (cm)\"\n    )\n\nWarning: Removed 28 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNotice how:\n\nthe labs() function is a separate function that we add on to the graph as a separate layer with the + operator\ninside the labs() function, we can supply an argument to the title parameter to change the title, and the y and x arguments to change the labels on those axes.\n\nA good title should succinctly describe what is being plotted on the graph.\nAxis labels should indicate what the variable is, and what units it is measured in.\n\n\nWe can also update any legend label as well by providing the same parameter name that we used in the aes() function. For example, with fill:\n\nggplot(data = starwars) +\n  geom_histogram(\n    mapping = aes(x = height, fill = gender),\n    position = \"identity\",\n    alpha = 0.4\n    ) +\n  labs(\n    title = \"Histogram of heights of Star Wars characters\",\n    x = \"height (cm)\",\n    fill = \"Gender of character\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 6 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nNote that I have left the default count label on the y-axis of this histogram, since that is a perfectly adequate name and has no units.\nIn the title I have explicitly called out that this is a histogram, since there are other types of graphs that also use bars and so we can make it clear to any viewer what is going on here. (There was no need to do anything similar in the previous scatter plot, since it was clearly a scatter plot an is unlikely to be confused with a different type of graph.)\n\n\n\n\n\n\nUsing the labs() function, add a title and y-axis label to the boxplot of price that you created earlier (we will leave the x-axis label as its default, since that is adequate for this graph.)"
  },
  {
    "objectID": "src/book/07_ggplot_chapter.html#footnotes",
    "href": "src/book/07_ggplot_chapter.html#footnotes",
    "title": "7  Graphs with ggplot",
    "section": "",
    "text": "I.e. functions that come as part of R, and don’t need to be imported from an extra package.↩︎"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#what-is-eda",
    "href": "src/book/08_eda_chapter.html#what-is-eda",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.1 What is EDA?",
    "text": "8.1 What is EDA?\n\n\n8.1.1 Exploratory Data Analysis (EDA)\nExploratory data analysis, or EDA for short, is a vague, hard-to-define concept… but is also the activity that will occupy much of your time as a data scientist.\nHere’s the problem:\n\nWhen you first get a dataset, you don’t know very much about it. You probably choose the dataset because you thought it might help you solve a particular problem, but you don’t know how (or if) the data can help you do that, or what other nuances might be present in the dataset.\nSo, the first step is to do a little digging: open the data up, and:\n\nPlot a few basic graphs (EDA is mostly done with graphs because they make it very easy to spot patterns).\nCalculate some simple statistics (e.g. averages and ranges)\n\nThis basic analysis will probably give you some ideas about the data. You’ll probably spot some patterns that bear further investigation, or maybe some issues that you’ll need to address.\nHow do you do this? More analysis, visualization, and general experimentation, which in turn will reveal more potential patterns, and so the cycle continues…\n\nYour end goal is to find some interesting questions in the dataset that deserve some kind of follow-up.\nDoes this seem all seem vague and open-ended? That’s because it is fundamentally a creative activity. There’s no one “right way” to create a painting or a book. Exploratory data analysis is the same. And just like those other creative activities, the way to get better is to practice.\n\n\n8.1.2 Types of variation to explore\nHopefully you are now convinced why graphing data is important. But what type of graphs should you create?\nThe rest of this tutorial will go through different types of graphs, but here are some general concepts to bear in mind:\n\nWe can broadly divide EDA into two types: univariate and multivariate\n\nUnivariate, or “one variable”, analysis looks at the variation within a single variable’s values. Our goal here is to understand the distribution of this variable.\nMultivariate (i.e. “many variables”) refers to analysis of 2 or more variables where we want to examine the covariance between those variables. In other words, how does one variable change in response to changes in another?\n\nAs a general principle, you should do univariate EDA first before multivariate EDA.\nWe have put together a flow chart to help you pick the right type of graph for particular types of data and analysis goals: available as a PDF here."
  },
  {
    "objectID": "src/book/08_eda_chapter.html#starting-eda",
    "href": "src/book/08_eda_chapter.html#starting-eda",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.2 Starting EDA",
    "text": "8.2 Starting EDA\nWhen you begin EDA on a dataset, it’s a good idea to proceed in the following order:\n\nUnderstand why you are exploring the data.\nMake sure you understand the dataset.\n\n\nLoad the data and check the dataset’s documentation.\nLook at the first and last rows of your dataframe.\nMake sure the data makes sense.\n\n\nStart the EDA cycle.\n\n\n8.2.1 Understand your why\nIt’s generally a good idea to go into EDA with a question in mind.\nThe question doesn’t have to be perfect - indeed, you might well decide that the question can be improved after you’ve explored the data.\nHowever, having a question in mind allows us to focus our EDA and not get distracted by the huge number of possibilities that we can explore with a single dataset.\nSo: what makes a good question?\nIn general, we want:\n\nto be as specific as possible,\nwhile still being interesting.\n\nSpecificity is good because it makes our analysis shorter and more direct. Broad questions are often of more general interest, but it is harder to come up with an approach to answering them.\nAn analogy is a New Year’s resolution: consider “Exercise more” vs. “Go for a 30 minute run 3x per week.” One of these is specific and measurable (while still being relevant to the overall goal of increasing your fitness), while the other is much broader and less-defined (and potentially easier to weasel out of…).\nFor this tutorial, our starting question will be:\n\nWhat is the effect of the size of a diamond on its price?\n\nThis is a better starting place that a broader question such as “What factors influence diamond prices?” Where would we start with such a question? Not only would we want data on the physical characteristics of diamonds, but probably also on the global supply of diamonds, the controls imposed by diamond cartels, the effect of advertizing to make diamonds seem exclusive, popular campaigns against blood diamonds… we could spend all day just thinking about the question, and never get to the actual EDA!\nCommon types of EDA questions are:\n\nWhat is the distribution of a variable?\nWhat are the relationships between two or more variables?\nAre there outliers?\nAre there any unusual patterns or artefacts in our data that might reduce its usefulness?\n\n\n\n8.2.2 Understand the dataset\n\n8.2.2.1 Load the data and read the documentation\nFor this tutorial, we will be using the diamonds dataset, which is automatically loaded with the ggplot2 graphing package.\nThis means that you just need to run library(ggplot2) or library(tidyverse) (the tidyverse meta-package includes ggplot2), and the diamonds dataset will be available for you to use.\nIn the real world, loading data tends to be a much messier task (a topic for another day: Chapter 13).\n\n\n8.2.2.2 Check the dataset’s documentation\nMost datasets come with documentation describing the data that they contain.\nIn R, most datasets that come as part of a package have some kind of documentation page. In RStudio, you can bring this up by running ?name_of_dataset in the console, e.g. ?diamonds. That won’t work in this tutorial, but you can also see the diamonds documentation page online here: https://ggplot2.tidyverse.org/reference/diamonds.html\n\n\n8.2.2.3 Get an overview of the data\nWe can use the glimpse function to get a compact overview of a dataframe.\nNote: the glimpse function is a modern “tidyverse” version of an older R function called str.\n\n\n\nWe can understand use the glimpse() function by passing a dataframe as the argument to the function.\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\n\n\nRun the glimpse function to get an overview of the columns in the diamonds dataset.\n\n\n\n\n8.2.2.4 Look at the first and last rows of your dataframe\nSometimes dataset creators start with grand intentions, but motivation or reality gets in the way. Because of this, the data at the top of datasets often looks great. However, the last rows in the dataset may contain incomplete or problematic data that got added on at the end.\nWe can examine both the first and last rows of the dataset with the head and tail functions respectively. The syntax of these functions is head(dataset_name), e.g.:\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nto display 6 rows from the top (or bottom) of the dataset. We can change the number of rows to display with the second argument:\n\nhead(diamonds, 10)\n\n# A tibble: 10 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n\n\nWe could also rewrite the previous code chunk using the pipe operator:\n\ndiamonds %&gt;%\n  head(10)\n\n\n\n\n\n\nAs you can see, the diamonds data contains complete and nicely formatted data at both the top and the bottom, as you might hope for an “offical” dataset provided with the ggplot2 package. The example datasets that come with packages are often nice like this - unfortunately real-world data is usually less user-friendly.\n\n\n8.2.2.5 Make sure that the data makes sense\nWe expect certain things about our dataset to be true:\n\nDoes the dataset have the correct number of rows and columns?\nDo the numbers in each column make sense:\n\ndoes a column called date contain dates, or does it have random numbers that don’t seem to be dates?\ndoes a column called USA_states contain all 50 states as you might expect? Or does it also contain “non-state” regions such as the District of Columbia and Puerto Rico.\n\nWhat is an observation in the dataset?\n\nIn the diamonds dataset, each row represents a different diamond. This is probably what you would expect. But what if the dataset rows each represented a diamond store instead, and the numbers were the average of all the diamonds in that shop? In this scenario, the data (and what questions we can ask with it) would be very different.\n\nAre the values plausible?\n\nFor example, in the diamonds dataset, are the prices for the diamonds realistic? Is the range of values of the carat variable realistic (for example, most diamonds are pretty small, so if this dataset claimed to have data on 50,000 diamonds that were all 100 carats or larger, then we might be a little suspicious…)\n\n\n\n\n\n8.2.3 Start the EDA cycle\nNow that you’ve verified the data, you can start on the EDA process.\nAs a reminder, we usually want to start with univariate analysis of each variable separately. Programs and data are both complicated, so it is always a good idea to start small and simple, and add in complexity gradually.\nIf we do the opposite and start complicated, it is easy to make small mistakes and assumptions which might make our final conclusions wrong!"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#univariate-variation",
    "href": "src/book/08_eda_chapter.html#univariate-variation",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.3 Univariate variation",
    "text": "8.3 Univariate variation\n\n8.3.1 Non-graphical methods (i.e. summary statistics)\nFor a continuous variable, we can calculate a few statistics (a number representative of other numbers) to summarize the:\n\ncenter (i.e. mean, or median), &\nspread (e.g. the standard deviation, the range, the interquartile range, the minimum and maximum, etc.)\n\nof a distribution.\nAn easy way to do this is with the summarize() function that we learned about in Section 6.10.\ndataframe %&gt;%\n  summarize(\n    mean    = mean(continuous_variable),\n    median  = median(continuous_variable),\n    std_dev = sd(continuous_variable),\n    iqr     = IQR(continuous_variable),\n    min     = min(continuous_variable),\n    max     = max(continuous_variable)\n  )\n\n\n\n\n\n\nIn our original question, we were interested in two variables: size (i.e. carat) and price. We should therefore calculate summary statistics for the price variable as well\n\n\n\n\n\n\nIf a variable is categorical, we can summarize it by looking at the proportions of observations in different categories. To do this we will first use the group_by() to create groups of the same variable that we want to summarize:\ndataframe %&gt;%\n  group_by(categorical_variable) %&gt;%\n  summarize(\n    count = n(),\n    proportion = n() / nrow(.),\n    percentage = 100 * proportion\n  )\nNotes on this code:\n\nthe n() function returns the number of rows within each group. It does not need an argument.\nthe nrow() function also calculates the number of rows, but needs an argument (a variable whose rows it should count).\n\nthe . argument inside nrow is used to refer to the original dataframe that we piped in.\n\nThus nrow(.) will give us the total number of rows in the original dataframe (ignoring the groups that we created).\n\n\n\n\n\n\n\n\n\n8.3.2 Graphical analysis of carat\nWith graphs, we want to visualize the distribution of a variable.\nOne good graph for visualizing the distributions of one continuous variable is a histogram.\nWe have already learned how to create ggplot histograms using the geom_histogram() layer (in Section 7.2). For example, this code would create a histogram of the depth column from the diamonds dataframe:\n\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = depth))\n\nHowever, there are other types of graph that can also be used to visualize a distribution, such as a box plot.\nIn the following picture, we can see how these graphs show the same distribution, just in different ways:\n\n\n\n\n\n\n\n\n\n\n\nPlots of distributions show common versus less common values of a variable. This allows us to answer questions like:\n\nWhat values are common? Does this match your prior expectations?\nWhat values are uncommon? Again, does this match your prior expectation?\nDoes the overall appearance show any unusual patterns that bear further investigation?\n\nOne thing you might note is that the distribution is not very smooth. To make it smoother, we could increase the size of the bins. However, to see what’s going on with this distribution, let’s decrease the bin size.\n\n\n\n\n\nInteresting! There’s an odd sawtooth pattern to this data that suggests several clusters to the data. This pattern raises several interesting questions:\n\nWhat’s causing these groups (“clusters”)?\nWhy does each cluster have a sharp left-hand side, and a long tail on the right?\n\n\n\n8.3.3 price visualization\nWe also want to look a the distribution of the price variable. Let’s do that with a new type of graph: the box plot.\nBox plots (also called box-and-whisker plots) visualize not only the distribution but also show several summary statistics:\n\n\n\n\n\nWe create a box plot using the geom_boxplot geom function. There are two required aesthetic mappings in a box plot (note that in ggplot the orientation of the box plot is rotated 90 degrees):\n\nx should be a categorical variable to create different box plots for.\ny is the variable that we want to show the distribution of.\n\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_boxplot(mapping = aes(x = price))\n\n\n\n\nOne slightly strange feature of the default box plot is that ggplot will put some meaningless numbers on the y-axis. We can get rid of these by mapping the y aesthetic to an empty character string:\n\nggplot(data = diamonds) +\n  geom_boxplot(mapping = aes(x = price, y = \"\"))\n\n\n\n\nThis still leaves us with an unnecessary y axis label, but we can also get rid of that with the labs() function.\n\n\n\n\n\n\n\n8.3.4 Violin plots\nAnother graph that shows the distribution of a single variable is the violin plot. This combines aspects of the histogram and box plot into a single graph.\nThe code for a violin plot is almost identical to the code or a box plot - we just use the geom_violin() geom function:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_violin(mapping = aes(x = depth, y = \"\"))\n\n\n\n\nHow do we interpret a violin plot? Like a histogram, the height (in the direction of the y-axis) of the violin tells us how many observations fall in that part of the graph. However, note that the height is mirrored - i.e. it extends both up and down (rather than just up from the x-axis, like in a histogram).\nTo the similarities between a histogram and violin plot clearer, we can overlay our violin plot and our histogram of depth:\n\n\n\n\n\nWe can see that the the violin is widest where the histogram’s bins are also tallest (around depth = 62). You can think of the violin’s border as a line joining the tops of each of the histograms bins.1\n2: Note that a violin plot shows the density of observations (which is kind of like the fraction of observations in that part of the graph), whereas a histogram shows the count. I have scaled down the histogram’s y-axis in this plot so that it overlaps the violin more closely.\nOne nice thing about the violin plot is that it shows us the long thin tails of the data, whereas the histogram’s bins are so small in the tails that they are invisible.\nWe can do a similar comparison of the violin plot with a box plot:\n\n\n\n\n\nAgain we see a lot of similarity:\n\nThe box of the box plot (the middle 50% of observations) corresponds with the widest part of the violin plot.\nThe violin’s tails extend as far as the most extreme outliers in the box plot.\n\n\n\n8.3.5 Describing distributions\nWe have seen three different plots that can visualize the distribution of a single continuous variable. Here’s a comparison:\n\n\n\n\n\nWhat conclusions can we draw from these graphs? First, we can describe the center of the distribution. In this particular example, each of the plots shows a very similar center (around 62%), but this won’t necessarily be the case because each graph shows the center in a different way. Secondly we can describe the shape, which in this case is unimodal and relatively symmetric.\nHere is how different distribution shapes will look in the three types of plots:\n\n\n\n\n\nWhich plot should you use? That’s going to depend on the data, and what you want to explore or communicate about it. Box plots, for example, are good for showing outliers and are the only one of the three plots that shows an exact central statistic (the median), whereas histograms or violin plots present a more fine-grained view of the shape of the distribution. The following table shows a comparison:\n\n\n\nType of graph\nCenter\nOther notes\n\n\n\n\nHistogram\nBin with the most observations\n\n\n\nBox plot\nMedian\nShows outliers, but not modality\n\n\nViolin plot\nGreatest density\n\n\n\n\nIt’s often a good idea to make a variety of different graphs exploring the same thing to see if you can get a different view on the data. When you come to present your data, you can pick one of those graphs that best communicates what you want to show."
  },
  {
    "objectID": "src/book/08_eda_chapter.html#multivariate-covariation",
    "href": "src/book/08_eda_chapter.html#multivariate-covariation",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.4 Multivariate covariation",
    "text": "8.4 Multivariate covariation\nWhen we have multiple variables, we want to understand how their variation is related: this is called covariation.\n\n\n8.4.1 Co-variation with summary statistics\nHow can we boil the covariation of two or more variables down to a single number?3\nWith continuous variables, we can calculate a statistic that measure the correlation. There are various such correlation coefficients, but the key idea is that they indicate how much of the variation in different variables is shared.\nOne such number is Pearson’s correlation coefficient, which varies between -1 and 1.\n\n\n\n\n\n\n\nTo calculate pairwise correlations between columns in a dataframe, we can use the correlate() function from the corrr package. For example, we can look at the correlations between the carat, depth, and price columns of the diamonds dataframe:\n\ndiamonds |&gt;\n  select(carat, depth, price) |&gt;\n  correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 4\n  term    carat   depth   price\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 carat NA       0.0282  0.922 \n2 depth  0.0282 NA      -0.0106\n3 price  0.922  -0.0106 NA     \n\n\nWe can see that there is a strong positive correlation of about 0.92 between carat and price (unsurprisingly!), but a weak relationship between the other two pairs of variables.\nNote that each correlation is reported twice in the table above. We can remove these unnecessary duplications by piping this output dataframe of correlate() onto the shave() function (also from the corrr package), which will just keep one half of the correlations.\nWe can also make the output a little prettier by piping the output dataframe of shave() onto a third function called fashion(), which will round the long correlation coefficients for us to just two decimal places:\n\ndiamonds |&gt;\n  select(carat, depth, price) |&gt;\n  correlate() |&gt;\n  shave() |&gt;\n  fashion()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n   term carat depth price\n1 carat                  \n2 depth   .03            \n3 price   .92  -.01      \n\n\n\n\n\n\n8.4.2 Visualizing two continuous variables\nIf you have two continuous variables, such as price and carat in the diamonds dataset, then a scatter plot is generally a good method of examining their covariation.\nWe learned how to create ggplot scatter plots using the geom_point() function in Section 7.4. Here, for example, is what a scatter plot of price vs. carat looks like:\n\n\n\n\n\n\n\n\n\n\nWe can see several things about this graph:\n\nThere’s obviously a positive relationship between these two variables: larger diamonds also tend to be more expensive.\nThere don’t seem to be any diamonds in the dataset worth more than about $18,000. However, a quick Google search tells us that such diamonds definitely exist! We are missing an entire segment of data in the more exclusive part of the diamond market. This tells us that we need to be careful if we want to do any statistical analysis of this data.\nIt’s hard to tell the density of the points (i.e. where most of the points fall), because in many places all the points overlap in a blob.\n\nWe have a several options to show density.\n\nWe could make the points transparent with the alpha parameter. Then dense regions would be darker (because there would be many overlapping transparent points). For example, with alpha = 0.1:\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = carat, y = price), alpha=0.1) +\n  labs(title = \"Scatter plot of dimond price vs size\", y = \"price ($)\", x = \"size (carats)\")\n\n\n\n\nThis graph is pretty interesting, because we can see a banding pattern at rounder values of the carat variable, e.g. 1.5, 2. It seems strange to have these jumps, and this is likely an artefact of the way the data is recorded, i.e. diamond appraisers are more likely to round a the diamond size to 0 or 1 decimal places.4\n\n5: And the fact that there are no points on the left of these bands compared to the right suggests that either we are missing a bunch of diamonds on the small side of each of these thresholds (which seems unlikely), or that diamond sizes are being rounded up. If we were suspicious people, we might start to ask why jewelers might prefer to round diamond sizes up instead of down…\n\nWe can also create a heat map, using the geom_bin2d() geom function, which is a colored grid where the intensity of the color represents a value on a scale - in this case, the color represents the number of observations (i.e. diamonds) that fall in that grid square.\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_bin2d(mapping = aes(x = carat, y = price)) +\n  labs(title = \"Heat map of dimond price vs size\", y = \"price ($)\", x = \"size (carats)\")\n\n\n\n\nThis is essentially a 2-dimensional histogram. Each grid square is like a bin in a regular histogram. In this heat map, the color represents the “height” of the 2-dimensional bin, i.e. how many diamonds fall into each cell of the heat map’s grid.\nThis graph also shows that small and cheap diamonds are by far the most common (the light blue grid squares).\n\n\n\n\n8.4.3 Visualizing continuous and categorical variables\nWith a mix of continuous and categorical variables, the general strategy is to graph the continuous variable(s) and then break the graph down into separate parts by the categorical variable(s).\nFor example, say you had one continuous variable and one categorical variable. We can graph the single continuous variable, using one of the graph types appropriate for a single continuous variable such as a histogram, box plot, or violin plot, and then break that graph into different pieces using the categorical variable.\nFor example, let’s say we want to visualize the relationship between color and price of our diamonds. One way to break our graphs down in discrete pieces is using the fill aesthetic to add colors to the inside of our shapes, as in this example code:\nggplot(data = diamonds) +\n  geom_...(mapping = aes(x = price, fill = color))\nDepending on what type of plot you are making, this will break it into different colored categories like this:\n\n\n\n\n\nNote that colors are randomly assigned by the ggplot package (i.e. these are not the actual colors of the diamonds).\nWe can see that there does seem to be a small effect of color on price: although most of the distributions overlap, the center of some color categories occurs at higher prices than others. This leads to a strange conclusion: the diamond color categories range from the best coloring (“D”) to the worst coloring (“J”), just from our graphs above it almost looks like the worst colored diamonds have higher prices.\nThe graphs above using the previously ignored y-axis of the box plot and violin plot to create separate boxes and violins. We can actually achieve the same effect without color6 by mapping the categorical variable to the y aesthetic instead of the fill aesthetic, e.g.\n... +\n  geom_...(\n    mapping = aes(x = price, y = color)\n  )\n7: Useful if you are restricted to black and white.\n\n\n\n\n\nUnfortunately we cannot do this with our histogram because it already uses the y-axis for the count.\nThe other problem with the default colored histogram is that each color is stacked on top of the colors beneath it. Unfortunately this violates one of the principles of visualization that we previously learned about, which is that if we are using length (i.e. height of bins) to communicate information, then we ideally want those lengths to start at the same point. If, as in the histogram above, the bins all start in different positions, then we have to mentally try to move the bins, and then it becomes very hard to figure out what is going on. For example it is almost impossible to see the distribution differences in the histogram that are readily apparent in the box plots and violin plots.\nWe have two strategies to fix this:\n\nWe can add the position = \"identity\" argument to the geom_histogram() function to start all the colored bins from the x-axis, and then because the different colors will all overlap we will also need to add the alpha parameter to make them transparent, e.g.\n\n\nggplot(data = diamonds) +\n  geom_histogram(\n    mapping = aes(x = price, fill = color),\n    bins = 15,\n    position = \"identity\",\n    alpha = 0.3\n  )\n\n\n\n\nInstead of solid bars, we can just connect the top of each bin with lines. This type of graph is called a frequency polygon, and can be created with the geom_freqpoly() geom function (used almost identically to the geom_histogram() function except that we set color instead of fill), e.g.:\n\nggplot(data = diamonds) +\n  geom_freqpoly(mapping = aes(x = price, color = color), bins = 15)\n\n\n\n\n\n\n\n\n\n\nAs we can see, the overlapping histogram has so many colors that it is a muddy mess! In my experience this muddle of colors is unavoidable if you have more than two categories, so colored overlapping histograms should usually only be used if you have two categories (or if you have categories that do not overlap much along the x-axis). By comparison, the difference between diamond types are definitely easier to spot in the frequency polygon.\nThis frequency polygon also makes it clear why the J colored diamonds seemed to be more expensive in the violin and box plots: we just don’t have many small (and cheaper) poorly colored diamonds. Either such diamonds don’t exist or, more likely, they were not included in this dataset because nobody cares about diamonds that are both small and ugly.\n\n8.4.3.1 Faceting\nAnother strategy for breaking our graph up by a categorical variable is to split it into sub-plots. In ggplot speak this is called “faceting”. We do this by adding an additional faceting function to the end of our graph code.\nWe can add the facet_wrap() function to any ggplot graph to facet over a single categorical variable. This will create a separate sub-plot (“facet”) for each category in that categorical variable. For example, to facet our histogram of diamond prices over the cut variable:\n\ndiamonds |&gt;\n  ggplot() +\n  geom_histogram(mapping = aes(x = price), bins = 15) +\n  facet_wrap(~ cut)\n\n\n\n\nNote how we passed the argument ~ cut to facet_wrap() to tell it what categorical variable to facet over? You must include the squiggly ~ symbol before the faceting variable’s name in the facet_wrap() function, otherwise you will get an error. The ~ symbol is called a “tilde”.\n\n\n8.4.3.2 Adding more variables\nTo add more variables, you can combine the different aesthetics and functions. Typically each variable you want to visualize will need to be mapped to its own aesthetic or facet.\nFor example, if we wanted to visualize two continuous variables and two categorical variables, then we could first map the continuous variables to the x- and y-axes of a scatter plot, and then map one of the categorical variables to the color parameter and facet over the second categorical variable.\nJust bear in mind that the more information you cram into a graph, the harder it will become to interpret. Here is a plot of the covariation between the price, carat, color, and cut variables from the diamonds dataframe:\n\ndiamonds |&gt;\n  ggplot() +\n  geom_point(mapping = aes(x = carat, y = price, color = color)) +\n  facet_wrap(~ cut) +\n  labs(\n    title = \"Effect of color, cut, and size on diamond price\",\n    x = \"size (carats)\",\n    y = \"price (USD)\"\n  )\n\n\n\n\n\n\n\n8.4.4 Visualizing multiple categorical variables\n\n\n\n8.4.5 The importance of EDA\nVisualization is a vital part of exploratory data analysis and we need to pair it with summary statistics to make sure that we are not being fooled by those statistics.\nConsider Anscombe’s quartet, a series of 4 small datasets (each has 11 observations of an x and a y variable).\nHere’s one of the datasets:\n\nanscombe %&gt;%\n  select(x1, y1) %&gt;%\n  head(11)\n\n   x1    y1\n1  10  8.04\n2   8  6.95\n3  13  7.58\n4   9  8.81\n5  11  8.33\n6  14  9.96\n7   6  7.24\n8   4  4.26\n9  12 10.84\n10  7  4.82\n11  5  5.68\n\n\nHere’s another:\n\nanscombe %&gt;%\n  select(x2, y2) %&gt;%\n  head(11)\n\n   x2   y2\n1  10 9.14\n2   8 8.14\n3  13 8.74\n4   9 8.77\n5  11 9.26\n6  14 8.10\n7   6 6.13\n8   4 3.10\n9  12 9.13\n10  7 7.26\n11  5 4.74\n\n\nThe numbers seem pretty similar, right? In fact, the summary statistics are identical:\n\n\n# A tibble: 8 × 3\n  column  mean standard.deviation\n  &lt;chr&gt;  &lt;dbl&gt;              &lt;dbl&gt;\n1 x1      9                  3.32\n2 x2      9                  3.32\n3 x3      9                  3.32\n4 x4      9                  3.32\n5 y1      7.50               2.03\n6 y2      7.50               2.03\n7 y3      7.5                2.03\n8 y4      7.50               2.03\n\n\nThe mean (center) and standard deviation (variation) is the same for all 4 x’s, and all 4 y’s.\nBut, if we plot the four distributions…\n\n\n\n\n\nVisualization allows us to detect:\n\noutliers (points that lie away from the rest)\ncharacterize relationships between variables: are they linear (straight) or non-linear (curved)?\nidentify hypotheses (theories) about the data\nspot problems that might exist in the data"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#your-turn",
    "href": "src/book/08_eda_chapter.html#your-turn",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "8.5 Your turn",
    "text": "8.5 Your turn\n\n\n8.5.1 Do some EDA\n\nThere are all kinds of other graphs we can plot.\nIt’s time for you to experiment on your own with some exploratory data analysis of the diamonds dataset.\nFrom our previous explorations, there is a positive relationship between price and carat; in other words, as one variable increases, so does the other. However:\n\nthe relationship was not perfect: there seemed to be some uneven scatter. What other variables might be influencing this relationship?\nrelatedly, what other variables are related to price (& what is the variation within those other variables by themselves)?"
  },
  {
    "objectID": "src/book/08_eda_chapter.html#footnotes",
    "href": "src/book/08_eda_chapter.html#footnotes",
    "title": "8  Exploratory Data Analysis and Scientific Discovery",
    "section": "",
    "text": "violin_density↩︎\nviolin_density↩︎\nWe will think about this more in Chapter 10.↩︎\ndiamond-size-rounding↩︎\ndiamond-size-rounding↩︎\nno-color-bw↩︎\nno-color-bw↩︎"
  },
  {
    "objectID": "src/book/09_computing_chapter.html#welcome",
    "href": "src/book/09_computing_chapter.html#welcome",
    "title": "9  More about R",
    "section": "9.1 Welcome",
    "text": "9.1 Welcome\nThis chapter/tutorial will start with a refresher on basic programming in R (refer back to Chapter 3), and then introduce some additional programming concepts such as control flow and loops."
  },
  {
    "objectID": "src/book/09_computing_chapter.html#values-and-variables",
    "href": "src/book/09_computing_chapter.html#values-and-variables",
    "title": "9  More about R",
    "section": "9.2 Values and variables",
    "text": "9.2 Values and variables\n\n9.2.1 A recap\nBy now you are familiar with the idea that in R we have values and variables. We can write something like:\nx &lt;- 2\nto store the value 2 in the variable x. We do so using the assignment operator, &lt;-.\nThis allows us to use a variable such as x in future lines of code. When that future code is run, the variable will be replaced with its value, e.g.\nx + 3\nbecomes\n2 + 3\n\n\n\n\n\n\nExercise: What value is assigned to x after the following lines of R code have run?\nx &lt;- 2 + 2\ny &lt;- 5\nx &lt;- x / y\n\n\n\n\n\n\n\nExplanation\n\n\n\n\n\nThe final line executes the expression on the right of the assignment operator first, retrieving the values of x and y that we have assigned in the earlier lines (4 and 5). The result of that expression, \\(\\frac{4}{5} = 0.8\\) , is then assigned to x, overwriting the previous value that was stored in x.\n\n\n\n\n\n\nTest reference to ?exr-test\n\n\n9.2.2 Expressions and statements\nAs we learn more about programming, it’s useful to distinguish between different types of code.\nIn programming, an expression is a piece of code that returns a value. For example:\n\n2 + 2 returns the value 4\n1 == 2 returns the value FALSE\n\nIf you run a line of R code that contains just an expression by itself, the expression will be evaluated, and the result will by displayed in the RStudio Console.\nHowever, the result of an expression is not saved by default. Instead it is just calculated and then discarded by the program. Otherwise, if you kept bits of data you don’t need, your computer would very quickly run out of memory!\nTherefore we almost never want an expression by itself - instead we want to do something with it, such as assigning it to a variable. Later in this tutorial we will learn of other things we can do with the results of expressions.\nA statement is a line of code that runs but does not return a value. If nothing is printed out in the Console when you run a line of code, then that line is a statement.\n\n\n9.2.3 How an R program runs\nWhat happens when you run an R program (or a code chunk in an RMarkdown document)? And why does this mean that 2 + by itself is not a valid expression?\nBut why will this code cause an error:\n2 +\nwhen this code will run successfully?\n2 +\n2\nWhen you run an R program, your code is sent, line by line to another program called the R interpreter (this is what is running in the RStudio Console). The interpreter converts your code into electrical signals that can be understood by your computer, and then takes the computer’s output and turns it back into a readable response that it displays on the screen.\nWe do not need to worry about how this happens, but the important thing to note is that this happens one line at a time. So this is why 2 + doesn’t work: it gets to the end of the line and has nothing to add to the first number.\nHowever… if a line ends in an incomplete expression, then before giving an error, R will first look to see if the next line could be the continuation of the first. This is why this code chunk works, even though the expression has been broken over multiple lines:\n2 +\n2\nAs programmers, we want to format our code to be read by humans not computers (R only cares if your code is correct, not it is easy to read). It sometimes helps readability to break long lines into multiple lines. For example, compare:\nsome_dataframe %&gt;% filter(col1==2) %&gt;% select(col2,col3) %&gt;% ggplot()+geom_point(mapping=aes(x=col3,y=col2))\nwith the same code broken over several lines:\nsome_dataframe %&gt;%\n  filter(col1 == 2) %&gt;% \n  select(col2, col3) %&gt;% \n  ggplot() + \n  geom_point(\n    mapping = aes(\n        x = col3,\n        y = col2\n      )\n    )\nAlso note that indentation aids readability by indicating what goes together:\n\nWhen we break an expression over multiple lines, you should indent every subsequent line of that expression by 2 spaces to indicate to a reader that it is part of the same expression.\nIn addition, if you insert a line break between a set of parentheses (...), you should also indent the contents of those parentheses by 2 spaces (as we have done with both the geom_point() and aes() functions above).\n\nLong lines of code in an RMarkdown code chunk will also overrun the right margin of a PDF after you knit, as lines in a code chunk are not automatically wrapped onto the next line, unlike regular text. Therefore you may have to break up a long line simply to fit it on the page.\nWhat about this code?\n2 + \n  y &lt;- TRUE\n\n\n\n\n\n\n\nExercise: Is this a valid R statement that will run successfully, or will it give an error?\n\n Run, but return the value 3 and store TRUE in y Run, but return the value 2 and store TRUE in y Give an error that the variable y does not exist in the environment Give an error that 2 + y is not a variable\n\n\n\n\n\n\n\nWhat about this code chunk?\nsome_dataset %&gt;%\n  filter(col_A == \"some_value\")\n  mutate(\n    new_column = col_B * 100\n  )\n\n\n\n\n\n\n\nExercise: Is this a valid R statement that will run successfully, or will it give an error?\n\n It will run just fine There will be an error in the mutate function There will be an error in the filter function"
  },
  {
    "objectID": "src/book/09_computing_chapter.html#data-types",
    "href": "src/book/09_computing_chapter.html#data-types",
    "title": "9  More about R",
    "section": "9.3 Data types",
    "text": "9.3 Data types\n\nIn the first interactive tutorial, An Introduction to Programming in R, you learned about basic data types such as numbers, character strings, and Boolean values.\nYou also learned about more complex data structures such as vectors and lists which can hold multiple values of those basic data types.\nIf you do not remember this, go back to the first interactive tutorial to refresh your memory.\nWe also have data structures that can hold a 2-dimensional table of data (with rows and columns): the dataframe.\nHowever, the dataframe has been in R since the language was first created, and as a result it has some odd behaviours that are counterintuitive and can lead to bugs.\nThe tidyverse collection of packages that we have been using add a new version of a dataframe that fix a lot of these problems called the tibble. We will use the names dataframe and tibble interchangeably in this course, and for the most part they are pretty similar, but you should be aware that they have some subtle differences and you should use tibbles when possible.\n\n9.3.1 Vectors\nYou can create a vector with the c() function. You can also create a vector of numbers using the : operator. For example, these two lines both create the same vector of the values 1 through 5:\n\nc(1,2,3,4,5)\n\n[1] 1 2 3 4 5\n\n\n\n1:5\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\n9.3.2 Other vector tips\nThere are several other useful things to remember about vectors:\n\nYou can combine two vectors into a single vector with the c() function:\n\nx &lt;- 1:3\ny &lt;- 4:6\n\nc(y, x)\n\n[1] 4 5 6 1 2 3\n\n\nYou can extract a single value from a vector by indexing into the vector with square brackets [...] and the position of item that you want:\n\nx &lt;- 101:110\n\nx[5]\n\n[1] 105\n\n\nYou can change any value in a vector with square brackets and the assignment operator:\n\nx &lt;- 1:5\nx[2] &lt;- 42\n\nx\n\n[1]  1 42  3  4  5\n\n\nYou can even extend a vector by one element by assigning to a position 1 higher than it’s current length:\n\nx &lt;- 1:5\nx[6] &lt;- 42\n\nx\n\n[1]  1  2  3  4  5 42\n\n\n\n\n\n\n9.3.3 Combining different data into lists\nTODO: section moved from first R chapter - needs rewriting\nSo far we have looked at pieces of data by themselves:\n\na &lt;- 1\nb &lt;- \"Hello!\"\nc &lt;- TRUE\nprint(a)\n\n[1] 1\n\nprint(b)\n\n[1] \"Hello!\"\n\nprint(c)\n\n[1] TRUE\n\n\nBut what about if we want to combine multiple pieces of data together?\nR includes several types of container that can hold multiple pieces of data. We can then refer to that container by a single variable. For example, instead of the three variables above, we can create a list object that holds all three values:\n\nl &lt;- list(1, \"Hello!\", TRUE)\nl\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"Hello!\"\n\n[[3]]\n[1] TRUE\n\n\nWe create a list using list(). Every value in the list goes inside the parentheses, separated by commas.\n\n\nCreate a list holding 4 values (in this order): 10, “z”, FALSE, -0.1*10\n\n\n\n\nlist(10, \"z\", FALSE, -0.1*10)\n\n\n\n\n9.3.4 Tibbles\nYou can create a tibble using the tibble() function. The arguments of the function should be vectors that will form the columns of the table, for example:\n\nb &lt;- c(TRUE, FALSE, TRUE)\nz &lt;- c(\"Anna\", \"Bob\", \"Carlos\")\n\ntibble(\n  a = 1:3,\n  4:6,\n  student_name = z,\n  b\n)\n\n# A tibble: 3 × 4\n      a `4:6` student_name b    \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;lgl&gt;\n1     1     4 Anna         TRUE \n2     2     5 Bob          FALSE\n3     3     6 Carlos       TRUE \n\n\nNote that:\n\nif the vector is stored in a variable, the variable name will be the column name (e.g. column b)\nyou can override the column name by using new_name = the_vector as we did for the 1st and 3rd columns\nif you do not, you may get an unusual column name, e.g. the 2nd column"
  },
  {
    "objectID": "src/book/09_computing_chapter.html#functions",
    "href": "src/book/09_computing_chapter.html#functions",
    "title": "9  More about R",
    "section": "9.4 Functions",
    "text": "9.4 Functions\nWe have been using numerous functions up to this point, all of which have been written by other people. However, back in Section 3.9 we learned that we can create our own functions. Remember that a function is like a reusable recipe. It takes some inputs, follows a defined set of steps to do something with those inputs, and then returns an output.\nIn Section 3.9 we learned that the basic syntax of an R function looks like this:\n1function_name &lt;- function(parameter_1, parameter_2, etc.){\n2  some code to do something\n  some more code if you want it\n3  return(output)\n4}\n\n1\n\nThe first line creates the function almost like a new variable! We put the name for the new function (e.g. function_name) on the left-hand side of the assignment operator &lt;-. Then on the right-hand side of the assignment operator we use the function keyword to indicate that we are creating a function instead of a normal variable. function is immediately followed by parentheses (...). Inside the parentheses we put the names of any parameters that the function will accept as inputs e.g. parameter_1. Each parameter name is separated by a comma. We end the line with a curly brace {. This indicates that all code after this is part of the function, until we reach a matching closing curly brace }.\n\n2\n\nWe can then add any code that we want to run inside the function. By default, we indent these lines by two spaces as a helpful visual cue that these lines are part of the function.\n\n3\n\nOn the last line of the function’s code we can indicate the output that we wish to return from the function by using the return(...) function. Whatever we pass as an argument to return() will be returned by the original function.\n\n4\n\nFinally, we need to remember to end the function with the closing curly brace }."
  },
  {
    "objectID": "src/book/09_computing_chapter.html#control-flow",
    "href": "src/book/09_computing_chapter.html#control-flow",
    "title": "9  More about R",
    "section": "9.5 Control Flow",
    "text": "9.5 Control Flow\nEarlier in this tutorial we talked about how R executes one line of code and then moves onto the next.\nBut what about if you don’t always want to go to the next line? Maybe want to jump to a different section of code or skip some lines entirely.\nIn the first interactive tutorial we mentioned functions, which are a way of saving particular lines of code so that they can be re-run. You learned how to define a function that had several input parameters, and then call it when you want to run it by passing values as arguments for each parameter. (Yes, there are a lot of words to learn for functions - you should review the first tutorial if this makes no sense!)\n\n9.5.1 If-statements\nThere are also methods of control flow for choosing whether to skip a line entirely. We can use an if statement to run one or more lines of code based on the result of a Boolean condition:\nif(some_boolean_condition_is_TRUE){\n  run the code inside these curly brackets\n}\n\nA Boolean condition is an expression that results in a Boolean value (TRUE or FALSE). If the expression evaluates to TRUE, then the code following the if statements inside the curly brackets {...} is run.\nYou are already familiar with these conditions, as you have used them in the filter() function to indicate which rows to pick from a dataframe.\n\n\n\n\n\n\n\n\n\n9.5.2 If/else-statements\nOften you may want to do one thing if a condition is TRUE, and something else if it’s not. In this case you can put an else{...} block after the if{...} block:\nif(some_boolean_condition_is_TRUE){\n  if TRUE, run the code inside these curly brackets\n} else {\n  otherwise, run this instead\n}\n\n\n\n\nIn general you won’t want to put just TRUE or FALSE in the parentheses in real world code, because it will always execute one of the blocks or the other - however it is a valid Boolean expression!"
  },
  {
    "objectID": "src/book/09_computing_chapter.html#loops",
    "href": "src/book/09_computing_chapter.html#loops",
    "title": "9  More about R",
    "section": "9.6 Loops",
    "text": "9.6 Loops\nSometimes you want to run a particular piece of code over and over again (after all, do simple things repeatedly is what computers are best at).\nTo do this we need to use a loop.\n\n9.6.1 For loops\nThe most common type of loop in R is the for loop.\nThe idea behind the for loop is that “for every thing in a group of things, run some code”.\nIn R code, we would write this as:\nfor(thing in many_things){\n  some code to run\n}\n\n\n\nNote that on each iteration of the loop we update the variable x (or whatever you choose to call it) and that variable can be used in the code inside the for loop’s code block {...}\n\n\n\n\n\n\n9.6.2 Vectorization\nWhen possible, we should try to work on entire vectors at once rather than looping over a vector with for loops.\nThis is because R will iterate through a loop one step at a time, whereas it can operate on multiple elements of a vector at the same time, so vectorized code is much faster.\nWe do not have to worry about this too much, as we are working on small datasets where small speed increases are not noticeable - however, much of the the tidyverse functions we have learned are already optimized to work on vectors.\nThere are also situations where we have to use loops rather than vectorized operations (i.e. if one calculation depends on the result of the previous). However, if you are working on large datasets in R in the future, and your code is running too slowly, see if there is a way to speed it up by applying an operation to entire vectors at a time (rather than looping through a vector)."
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#what-is-a-model",
    "href": "src/book/10_modeling_chapter.html#what-is-a-model",
    "title": "10  Modeling",
    "section": "10.1 What is a model",
    "text": "10.1 What is a model\nThis video will introduce you to the concept of models, and in particular the linear model (which you may already be familiar with as a line of best fit).\n\n\nSlides: PDF\nThe real world is complicated. There are many moving pieces, all interacting with each other.\nAs scientists, we want to understand these interactions. We want to be able to describe the real world as a theory.\nUnfortunately the real world is so complicated that we cannot possibly comprehend all of the interactions at once. Therefore our theoretical description of the world has to be a simplified version. We call these simplified theories “models”.\nOften (as scientists) we would like our models to be mathematical, because we want to know exactly how much two things are related to each other.\nOne common type of mathematical model is called the linear model. You have probably encountered this before, although possibly under a different name, such as linear regression or a line of best fit.\nWhat a linear model means is that if we have two variables,1 then we can simplify their relationship to a straight line:\n\n\n\n\n\nThis linear model’s line is simpler that the original dataset of 10 data points, because we can represent it with just two numbers: the slope and the intercept.\nIn other words, we can model the relationship between the variable on the \\(y\\) axis and the variable on the \\(x\\) axis as a mathematical relationship:\n\\[\ny = ax + b\n\\] where \\(a\\) is the slope of the line (\\(a\\) increases as the slope gets steeper) and \\(b\\) is the value at which the line intercepts the \\(y\\)-axis.\nWe can represent any two dimensional straight line with just these two numbers \\(a\\) and \\(b\\). You could have millions of data points instead of just 10, but the linear model representation will still only need two numbers.\nWe call this relationship linear because as \\(x\\) goes up by 1, the \\(y\\) value of the line always goes up by \\(a\\) (the slope). In other words, the slope tells us how much the y value changes as the x value increases by 1. Since the line is linear, it’s \\(y\\) will always increase by the same amount (\\(a\\)) regardless or whether \\(x\\) increases from \\(0\\) to \\(1\\) or from \\(99\\) to \\(100\\).\nIf \\(x\\) goes up by 2, then \\(y\\) goes up by \\(2a\\). It doesn’t matter how much we change \\(x\\), \\(y\\) will change by a constantly proportional amount.\nNow, we will lose some information if we simplify our data down to a linear model. As we can see from graphs above, the points do not fall exactly on the line, but some distance from it. So there are clearly other things going on in the world which we have not measured that affect the \\(y\\) and \\(x\\) variables."
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#describing-linear-relationships",
    "href": "src/book/10_modeling_chapter.html#describing-linear-relationships",
    "title": "10  Modeling",
    "section": "10.2 Describing linear relationships",
    "text": "10.2 Describing linear relationships\n\n10.2.0.1 Linear versus non-linear\nThe opposite of a linear relationship is a non-linear relationship, which you can see in the underlying data on the right in this figure. Non-linear basically means “there’s a curve” in the data.\n\n\n\n\n\nMathematically, there is nothing that stops us we can still create a linear model for non-linear data! Unfortunately, it doesn’t do a very good job of capturing the underlying relationship between \\(x\\) and \\(y\\). It’s on you, the human operating the machine, to make sure that you only use a linear model if there is actually a linear relationship in the data. Later in this chapter, we will see how we can assess whether this is the case for any dataset.\n\n\n10.2.0.2 Correlation\n\nConsider these two different sets of data:\n\n\n\n\n\nThere is a linear relationship in both of these plots! In both cases, the y values seem to increase by a constant amount (on average) as the x values increase by a constant amount.\nThe difference is that the points in the left hand plot are much more closely clustered around the line than on the right. More formally we would say that the left hand y is strongly correlated with x, whereas in the right hand side graph there is a weak correlation.\nWhen you are new to linear relationships, it can be easy to mistake a weak correlation for a non-linear relationship. The best way to be sure is to ask yourself:\n\nIs there an obvious curved trend to the underlying data?\n\nThis is the same as asking is there a non-linear relationship? If the answer is no, then, by a process of elimination, there is either a linear relationship (or no correlation at all, as we will see in the next section).\n\n\n10.2.0.3 Positive, negative, or no correlation?\n\n\n10.2.0.4 Other linear model terminology\n\nWhen we create a simple linear model (e.g. a line of best fit) like the ones above, we have 2 variables:\n\nThe variable on the \\(y\\)-axis, which we call the response variable.\nThe \\(x-axis\\) variable, the explanatory variable\n\nThese names (“response” and “explanatory”) seem to imply that the values of \\(x\\) explain the values of \\(y\\), or conversely that the \\(y\\) variable responds to changes in \\(x\\). In other words, we seem to be saying that a change in \\(x\\) is directly causing the change in \\(y\\), and not the other way around.\nHowever…\n\n\n10.2.1 “Correlation does not imply causation”\nA common saying when working with models is that “correlation does not imply causation”. What does this mean?\nThe linear model cannot actually tell us which variable is causing changes in the other (causation). Just because we decide to call one variable the “response” and the other “explanatory”, does not mean that is necessarily the direction of effect in the real world.\nFor example, taller people generally weigh more, but there is nothing stopping us from creating a linear model in which height is the response variable (y), even though most people would say that it makes no sense to say that your weight causes your height.\nSince we have picked which variable is the “response” before creating the linear model, it would be circular for us to then claim that the linear model shows that the explanatory variable is causing the response. Of course it seems to be showing that, because we told it to show it! The linear model (which is fundamentally a fairly simple mathematical formula) has no way of detecting the true causal relationships that exist in the real world.\nTo summarize in more general terms: even if there seems to be a linear relationship between two variables, we cannot definitively say that one causes the change in the other - only that they are correlated.\nEven when we have a correlation, it’s possible that neither of the variables in your model is directly causing a change in the other. There might be other variables (that are not in our model) that actually cause the change in both of the modeled variables. To return to our height vs. weight example: the variation in both height and weight might be caused by diet (or genetics), yet we have not included diet (or genetics) as a variable in our model. Imagine a world in which better2 diet causes all of the variation in people’s height and weight. A linear model of height and weight would show a strong correlation - however (in this contrived example) neither is causing the changes in the other since both are ultimately being determined by another factor which we have not analyzed.\n3: Or, at least, more calorific…"
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#creating-models-in-r",
    "href": "src/book/10_modeling_chapter.html#creating-models-in-r",
    "title": "10  Modeling",
    "section": "10.3 Creating models in R",
    "text": "10.3 Creating models in R\n\n\nSlides: PDF (same as before)\n\n10.3.1 Let’s create a linear model\n\n\nSlides: PDF (same as before)\nLet’s create your first linear model in R, using a simulated4 dataset called sim_df.\n\nThis is what the first few rows of the simulated dataset stored in the sim_df variable look like:\n\n\n\n\n\nx\ny\n\n\n\n\n8.551703\n17.739356\n\n\n8.930282\n17.576311\n\n\n-2.135628\n-6.927711\n\n\n7.117610\n11.794752\n\n\n3.909674\n9.139461\n\n\n1.824631\n3.342624\n\n\n\n\n\n\n\nAs you can see, there are two variables, \\(x\\) and \\(y\\). Here’s what they look like:\n\nsim_df %&gt;%\n  ggplot() +\n  geom_point(aes(x,y))\n\n\n\n\nThis looks like a pretty good candidate for a linear model, so let’s create one.\nWe’ll do that with the lm() function. The general syntax of this function is:\nlm(response ~ explanatory, data = _______)\n\nresponse and explanatory are the column names of those two variables.\nThe data parameter expects the dataframe containing those columns.\n\n\nWhat’s going on with response ~ explanatory? This is what’s know as an R formula.\nThe mathematical formula for a linear model is \\(y=ax+b\\). In R, we would write this as y ~ x: i.e. we replace \\(=\\) with ~, and we only include the actual variables. From this, R infers that we want to estimate the model’s coefficients (the slope, \\(a\\), and the intercept, \\(b\\)).\n\n\n\n\nExercise: In the linear model formula \\(y = ax + b\\),\n\n\nThe slope is represented by \n\n\n\n\n\n\n\nNote: Unfortunately the lm() function is a part of the core R programming language that predates the tidyverse packages. Unlike the tidyverse functions, the first argument of lm() is the formula, not the data frame. Therefore, you cannot pipe a data frame to lm(), because pipe inserts the data frame into the first argument position (which will cause an error).\n\n\n\n10.3.2 Your model’s coefficients\nWe usually want to report the slope and intercept of a linear model. The slope is especially useful to report, because it indicates how much \\(y\\) changes as \\(x\\) changes.\n\nIf we have a slope of 1, then this means that as \\(x\\) increases, \\(y\\) increases by the same amount (i.e. a ratio of 1).\nIf we have a slope of 3, then as \\(x\\) increases, \\(y\\) increases by three times that value (e.g. if \\(x\\) increases by 10, \\(y\\) increases by \\(3 \\times 10 = 30\\)).\nA slope of \\(0\\) means that \\(x\\) and \\(y\\) are uncorrelated, i.e. as \\(x\\) increases, the value of \\(y\\) is not affected (on average). (We can still have variation in the \\(y\\) variable - it’s just that such variation seems to be entirely unconnected to changes in the \\(x\\) variable).\n\n\nThe intercept is the point at which the line of our linear model intersects the \\(y\\)-axis. This is the value of the response variable when the explanatory variable is 0.\nWe can report the model’s coefficients with the tidy() function from the broom package. The syntax is like this:\ntidy(your_model)\nor equivalently\nyour_model %&gt;%\n  tidy()\nif you prefer to pipe the linear model in (the your_model variable above refers to the output of the lm() function).\nFor example, here is the table of coefficients for the model we were trying to create in the previous section:\n\nsim_model &lt;- lm(y ~ x, data = sim_df)\n\nsim_model %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -0.0750    0.345     -0.217 8.29e- 1\n2 x             1.95      0.0577    33.7   3.22e-24\n\n\nThe important column here (for our current purposes) is the estimate column. This tells us that the lm() function estimates the intercept to be -0.0749858 and the slope to be 1.9468815.\nNote that although the intercept row is called (Intercept), the slope row is named after the explanatory variable, x. As we shall see in the next section, we can have more than one explanatory variable in a single model, and this helps us keep track of the slope associated with each one.\n\n\n\n\n\n\n\n\n\n\n10.3.3 Categorical explanatory variables\n\n\n10.3.4 More than one explanatory variable\n\n\n10.3.5 Multiple explanatory variables\nIn our previous module on linear models, we considered what is called simple linear regression: a linear model with just one explanatory variable, \\(x\\):\n\\(y = m \\times x + c\\)\nwhich we created using the lm function.\nHowever, we can technically have as many explanatory variables in our model as we like! (But we can only ever have a single response variable, \\(y\\)) For example, this model has two explanatory variables, called \\(x_1\\) and \\(x_2\\) (we use the subscripts 1 and 2 to tell them apart):\n\\(y = m_1 \\times x_1 + m_2 \\times x_2 + c\\)\nNote:\n\nEach explanatory variable gets its own coefficient (\\(m_1\\) and \\(m_2\\)) - note how the number mathces the number of the explanatory variable. If we added another explanatory variable, we would call it \\(x_3\\) and it would have a coefficient \\(m_3\\), and so on. (In theory, there is no limit on the number of explanatory variables we can include in a single linear model.)\nThere is still just a single intercept, \\(c\\). It doesn’t matter how many explanatory variables we add, we can only have one intercept (because we will only have one y-axis, which the model will only cross at one place).\n\n\n\n10.3.6 But what does that look like?\nWe can pretty easily plot \\(y\\) vs a single explanatory variable \\(x\\). In this case, a linear model is just a best-fit line through the data:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nBut what does a linear model look like in higher dimensions?\n\n\n\n\n\n\nAs you can see, in three dimensions we have a plane of best fit rather than a line (note: you should be able to click-and-drag to rotate the 3D graph).\nBut what about if we have 3 explanatory variables? Then our model would exist in 4-D space! I don’t know if you’ve ever tried to imagine what things look like in four dimensions, but it’s kind of impossible. And, of course, the more explanatory variables we add, the more dimensions this “space” has. However, it turns out that in this high dimensional “hyperspace”, the linear model is still the equivalent of a flat surface which we would call a “hyperplane”.\nThe fact that we can’t visualize these high dimensional spaces (even the 3-D graph above is hard to understand) means that we need some way of plotting them in just 2 dimensions. We want to plot our model because its the easiest way of telling whether the model is an appropriate summary of the data. For example, we don’t want to plot a linear model if there is a non-linear relationship (i.e. if the data has a curved pattern).\n\n\n10.3.7 lm() with multiple explanatory variables\nRecall that the first argument of the lm function was a formula. We create an R formula by:\n\nremoving the constant coefficients\nreplacing the \\(=\\) sign with ~ (~ means “is related to”)\n\nFor example,\n\\(y = m x + c\\)\nbecomes\ny ~ x\nThe same principle applies to multiple explanatory variables:\n\\(y = m_1 x_1 + m_2 x_2 + c\\)\nbecomes\ny ~ x1 + x2\n\n\n\n\n\n\n\n\n\n10.3.8 Create a multiple regression model\n\nLet’s create a linear model from the diamonds dataset.\nThe diamonds dataset is included as part of the ggplot2 package and contains observations about the properties of thousands of diamonds. We are going to use three variables from the dataset:\n\nthe price variable will be our response variable (i.e. y)\nthe carat and depth variables will be our explanatory variables\n\n\n\n\n\n\n\n\n10.3.9 \\(R^2\\): how good is your model?\nSo we’ve reported the slope and intercept: however, these don’t tell us about how well the model fits the data, i.e. how close the points are to the model’s line.\nFor example, you could have a steep slope (suggesting that a change in \\(x\\) is correlated with a big change in \\(y\\)), but the points could be scattered a long way from the line (i.e. large residuals). For example, consider these two graphs:\n\nset.seed(42)\nx1 &lt;- 1:10 + rnorm(10, 0, 0.5) \nx2 &lt;- 1:10 + rnorm(10, 0, 0.5)\nx &lt;- c(x1, x2)\ny &lt;- c(x1 + rnorm(10, 0, 3), 2*x2 + rnorm(10, 0, 0.5))\nseries &lt;- c(rep(\"Low slope, large scatter\", 10), rep(\"Steep slope, low scatter\", 10))\ntibble(x, y, series) %&gt;%\n  ggplot(aes(x,y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE) +\n  facet_wrap(~ series)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can use a statistic called \\(R^2\\) (R-squared). You can think of \\(R^2\\) as the % of the variation in \\(y\\) that is explained by the explanatory variable, \\(x\\).\n\\(R^2\\) varies between 0 and 1. 1 means that the model explains all the variation: i.e. the points fall exactly on the line of best fit! As the points get further from the line, \\(R^2\\) will decrease.\nWe can report the \\(R^2\\) by piping a model to the glance() function.\nyour_model %&gt;%\n  glance()\n\nFill in the blank with the correct function to report the \\(R^2\\) value of your model.\n\n\nsim_model %&gt;%\n  _______()\n\n\nsim_model %&gt;%\n  glance()\n\n\n# check code\ngradethis::grade_code()\n\nYou’ll note that glance() reports a whole bunch of statistics. We only care about the first column, r.squared, and can ignore the rest.\n\nquestion(\n  \"What is the R-squared value of your model?\",\n  answer(\"0.935\", correct=TRUE),\n  answer(\"0.926\"),\n  answer(\"1.52\"),\n  answer(\"114\"),\n  allow_retry = TRUE\n)\n\n\n\n10.3.10 What’s a good \\(R^2\\)?\nWe can use the \\(R^2\\) to determine how “good” a model is. However, what value of \\(R^2\\) is “good”?\nIt turns out that the answer to this question varies widely between different areas of science.\nIn physics, we often collect data from simple systems where data can be collected very accurately, so physicists expect high \\(R^2\\) values in their experiments. For example, if you are measuring the speed of an object, there will be only one or two variables that will affect the speed, so you should get a very good correlation. Physical variable are generally easy to predict.\nIn contrast, the social sciences (e.g. psychology or economics) will accept lower \\(R^2\\) values, because there social systems are very complex with a huge number of interacting variables. For example, you could measure the effect of a good night’s sleep on students’ exam results the next day, but there are going to ge a lot of other things that affect how well a student does (such as how much they studied). As a result, even if sleep and exam results are related, the correlation will be very weak. Psychologists may be quite happy with \\(R^2\\) values of less than 0.5 (i.e. 50%).\nFor the purposes of this class, you can think of an \\(R^2\\) &gt; 0.5 as good, an \\(R^2\\) &gt; 0.25 as okay, and an \\(R^2\\) &gt; 0.1 as weak."
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#testing-our-assumptions",
    "href": "src/book/10_modeling_chapter.html#testing-our-assumptions",
    "title": "10  Modeling",
    "section": "10.4 Testing our assumptions",
    "text": "10.4 Testing our assumptions\n\n10.4.1 Assumptions of the linear model\nAll models make assumptions - that’s how they are able to simplify complexity.\nThe linear model makes these assumptions:\n\nLinearity: we assume that there is a linear relationship between the response and explanatory variables (i.e. that they fall more or less along a straight line).\nNearly normal residuals: the residuals are normally distributed around the model line.\nConstant variation of residuals: the variation in residual size (above and below the model line) is similar in all parts of the model.\nIndependent observations: Each observation (i.e. each \\((x,y)\\) point) was generated independently from the others.\n\nThese might seem a little complicated, but they all make some intuitive sense when you think about what they mean, so let’s go through them one-by-one.\n\n\n\n10.4.2 Linearity\n\n\n10.4.3 The assumption of linearity\nBy far the most important important assumption that we make when we fit a linear model to some data is that there is actually a linear relationship between the response and explanatory variables!\nIf we have a simple linear model with just one explanatory variable, then we easily see whether the relationship is linear or non-linear (i.e. curved):\n\n\n\n\n\nIn the left-hand graph the points obviously fall on a straight line (a linear trend). However, in the right-hand side plot, there is an obvious curved (non-linear) trend, and so that data violates this assumption of the linear model.\n\nWhen we have a linear model with more explanatory variables, then we cannot plot the original data to see whether there is a curve. Fortunately, there is a 2-D graph we can create for any linear model,\nBy definition, a linear model’s predictions (for the response variable) will all fall on a flat line/plane/hyperplane. Therefore, if the observed (i.e. true) values of the response variable are linearly related to the explanatory variables, they will also be linearly related to the predicted response variables. \nIn other words, no matter how many dimensions in our original model, we can always create a 2D plot to examine the assumption of linearity! We just have to create a scatter plot of the observed response variable vs. the predicted response variable.\n\nWe call this an observed vs predicted plot (with observed on the y-axis, predicted on the x-axis). Because these values are essentially the same number (or would be if the model went through every data point exactly), we should expect these variables to fall on a line with a slope of 1 and an intercept of 0. We will add such a reference line to our plot for comparison.\nBefore we can do so, we need to calculate the predicted values. We can easily create a column of predicted values using the augment() function from the broom package, which adds a column called .fitted to a dataframe using a model (such as the diamonds_model that you created earlier with the lm function):\n\ndiamonds_plus_preds &lt;- augment(diamonds_model, diamonds)\n\n(Note that the augment() function also adds a column called .resid which contains the residuals.)\n\n\n\n\n\n\n\n\n10.4.4 Constant variability of residuals\n\n\n10.4.5 Constant variability\nThe residuals of a linear model are arranged above and below the line. As we saw in the previous section, this arrangement should look similar to a normal distribution, i.e.\n\nmost residuals will be small (close to the line)\na few residuals will be large\n\nThe linear model also assumes that this normal distribution is the same in all parts of the model (i.e. at both high and low value of \\(x\\)).\nFor example, in this model, we can clearly see that the variation in the residuals is not constant - this is a violation of this third assumption of the linear model:\n\nset.seed(46)\nx &lt;- 1:100 + rnorm(100)\ny &lt;- x + 100:1 * rnorm(100)\nggplot(mapping = aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Decreasing variation in points\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIn the graph above, the points on the right lie much closer to the line: i.e. the residuals will be smaller.\nFor simple linear models like this (with only a single explanatory variable \\(x\\)), we can get a reasonable idea of the variability of the residuals just by looking at a scatter plot of the model, as above.\nFor comparison, here is the model you fit earlier:\n\nsim_df %&gt;%\n  ggplot(mapping = aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nquestion(\n  \"Does your model (as shown above) satisfy the linear model's assumption of constant variability of residuals?\",\n  answer(\"Yes, the model satisfies this assumption.\", correct = TRUE),\n  answer(\"No, the model violates this condition.\"),\n  allow_retry = TRUE\n)\n\nBut as with the previous assumption of linearity we can’t plot the original model in a two dimensional screen or page if we have more than one explanatory variable.\nFortunately there is another that we can make for any type of model! This is a residuals vs predicted plot, which has the residuals (on the y-axis) vs the predicted response variable (x-axis). This graph will also always have just two dimensions, regardless of how many explanatory variables are in our model, and so it is ideal for assessing this assumption.\nIn this graph, our reference line should be a horizontal line plotted at \\(y = 0\\), because the residuals should be distributed around zero in all parts of the model.\nHere are three example residual vs predicted plots (with purple guide lines showing the variability):\n\nThe left-most plot shows constant variability of the residuals above and below the line.\nThe middle graph violates this assumption because the variability of the residuals increases as the predicted values increase.\nThe third graph also violates this assumption, because the variation above and below the line is not consistent (this pattern arises because there is not a linear relationship between the response and explanatory variables - the residual vs predicted plot can also be used to examine the first assumption of linearity).\n\n\n\nknitr::include_graphics(\"../img/resid-vs-pred.png\")\n\n\n\n\n\n\n\n\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = ______, y = ______)) +\n  geom_hline(yintercept = ____, color = \"white\", size = 2)\n\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x = .fitted, y = .resid)) +\n  geom_hline(yintercept = 0, color = \"white\", size = 2)\n\n\n# check code\ngradethis::grade_code()\n\n\n\n\n10.4.6 Nearly normal residuals"
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#nearly-normal-residuals-1",
    "href": "src/book/10_modeling_chapter.html#nearly-normal-residuals-1",
    "title": "10  Modeling",
    "section": "10.5 Nearly normal residuals",
    "text": "10.5 Nearly normal residuals\n\nThe best linear model is the one that has the smallest sum of the squared residuals. We don’t need to worry too much about the mathematics behind this - however, you should be aware that the math does make the simplifying assumption that the residuals have a normal distribution (more or less).\nThe normal distribution is a bell-shaped distribution:\n\n\n\n\nWe can plot a histogram of the residuals and see if that has a (more or less) normal distribution. However, it is often hard to interpret the “normality” of a histogram just by looking at it, since there are many different types of unimodal symmetric distribution.\nInstead, a much better method is to create a Quantile-Quantile (Q-Q) plot. This video from Stat Quest gives an excellent overview of what a Q-Q plot is:\n\n\n\nQ-Q plots are so useful that the ggplot2 package already contains geom functions for creating them.\nWe can use the diamonds_plus_preds data frame to create a Q-Q plot (with both points and a reference line):\n\ndiamonds_plus_preds %&gt;%\n  ggplot() +\n  geom_qq(mapping = aes(sample = .resid))+\n  geom_qq_line(mapping = aes(sample = .resid))\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.1 Independent observations\n\n\n10.5.2 Independent Observations\nThe fourth (and final) assumption of the linear model is that all the observations (data points) were generated independently.\nWe will not worry to much about this assumption in this course, as the datasets we give you will have independent observations (for the most part).\nBut what does it mean for observations to be non-independent?\nConsider the stock market: we could have a dataset that contains measurements of a company’s stock price at the end of each day. However these observations are not independent. A stock price will rise or fall, but this change will be from the previous observation. I.e. if the stock ends day 1 at 100, it will probably end day 2 pretty close to 100. Therefore, the data points are not independent.\nFor comparison, think about measuring the height of all the students in this class: one student’s height does not affect anyone else’s height, so these observations are all independent. (Assuming that none of the students are related to each other!)"
  },
  {
    "objectID": "src/book/10_modeling_chapter.html#footnotes",
    "href": "src/book/10_modeling_chapter.html#footnotes",
    "title": "10  Modeling",
    "section": "",
    "text": "Or, as we will see later, more than two variables…↩︎\ncalorie-caveat↩︎\ncalorie-caveat↩︎\nA simulated dataset contains made-up values, usually generated randomly by a computer, rather than observations collected from real life.↩︎"
  },
  {
    "objectID": "src/book/11_inference_chapter.html#what-is-inference",
    "href": "src/book/11_inference_chapter.html#what-is-inference",
    "title": "11  Inference",
    "section": "11.1 What is inference?",
    "text": "11.1 What is inference?\nThe world is an uncertain place. If you look outside in the morning and notice dark clouds, then you might infer that it is likely to rain that day.\nIn this chapter we will be learning about statistical inference, which is the idea that if we collect some data from the real world, then we can use it to infer other facts about the world (such as the processes that generated that data, or the population that our observations came from).\n\n11.1.1 Probability versus inference\nWhen things are uncertain, we often talk about the probability that an event will occur (such as the high probability that it might rain on a cloudy day, or the low probability that the Washington Wizards will win the NBA Championship this year).\nMathematically, probability is about making predictions about the future. If we know the probability of different events or observations, then we can predict the likelihood of different outcomes. If I tell you that I have a fair coin, you know that there is a 50% chance of either heads or tails, and so you can calculate the probability of getting 6 heads if I flip the coin 10 times.\nBy contrast, statistical inference is about the understanding the past. In inference we are typically given a sample of data, but we don’t know exactly how the data was generated. This is how things tend to work in both real life and in science. You observe that I get 6 heads from 10 coin flips, but you don’t know if the coin was fair or not. Inference allows you to calculate the likelihood that the coin was fair.\nThus we need to know a little bit about probability to do statistical inference. Fortunately, everything we need to know for this chapter is fairly intuitive:\n\nA probability has to be between 0 and 1. A probability of 0.5 means that something happens half of the time; a probability of 1 means that it happens all of the time!\nIn informal settings (such as weather forecasts) we sometimes talk about probabilities as percentages. You can multiply a probability by 100 to get it as a percentage. For example, a probability of 0.5 is equivalent to a 50% chance that something will happen."
  },
  {
    "objectID": "src/book/11_inference_chapter.html#distributions",
    "href": "src/book/11_inference_chapter.html#distributions",
    "title": "11  Inference",
    "section": "11.2 Distributions",
    "text": "11.2 Distributions\n\n11.2.1 Data sampling\n\n\nSlides: PDF\nBefore we can do any inference, we need to be clear on (1) what data we are using to make our inferences, and (2) what other data those inferences then apply to.\nTypically we analyze a sample of data (e.g. a group of people, or a series of coin flips). This sample is either:\n\nDrawn from a population of all possible observations (e.g. our sample of people is drawn from a larger population of people).\nGenerated by some random process (such as flipping a coin).\n\n\nWe then use the sample to make inferences about the population or process.\nWe also need to distinguish between statistics that we calculate from the sample, and the true value of that statistic from in the total population. If I collect heights from a sample of people, I can then calculate a statistic such as the mean - we would call this the sample mean. However, there is also a population mean that is probably different, since we might have slightly more tall or short people in our sample than in the total population.\nWhen we collect samples, we always hope that our sample is representative of the broader population. If it is, then we should be able to make accurate inferences.\n\n\n11.2.2 Sampling strategies and how it can go wrong\nThe best way to create a sample is to first decide what your population is, and then randomly sample members of that population.\nIf you are interested in the entire population of the USA, for example, then you would want to put every American’s name in a (virtual) hat, and then draw a sample of those names.\nOf course, there are problems with this! You might not know everyone in your population, and you might not be able to collect data from every member of the sample (for example, it’s usually unethical to force people to take part in a survey against their will). If your sample is not completely random, then it is unlikely to be representative of the population.\nThe alternative approach is to start with the sample that you have, and calculate the largest population that we can generalize to. Let’s image we measure the heights 100 male students sampled randomly at George Mason University.\nCommon sampling biases: * self-selected * convenience\nMany statistical analyses have gone astray when they generalize to a broader population that they should have done.\n\n\n11.2.3 Population-wide censuses\n\n\n11.2.4 Quantifying data distributions\n\n\nSlides: PDF\n\n\nSlides: PDF\nFill in the ellipses (...) to create a boxplot of the Sepal.Length variable from the iris dataset.\n\n\n11.2.5 Probability mass functions\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/11_inference_chapter.html#hypothesis-testing",
    "href": "src/book/11_inference_chapter.html#hypothesis-testing",
    "title": "11  Inference",
    "section": "11.3 Hypothesis testing",
    "text": "11.3 Hypothesis testing\n\n11.3.1 Gender discrimination case study\n\n\nSlides: PDF\n\n\n11.3.2 A hypothesis test as a court trial\n\n\nSlides: PDF\n\n\n11.3.3 A manual simulation of the gender discrimination experiment\n\n\nSlides: PDF\n\n\n11.3.4 Simulating the gender discrimination experiment in R\n\n\nSlides: PDF\n\n\n11.3.5 One-sided hypothesis tests using infer\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/14_reproducibility_chapter.html",
    "href": "src/book/14_reproducibility_chapter.html",
    "title": "12  Reproducibility",
    "section": "",
    "text": "Warning\n\n\n\nThis chapter is a draft, and may be updated soon."
  },
  {
    "objectID": "src/book/16_more_graphs_chapter.html",
    "href": "src/book/16_more_graphs_chapter.html",
    "title": "14  More Graphs",
    "section": "",
    "text": "Under Construction\n\n\n\nThis chapter is not yet complete."
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#statistical-inference-review",
    "href": "src/book/17_inference_advanced_chapter.html#statistical-inference-review",
    "title": "15  More Statistics",
    "section": "15.1 Statistical Inference Review",
    "text": "15.1 Statistical Inference Review\n\n15.1.1 Video overview\nThis video reviews the inference material covered in the first module.\n\n\nSlides: PDF\n\n\n15.1.2 Test your understanding\n\n\nquestion(\n  \"You give sick patients either a drug or a placebo, and you measure whether they die or survive. This is stored in 2 variables, `treatment` and `outcome`, which are both categorical. What are your explanatory and response variables?\",\n  answer(\"response = `treatment`, explanatory = `outcome`\"),\n  answer(\"response = `outcome`, explanatory = `treatment`\", correct = TRUE),\n  answer('response = `\"survived\"`, explanatory = `\"drug\"`'),\n  answer('response = `\"survived\"`, explanatory = `\"placebo\"`'),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"In a particular hypothesis test, we generate 10,000 permutations of our original data. 200 of them have a test statistic more extreme than our observed statistic. What is the p-value?\",\n  answer(\"0\"),\n  answer(\"0.01\"),\n  answer(\"0.02\", correct = TRUE, message=\"The p-value is the probability that a value as or more extreme than the observed statistic came from the null distribution, which equivalent to the fraction of simulations more extreme than the observed statistic. This is 200/10,000 = 0.02\"),\n  answer(\"0.05\"),\n  answer(\"0.1\"),\n  answer(\"0.2\"),\n  answer(\"0.5\"),\n  answer(\"1\"),\n  answer(\"2\"),\n  answer(\"5\"),\n  answer(\"50\"),\n  allow_retry = TRUE\n)\n\n\n\n15.1.3 Supplementary reading\nThese two chapters provide a good review of the original statistical inference material that we covered in the first inference module. You should review them before starting this week’s assignment:\n\nStatistical Inference via Data Science by Chester Ismay and Albert Kim (available online at: https://moderndive.com/)\n\nChapter 7: Sampling - https://moderndive.com/7-sampling.html\nChapter 9: Hypothesis testing - https://moderndive.com/9-hypothesis-testing.html\n\n\nOn a lighter note, you can get an animated explanation of a hypothesis test with alpacas here: https://www.jwilber.me/permutationtest/\n\n\n15.1.4 Code review\nIn the first inference module, we used the infer package to simulate the null distribution.\nWe ran 10,000 random permutations to shuffle up the values of the response and explanatory variables. This random shuffling ensures that we simulate a distribution where there is no relationship between the two variables.\n&lt;DATAFRAME&gt; %&gt;%\n  specify(&lt;RESPONSE&gt; ~ &lt;EXPLANATORY&gt;) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(10000, type = \"permute\") %&gt;%\n  calculate(stat = \"...\", order = c(..., ...))\n\nThe specify function defines the response and explanatory variables.\nThe hypothesize function declares what the null hypothesis is between those variables. \"independence\" means that there is no relationship between them.\nThe generate function is where the simulations are actually run: we specify how many, and how to shuffle the data (in this case, using permutations).\nThe calculate functions takes the simulations and calculates a test statistic: i.e. a single value for each one (e.g. the difference in proportions, or the difference in means)."
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#one-sided-vs.-two-sided-tests",
    "href": "src/book/17_inference_advanced_chapter.html#one-sided-vs.-two-sided-tests",
    "title": "15  More Statistics",
    "section": "15.2 One-sided vs. two-sided tests",
    "text": "15.2 One-sided vs. two-sided tests\n\n15.2.1 Video overview\n\n\nSlides: PDF\n\n\n15.2.2 1 or 2?\n\n\nquestion(\n  \"You want to conduct a *two-sided* hypothesis test on data from an experiment comparing whether a patient survive a disease after receiving a drug or a placebo. What is your alternative hypothesis?\",\n  answer(\"The difference in proportions of patients who survive is significantly different between drug and placebo groups.\", correct = TRUE, message = \"In a two-sided test, the alternative hypothesis is that there is a difference, regardless of direction (i.e. whether the difference is positive or negative is unimportant).\"),\n  answer(\"The difference in means of patients who survive is significantly different between drug and placebo groups.\"),\n  answer(\"The difference in means of patients who survive is significantly greater in the drug group than the placebo group.\"),\n  answer(\"The difference in proportions of patients who survive is significantly greater in the drug group than the placebo group.\"),\n  allow_retry = TRUE\n)"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#effect-sizes",
    "href": "src/book/17_inference_advanced_chapter.html#effect-sizes",
    "title": "15  More Statistics",
    "section": "15.3 Effect sizes",
    "text": "15.3 Effect sizes\n\n15.3.1 Video overview\n\n\nSlides: PDF"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#confidence-intervals",
    "href": "src/book/17_inference_advanced_chapter.html#confidence-intervals",
    "title": "15  More Statistics",
    "section": "15.4 Confidence intervals",
    "text": "15.4 Confidence intervals\n\n15.4.1 Video overview\n\n\nSlides: PDF\n\n\n15.4.2 Questions\n\n\nquestion(\n  \"You want to conduct a hypothesis test using the `infer` package in R. How should you shuffle the data to generate a null distribution?\",\n  answer(\"Bootstraps\"),\n  answer(\"Permutations\", correct = TRUE, message = \"A permutation shuffles up the values of the explanatory and response variables, and so simulates a world in which there is no relationship between the two (i.e. the null hypothesis).\"),\n  answer(\"It doesn't matter\"),\n  allow_retry = TRUE\n)\n\n\nquestion(\n  \"You want to calculate the confidence interval around a statistic (e.g. the mean) using the `infer` package in R. How can you approximate resampling the original population?\",\n  answer(\"Bootstraps\", correct = TRUE, message = \"By taking random samples of rows from the original sample (*bootstrap resampling*), we can approximate resampling the original population.\"),\n  answer(\"It doesn't matter\"),\n  answer(\"Permutations\"),\n  allow_retry = TRUE\n)"
  },
  {
    "objectID": "src/book/17_inference_advanced_chapter.html#further-reading",
    "href": "src/book/17_inference_advanced_chapter.html#further-reading",
    "title": "15  More Statistics",
    "section": "15.5 Further reading",
    "text": "15.5 Further reading\nIf you wish to do further reading on bootstrapping and confidence intervals, here are some online book chapters on the topic. (They cover the same material, so pick the one that makes the most sense to you.)\n\nStatistical Inference via Data Science by Chester Ismay and Albert Kim\n\nChapter 8: Bootstrapping and Confidence Intervals - https://moderndive.com/8-confidence-intervals.html\n\nStatistical Modeling by Daniel Kaplan\n\nChapter 5: Confidence Intervals - https://dtkaplan.github.io/SM2-bookdown/confidence-intervals.html"
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#interaction-terms",
    "href": "src/book/18_modeling_advanced_chapter.html#interaction-terms",
    "title": "16  More Models",
    "section": "16.1 Interaction terms",
    "text": "16.1 Interaction terms\n\n16.1.1 Independence and interaction\nSo far we have assumed that each term is independent: i.e. each one contributes to the response variable \\(y\\) a separate amount proportional to its coefficient.\nHowever, it is possible for variables to interact with each other:\n\\(y = m_1 x_1 + m_2 x_2 + m_3 x_1 x_2 + c\\)\nThis linear model has an interaction term: \\(x_1 x_2\\). What this means is that the contribution of either variable depends somewhat on the value of the other!\nFor example, consider an experiment where we are looking at the effect of water and sunlight on plant height. Both variables may increase a plant’s height - however they are also dependent on each other. If we keep a plant in complete darkness, then no amount of water will help it grow (and vice versa). In otherwords, the effect of either sunlight or water on plant height depends on how much of the other variable the plant is receiving.\nIt is very easy to add interaction terms to a linear model. For example, the formula above would be written something like:\ny ~ x1 + x2 + x1 * x2\nAs you can see, we just add an asterisk * between variables that we wish to have an interaction between."
  },
  {
    "objectID": "src/book/18_modeling_advanced_chapter.html#hypothesis-tests-for-models",
    "href": "src/book/18_modeling_advanced_chapter.html#hypothesis-tests-for-models",
    "title": "16  More Models",
    "section": "16.2 Hypothesis tests for models",
    "text": "16.2 Hypothesis tests for models\n\n16.2.1 Linear models and hypothesis tests\nYou may remember from our first modeling module that the tidy function reported a p-value for the model and for each explanatory variable. See the p.value columns in both of these examples:\n\nsim_model &lt;- lm(y ~ x, data=sim_df)\n\n\nsim_model %&gt;%\n  glance() %&gt;%\n  select(r.squared, p.value)\n\n# A tibble: 1 × 2\n  r.squared    p.value\n      &lt;dbl&gt;      &lt;dbl&gt;\n1     0.935 0.00000512\n\n\nAs you will remember from our inference modules, the p-value is the probability that the actual data was generated in a world in which the null hypothesis is true.\nNull hypothesis?! But we haven’t specified any hypotheses for our linear model, have we?\nIn fact, there is a null hypothesis for every linear model, although we have never formally written it down. The null hypothesis for a linear model is that there is no relationship between the response and explanatory variables. In other words, our null hypothesis is that the line of best fit is a horizontally flat line (e.g. the blue line in this plot):\n\nsim_df %&gt;%\n  ggplot() +\n  geom_point(mapping = aes(x, y)) +\n  geom_abline(slope = 0, intercept = 0, color = \"blue\") +\n  ylim(-5,25)\n\n\n\n\nIn other words, the p-value is the probability that the data came from this line. Just by looking at it, we can see that this is not very likely, and in fact the p-value is 0.000005 (or 0.0005%), i.e. statistically very unlikely.\nWe continue to use our regular significance threshold, \\(\\alpha = 0.05\\). In this case, we can reject our null hypothesis (that there is no relationship).\nHowever, hypothesis testing only tells us the probability that there is a relationship, or not. If there is a relationship, it doesn’t tell us how good it is, i.e. how well our model fits the data. For this, we need to continue to use the \\(R^2\\) value, as well as our graphs to check the 3 main assumptions of the linear model.\n\n\n16.2.2 p-values of individual variables\nThe tidy() function also reports a p-value for each variable in the model.\n\nsim_model %&gt;%\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)     1.27     1.06       1.20 0.264     \n2 x               1.75     0.164     10.7  0.00000512\n\n\nFor individual variables, the null hypothesis is that each individual explanatory variable has no relationship with the response variable. In other words, our null hypothesis is that the intercept and slope are 0.\nAs we can see for this example, because the p-values of the slope is below 0.05, so we reject our null hypothesis. We find that the non-zero slope of the x variable is statistically significant. However, the intercept has a p-value of 0.26. This is greater than 0.05, so we cannot reject our null hypothesis of a zero intercept for this particular dataset.\n(This dataset was simulated with an intercept of zero, so that makes sense!)\n\nA note on scientific notation\nIn the previous examples, our p-values have been written as strange numbers, such as: 2.637853e-01\nThis is a convenient way of writing numbers that are very large or very small. We can write 1.2 million (1,200,000) as:\n\\(1.2 \\times 10^6\\)\ni.e. 1.2 times 1 million, or\n1.2e6\nwhich uses e to represent “10 to the power of”.\nFor numbers that are less that one, we can do the same thing but use a negative power of ten. For example, we can write 0.263 as:\n\\(2.63 \\times 10^{-1}\\)\nor\n2.63e-1\nbecause \\(10^{-1}\\) is 0.1"
  },
  {
    "objectID": "src/book/20_simulation_chapter.html",
    "href": "src/book/20_simulation_chapter.html",
    "title": "17  Simulation",
    "section": "",
    "text": "Under Construction\n\n\n\nThis chapter is not yet complete."
  },
  {
    "objectID": "src/book/21_prediction_chapter.html#predictive-modeling",
    "href": "src/book/21_prediction_chapter.html#predictive-modeling",
    "title": "18  Prediction",
    "section": "18.1 Predictive modeling",
    "text": "18.1 Predictive modeling\n\n18.1.1 Video overview\n\n\nLink to slides: PDF"
  },
  {
    "objectID": "src/book/21_prediction_chapter.html#data-splits",
    "href": "src/book/21_prediction_chapter.html#data-splits",
    "title": "18  Prediction",
    "section": "18.2 Data splits",
    "text": "18.2 Data splits\n\n18.2.1 Video overview\n\n\nLink to slides: PDF"
  },
  {
    "objectID": "src/book/21_prediction_chapter.html#linear-models-for-prediction",
    "href": "src/book/21_prediction_chapter.html#linear-models-for-prediction",
    "title": "18  Prediction",
    "section": "18.3 Linear models for prediction",
    "text": "18.3 Linear models for prediction"
  },
  {
    "objectID": "src/book/21_prediction_chapter.html#logistic-regression-for-categorical-predictions",
    "href": "src/book/21_prediction_chapter.html#logistic-regression-for-categorical-predictions",
    "title": "18  Prediction",
    "section": "18.4 Logistic regression for categorical predictions",
    "text": "18.4 Logistic regression for categorical predictions\n\n18.4.1 Video overview\n\n\nLink to slides: PDF"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#what-is-deep-learning",
    "href": "src/book/22_deep_learning_chapter.html#what-is-deep-learning",
    "title": "19  Deep Learning",
    "section": "19.1 What is Deep Learning?",
    "text": "19.1 What is Deep Learning?\nIn the Prediction module we learned how we could use models to make predictions about a response variable.\nAll of the models we considered were fairly simple: we took some inputs, fed them into a mathematical equation, and got an output. This simplicity makes it easy to understand the connection between the inputs and output. However, it also limits the accuracy of our predictions when this relationship is complicated.\nConsider the challenge of seeing an image (the input) and identifying what it is an image of (the output). Methods like linear models struggle with this type of prediction because there is not an obvious linear connection between the pixels of an image and the object that the image represents.\nHowever, the human brain is very good at this type of task! This inspired computer scientists to study the human brain and try to develop computer models that work in a similar manner.\nThe human brain (and nervous system) is composed of long cells called neurons. Each neuron has lots of arms that connect to other neuron cells. Electrical pulses travel up and down these arms, and based on the signals that each neuron receives, it then sends its own signal on to other neurons.\n\n\n\n\n\n\nThus each neuron is a bit like one of the models that we created in the Prediction module. However, the brain is composed of many of these interconnected neurons.\nCould we make a brain-like computer model by connecting together many simple models?\nIt turns out that the answer is yes! These are called artifical neural networks and work by building up layers of simple models:\n\n\n\nA simple artifical neural network.\n\n\nEach of the circles in this picture represents a node.\n\nThere are three input nodes (explanatory variables) to this neural network (the yellow circles).\nThere are two “hidden layers” (blue and green). Each of the nodes in these layers takes all of the nodes in the previous layer as inputs, and then sends its output to all of the nodes in the next layer.\nThe final layer in this example contains a single node. This node will produce a single prediction (e.g. yes/no, true/false, or a single number).\n\nIn subsequent sections we will learn more about all of these pieces, such as:\n\nWhat are the input nodes, and how many do we have?\nWhat are the models at each of nodes in the hidden layers?\nHow many output nodes do we need, and what should they output?\nHow do we train these networks to make accurate predictions?"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#deep-learning-in-r",
    "href": "src/book/22_deep_learning_chapter.html#deep-learning-in-r",
    "title": "19  Deep Learning",
    "section": "19.2 Deep learning in R",
    "text": "19.2 Deep learning in R\nWe will be using a package called torch to create neural networks. torch is the R version of a machine learning software library called PyTorch, which was originally developed by Facebook for their AI research.\nWe will also be using two additional packages:\n\ntorchvision is an R package that contains helpful code (and datasets) for working with images.\nluz is an R package that provides lots of helpful functions to make training torch neural networks much easier."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#the-mnist-dataset",
    "href": "src/book/22_deep_learning_chapter.html#the-mnist-dataset",
    "title": "19  Deep Learning",
    "section": "19.3 The MNIST dataset",
    "text": "19.3 The MNIST dataset\nAs an example, we will use the MNIST dataset, which is a famous dataset containing pictures of numbers from 0 to 9, such as this one:\n\n\n\n\n\nBefore we can build our model, we need to understand how data is stored in digital images.\nDigital images are composed of small squares called pixels. Each pixel has a single color.\nThe MNIST images are 28 pixels wide and 28 pixels high. They are greyscale images, which means that each pixel has a single color that is somewhere on the spectrum from white to black. There are 256 possible shades on this spectrum, each represented by an integer number from 0 (white) to 255 (black).\nFor example, this image shows the intensities in each of the pixels of one of the MNIST images:\n\n\n\nA simple artifical neural network.\n\n\nEach pixel in our image will be an input to the model. Since there are 28 by 28 pixels in each images, then we will need \\(28 \\times 28 = 512\\) input nodes in any artificial neural network that we create.\nThere are 60,000 training images in the MNIST dataset, and an additional 10,000 test images."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#data-in-torch",
    "href": "src/book/22_deep_learning_chapter.html#data-in-torch",
    "title": "19  Deep Learning",
    "section": "19.4 Data in torch",
    "text": "19.4 Data in torch\nThe torch package comes with its own concept of a dataset, that is different to previous R datasets that we have worked with.\nFortunately the MNIST dataset is already built-in to the torchvision package, and so there is a handy function called mnist_dataset() that we can use to load it from our computer’s hard drive (or download it, if this is the first time running the function):\n\ntrain_ds &lt;- mnist_dataset(\n  \"images\", \n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    torch_flatten()\n  )\n\nIf you are looking to modify the mnist_dataset() function, then note a few things:\n\nThe first argument, \"images\" is the name of the folder where the data will be stored.\ndownload = TRUE means that the dataset will be downloaded if it can’t be found in the images folder.\nThe transform parameter tells torch how to load each image. Here we are converting each image to a tensor (a 28x28 grid of numbers), and then flattening that grid into one long 1x784 row of numbers. (So that we can treat each image like the row of a dataframe with 784 columns [one column for each pixel]).\n\nThe training dataset loaded above contains the 60,000 training images. We can also use the mnist_dataset() function to load the 10,000 testing images. Note that everything is the same, except that we have added the train = FALSE argument to the function:\n\ntest_ds &lt;- mnist_dataset(\n  \"images\",\n  download = TRUE,\n  train = FALSE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    torch_flatten()\n)"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#torch-dataloaders",
    "href": "src/book/22_deep_learning_chapter.html#torch-dataloaders",
    "title": "19  Deep Learning",
    "section": "19.5 torch dataloaders",
    "text": "19.5 torch dataloaders\nThere’s an extra step to stetting up our data in torch so that it’s ready for our model: we need to create dataloaders.\nSince deep learning datasets are often large, we cannot fit all the data into the computer’s memory at once. Instead, torch will load the data in batches, tune the neural network to that batch, and then repeat on the next batch of data. Loading batches of data is the job of a dataloader, which we need to create with the dataloader() function:\n\ntrain_dl &lt;- dataloader(\n  train_ds, \n  batch_size = 32,\n  shuffle = TRUE\n  )\n\nNote: * The first argument is a torch dataset - here we are using the training train_ds dataset. * We specify that we want to load the images in batches of 32 images at a time. * We will also shuffle up the training images when we load them (this will help with training our model in a future step)."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#splitting-the-dataset-for-training-and-validation",
    "href": "src/book/22_deep_learning_chapter.html#splitting-the-dataset-for-training-and-validation",
    "title": "19  Deep Learning",
    "section": "19.6 Splitting the dataset for training and validation",
    "text": "19.6 Splitting the dataset for training and validation\nRecall  that it’s a good idea to split our training set into training and validation data, so that we can tune the model’s hyperparameters without overfitting to the test dataset.\nInstead of cross-validation, we will simply take a fixed subset of the training images to be the validation set, which we can do with the dataset_subset() function from the torch package. Here we are selecting the last 10000 images to be our validation set:\ntrain_ds &lt;- dataset_subset(train_ds, indices=1:50000)\nvalid_ds &lt;- dataset_subset(train_ds, indices=50001:60000)\n\nNow we need to recreate the training dataloader train_dl on the reduced train_ds dataset as well as creating a new dataloader for the data in the validation subset:\n\ntrain_dl &lt;- dataloader(\n  train_ds, \n  batch_size = 32,\n  shuffle = TRUE\n  )\n\nvalid_dl &lt;- dataloader(\n  valid_ds,\n  batch_size = 32\n  )\n\nNote that we do not need to shuffle the images for the validation set, because we are not using those to train the model (and the order doesn’t matter when we are just using images to see how accurate the model is)."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#inspecting-the-data",
    "href": "src/book/22_deep_learning_chapter.html#inspecting-the-data",
    "title": "19  Deep Learning",
    "section": "19.7 Inspecting the data",
    "text": "19.7 Inspecting the data\nOne of the tricky parts of creating a deep learning model is making sure that you get the right number of nodes in each layer. Do do that, we need to understand the dimensions of our input data.\nThe dataloaders will convert batches of 32 images at a time into something called a tensor. This is essentially a table of numbers. We can use the following code to see the dimensions of the first batch of training data:\n\nbatch &lt;- train_dl$.iter()$.next()\nbatch[[1]]$size()\n\n[1]  32 784\n\n\nThe tensor is 32 by 784 - in other words, 32 rows (each representing one of the images in the batch) and 784 columns (each representing a pixel in one of those 28x28 images)."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#visualizing-an-image",
    "href": "src/book/22_deep_learning_chapter.html#visualizing-an-image",
    "title": "19  Deep Learning",
    "section": "19.8 Visualizing an image",
    "text": "19.8 Visualizing an image\nIt’s sometimes helpful to look at a few of the training images to get an idea of the data that they contain. Here we are using a custom function called visualize_image() which takes a number from 1-32 (corresponding to one of the images in the batch from the training set), and shows it:\n\nvisualize_image(1)"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#creating-the-network",
    "href": "src/book/22_deep_learning_chapter.html#creating-the-network",
    "title": "19  Deep Learning",
    "section": "19.9 Creating the network",
    "text": "19.9 Creating the network\nFinally we are ready to define the shape of our network. We know that we need 784 inputs to the model (one for each pixel), and 10 outputs at the end (one for each of the 10 digits we are trying to predict).\nTo create the model, we use the nn_module() function. Here we are creating a model with a single hidden layer of 256 nodes:\n\nnet &lt;- nn_module(\n  \"onelayer\",\n  initialize = function() {\n    self$net &lt;- nn_sequential(\n      nn_linear(784,256),\n      nn_relu(),\n      nn_linear(256,10)\n    )\n  },\n  forward = function(x) {\n    self$net(x)\n  }\n)\n\n\nThe first argument is just a name for the network (this could be any character string).\nThe second parameter, initialize is passed a new function in which we define the network’s layers. The key steps here are inside the nn_sequential() function. With the code:\nnn_linear(784,256),\nnn_relu(),\nnn_linear(256,10)\n\nwe first define the hidden layer of 256 nodes (each of which gets 784 inputs) with the nn_linear() function,\nwe then use a function nn_relu() which is called an activation function (we should put an activation function after every hidden layer),\nand finally the last nn_linear() layer creates the 10 output nodes (each of which takes as input the 256 outputs of the previous hidden layer).\n\n\nWhat’s going on with these “linear” nodes and the “activation function”?\nFundamentally, each node in the hidden layer just contains a linear model (i.e. a linear combination of all its inputs)! However, the relationship between image pixel intensities and the digits 0-9 is non-linear. Therefore we convert each node’s linear output into a non-linear output.\nWe did something very similar with logistic regression (where we used the sigmoid function to convert from a linear output to a response between 0 and 1). Here we are using a function called a “Rectified Linear Activation function” or relu for short. You don’t need to understand the details of the relu activation for this module, but there are many good tutorial or videos about it on the internet if you want to learn more (such as this article)."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#set-up-the-model",
    "href": "src/book/22_deep_learning_chapter.html#set-up-the-model",
    "title": "19  Deep Learning",
    "section": "19.10 Set-up the model",
    "text": "19.10 Set-up the model\nNext we can use the setup() function from the luz package to define how we wish to train this model. Note that we pipe in the net that we created in the previous section.\n\nmodel &lt;- net %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(), \n    optimizer = optim_adam, \n    metrics = list(\n      luz_metric_accuracy()\n    )\n  )\n\n\nloss defines how we wish to measure the model’s error (how wrong it’s predictions are). This is the score that we will attempt to reduce as we train the model on batches of 32 images at a time.\noptimizer sets the algorithm to train the neural network on each batch. We are using an optimizer called the Adam optimizer which generally works well for most neural networks.\nmetrics defines any additional scores that we wish to calculate during the training process - here we are telling R to report the accuracy as well."
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#training-the-model",
    "href": "src/book/22_deep_learning_chapter.html#training-the-model",
    "title": "19  Deep Learning",
    "section": "19.11 Training the model",
    "text": "19.11 Training the model\nAt last we are ready to train the model!\nWe can use the fit() function (also from the luz package) to quickly define the training process to run:\nfitted &lt;- model %&gt;% \n  fit(\n    train_dl, \n    epochs = 10,\n    valid_data = valid_dl,\n    verbose = TRUE\n    )\n\nThe first two arguments are (1) the model that we created from the setup() function, and (2) the training dataloader train_dl to generate batches of 32 training images.\nThen we specify that we want to run the model for 10 epochs. An epoch is one training cycle over all of the training images. Since we are training the model by incrementally improving it on all of the images, we will want to go over the entire dataset multiple times to keep improving the model.\n\n\n\nWe also specify a validation dataset here, by passing the valid_dl dataloader to the valid_data parameter.\nFinally, verbose = TRUE means that the function will print out the results of the training as it goes (when you run the function, you should see these either below the code chunk or in the Console).\n\n\n\n\n\n\n\nFor Mac users\n\n\n\nIf you have a recent-ish Mac with an M1 or M2 chip, then the code above will generate an error because the GPU (graphics) part of those chips are not yet fully supported by the torch package.\nTo get it to run, you will need to force your code to run on the CPU part of the chip (i.e. on the regular processor). To do this you will need to create a torch accelerator using the accelerator() function, and then pass that accelerator to the fit() function, e.g.\nacc &lt;- accelerator(cpu=TRUE)\nfitted &lt;- model %&gt;% \n  fit(\n    ...,\n    accelerator = acc\n    )"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#assess-the-training-history",
    "href": "src/book/22_deep_learning_chapter.html#assess-the-training-history",
    "title": "19  Deep Learning",
    "section": "19.12 Assess the training history",
    "text": "19.12 Assess the training history\nWe can use the plot() function on the output of the fit() function to create a ggplot2 graph showing how the loss and the accuracy change over the epochs that we trained the model for:\n\nplot(fitted)\n\n\n\n\n\n\nThe left hand column of graphs show how the good the model’s predictions were on the training data (at the end of every epoch of improving the model). The right hand graphs show the same metrics, but on the validation data.\nRemember that the accuracy is the fraction of images that the model predicts correctly (so a high number is better). The loss is the opposite - it’s a measure of how many predictions the model gets wrong (so we want it to decrease).\nThese graphs can tell us about two things:\n\nGeneralization error.\nOverfitting.\n\nGeneralization error: Remember that because we train the model on the training data, it is optimized to predict the correct labels for the training data. However, this is likely to be an overestimate of how good the the model is, since of course it’s going to be good at predicting the stuff it learned how to predict. The real test is whether it can also make good predictions on data is has never seen before, such as the validation dataset.\nWe can compare how well the model generalizes by comparing the metrics between the training and validation datasets. Usually the model will do slightly worse on the validation set, but as long as they are similar then this is a sign that our model is able to make good predictions on new data (we would say that it generalizes well, or has low generalization error).\nIn the plots above, we can see that the losses and accuracies are similar, and so we have a low generalization error.\nOverfitting: Neural networks are so powerful that they can sometimes fit the training data too well. If we keep on training for more epochs, then the model is likely to get increasingly good at predicting the training data. However, it will plateau or even get worse at predicting data that it hasn’t seen before (such as the validation data).\nWhen the plots above show that the validation metrics are plateauing or declining, even though performance on the training data is still improving, then we say that the model is starting to overfit the training data.\n(Typically we want to stop training after just enough epochs to get the best performance on the validation data, without overfitting to the training data.)"
  },
  {
    "objectID": "src/book/22_deep_learning_chapter.html#testing",
    "href": "src/book/22_deep_learning_chapter.html#testing",
    "title": "19  Deep Learning",
    "section": "19.13 Testing",
    "text": "19.13 Testing\nOur last step is to test our best model on the test set.\nFirst we need to write a new dataloader for the test data (which we will do in the same way as we created the validation dataloader):\n\ntest_dl &lt;- dataloader(\n  test_ds, \n  batch_size = 32\n  )\n\nThen we can use the evaluate() function to calculate the loss and accuracy on that test data:\n\nevaluate(fitted, test_dl)\n\n\n\n\n\n\n\nFor Mac users\n\n\n\nAs with the fit() function, if you have a Mac with an M1 or M2 chip, then you will also need to pass your accelerator to the evaluate() function, e.g.\nevaluate(fitted, test_dl, accelerator = acc)\n(Here we are using the same accelerator that we used previously.)"
  },
  {
    "objectID": "src/book/29_online_data_chapter.html",
    "href": "src/book/29_online_data_chapter.html",
    "title": "20  Online Data",
    "section": "",
    "text": "Under Construction\n\n\n\nThis chapter is not yet complete."
  },
  {
    "objectID": "src/book/33_databases_chapter.html#relational-data",
    "href": "src/book/33_databases_chapter.html#relational-data",
    "title": "21  Databases",
    "section": "21.1 Relational data",
    "text": "21.1 Relational data\nA database is simply an organized way of storing data.\nThere are obviously many ways that one could choose to organize data, but a particularly common form of database is the relational database, in which data is broken up into multiple tables. In general the goal of a relational database is to avoid repeating the same piece of data in multiple rows.\nFor example, in the dataset of hobbits and addresses in the previous section, we can avoid repeating the same address for Bilbo and Frodo by splitting the addresses into a separate table. Then every hobbit who lives at a particular address gets a link to that address row in the separate address table (instead of repeatedly writing out the same address for each hobbit).\nIn addition, relational databases follow the same rules that tidy datasets have to follow:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nRelational databases are extremely common (for example, almost every website you visit has a relational database that stores the website’s information), and learning how to work with these databases is a vital skill for every data scientist.\n\n21.1.1 Joining matching rows\nIn such cases, there are often columns in each table that link the two tables back together.\nFor example, consider these two tables:\n\n\n# A tibble: 3 × 3\n  names   age hobbit_hole\n  &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Bilbo   111           1\n2 Frodo    50           1\n3 Sam      38           2\n\n\n# A tibble: 2 × 2\n     id address                \n  &lt;dbl&gt; &lt;chr&gt;                  \n1     1 Bag End, Hobbiton      \n2     2 3 Bagshot Row, Hobbiton\n\n\nThe first table contains three hobbits. However, instead of listing out their addresses, we have instead recorded a number that corresponds to the id column of the second table.\nBy matching the hobbit_hole column with the id column, we can see that Bilbo and Frodo both live at Bag End whereas Sam lives at 3 Bagshot Row.\nSome terminology:\n\nIn the mathematical theory of databases, each of these tables would be called a “relation”, hence the name relational data for data stored across multiple tables in this fashion.\nThe id column in the table of addresses is an example of a key. A key is any column (or columns) that provide a unique way of identifying every row in a table.\nThe hobbit_hole column in the table of hobbits is a foreign key. This links each hobbit to the identifying key in a different (i.e. “foreign”) table.\n\nOf course, we could just have recorded this data as a single table! For example:\n\n\n# A tibble: 3 × 3\n  names   age hobbit_hole            \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;                  \n1 Bilbo   111 Bag End, Hobbiton      \n2 Frodo    50 Bag End, Hobbiton      \n3 Sam      38 3 Bagshot Row, Hobbiton\n\n\nWhat are the advantages to splitting data over multiple tables?\n\nOne reason is to avoid repeating certain values. For example, we have written the address Bag End twice. If we split addresses into a separate table then we only need to write it once. This means that:\n\nWe need less space to store our data.\nWe can reduce errors and inconsistencies (for example, we won’t end up with two different spellings of Bag End by accident (Bag End vs. Bag-End)).\nUpdating the dataset is easier (if we later decided that Bag End did need to be spelled with a hyphen then we would only need to update it in one place - otherwise we might easily update it for one hobbit and forget to do so for another).\n\n\nOf course, sometimes we need to combine data from multiple tables into a single dataframe for analysis.\nTo do this we need to join the separate datasets back together, typically using by using columns that link to connected rows in different tables (such as keys and foreign keys).\nThere are several different ways to join tables, which will link rows in different ways.\nTo illustrate these different joins let us imagine that we are ecologists in a similar world to that inhabited by the hobbits from the previous section. We have gathered the following two tables of data about different species inhabiting this unusual world.\n\nspecies &lt;- tibble(\n  species = c(\"Giant Eagle\", \"Goblin\", \"Goblin\", \"Giant Spider\", \"Balrog\"),\n  location = c(2, 2, 3, 1, NA)\n)\n\nsightings &lt;- tibble(\n  location_id = c(1,2,3,4),\n  name = c(\"Mirkwood Forest\", \"Misty Mountains\", \"Mordor\", \"The Shire\")\n)\n\nOne important thing to note is that there are some rows in each table that do not have a match in the other table.\n\nThe default type of join is an inner join (if somebody ever says just “join” then they are usually referring to an inner join).\nAn inner join matches rows that have the same values in some column(s). Any rows in either table that do not have a match are omitted.\nWe will use the inner_join() function from the dplyr package which has the following signature:\ninner_join(x, y, by=NULL)\n\nx and y are two dataframes that we wish to join.\nThe by parameter specifies which columns to use for matching rows. If we do not specify it then R will automatically match any columns with the same names. It is a good idea to always specify it to make sure that we are only joining on the columns that we want to match!\n\nNote that: * We are piping in the species tibble to the inner_join() function, but we could have written it inside the function instead. * The argument to by is a named vector specifying the names of the columns. * \"location_id\" is the only value in this 1 item vector, but we have also named this item as \"location\". * R will look for the column \"location\" in the first dataframe (which is species in this example), and location_id in the second dataframe (sightings).\n\nspecies %&gt;%\n  inner_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 4 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n\n\nIn our output we only have rows that had a matching row in the other dataframe (i.e. we dropped the Balrog row from the species dataframe and The Shire from the sightings table).\nAlso, note that the Misty Mountains now appear twice in the output, even though there is only one Misty Mountains row in the sightings table."
  },
  {
    "objectID": "src/book/33_databases_chapter.html#database-software-and-sql",
    "href": "src/book/33_databases_chapter.html#database-software-and-sql",
    "title": "21  Databases",
    "section": "21.2 Database software and SQL",
    "text": "21.2 Database software and SQL\nYou could create your own simple version of a relational database by storing each of your tables in a separate file on your computer’s harddrive. For simple applications this is a valid solution - however in more complex projects you start to run into problems with ensuring that tables contain the corret data, or that different programs can simultaneously access the data (and don’t accidentally overwrite an update to a file created by another computer program), or simply with efficiently wrangling large amounts of data.\nWe can solve all of these problems by using a dedicated database program that takes of care of storing our data for us. In addition to storing the data for us, these database management systems (DMS) will also allow us to query the data (i.e. access it) and will run those queries in an efficient manner.\nRelational databases all use a special programming language for accessing their data. This programming language is called SQL.\n\nSQL stands for Structured Query Language. It is properly pronounced using by spelling out the acronym: “ess-kew-ell”. However, some people pronounce it as “sequel”.\n\nFortunately we do not need to learn SQL for this class (although you should learn SQL at some point if you wish to do more work with data in the future, since most of it will be stored in databases). Instead we can interact with relational databases using the data wrangling functions that we have already learned.\nI.e. we can use functions like select(), filter(), and inner_join() directly on a relational database and R will translate these into the SQL programming language for us!\n\n21.2.1 Connecting to a database.\nSince a database management system is a separate software program, we need first need to set up a connection to that database software from R.\nTo do this we can use an R package called DBI. In these examples we will connect to a database program called SQLite. (There are many other relational database systems (MySQL, PostgreSQL, etc.) but we could connect to those with very similar code.)\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \"my_database_name.sqlite\")\nWhat is this code doing?\n\nThe dbConnect() function from the DBI package sets up the connection to the database.\n\nNote that we have used the :: operator in the general form package_name::function to access the function from a particular package. This way we do not have to load the package first with the library() function.\n\nThen we use the SQLite() function from the `RSQLite package to specify that this database uses the SQLite dabase management system.\nFinally we have to specify the location of the database. SQLite stores databases as a file on your computer, so here we have just written the name of such a file: \"my_database_name.sqlite\".\n\n\n\n21.2.2 Listing tables\nWe can get a list of all the tables in a database by running this DBI package function on our new connection:\nDBI::dbListTables(con)\n\n\n21.2.3 Connecting to a database table\nTo access a table in our database with our standard data wrangling functions, we need to create a new R variable that points to that table. We do this with the tbl() function from the dbplyr package:\nsome_table &lt;- tbl(con, \"table_name\")\nThis code creates a new variable called some_table that allows us to access a particular database table.\nYou will need to create a new variable in the same way for every database table that you wish to access from R.\n\nNote that the dbplyr package is different to dplyr:\n\ndplyr contains the data wrangling functions that we have used to interact with R dataframes (e.g. filter(), select(), etc.).\ndbplyr allows us to use these data wrangling functions on databases by translating the functions into SQL, and then converting the database output back into an R dataframe.\n\n\n\n\n21.2.4 An example: selecting columns\nWe can now use our data wrangling functions on the database table just as if it was an R dataframe.\nFor example, to select() columns:\nexample_query &lt;- some_table %&gt;%\n  select(example_column_1, example_column_2)\nOne difference, however, is that this code will not create a new dataframe. Instead it creates a SQL query that can be run on the database (in the future) to create a new dataframe.\nWe have stored this query in a new variable called example_query. If we wish, we can view what the SQL query looks like by running the show_query() function:\nexample_query %&gt;%\n  show_query()\nTo run the query and return a dataframe, you need to run the collect() function on the query:\nexample_df &lt;- example_query %&gt;%\n  collect()\nThis will retrieve the data from the database and store it as an R dataframe in a new variable called example_df."
  },
  {
    "objectID": "src/book/33_databases_chapter.html#joining-data-from-relational-database-tables",
    "href": "src/book/33_databases_chapter.html#joining-data-from-relational-database-tables",
    "title": "21  Databases",
    "section": "21.3 Joining data from relational database tables",
    "text": "21.3 Joining data from relational database tables\nIn the More Data Wrangling chapter, we learned that we could join separate tables of data together by matching rows with\n\n21.3.1 Outer joins\nSometimes we want to preserve a row from one (or both) of the tables we are joining even if there is no match with the other table. These are called outer joins, and there are three types depending on which table(s) we want to keep unmatched rows from:\n\nA left outer join (or just “left join”) keeps all rows from the table on the left (i.e. the first dataframe we name).\nA right outer join (aka “right join”) keeps the rows in the table on the right instead (the second dataframe we name).\nA full outer join (“full join”) keeps all rows from both tables.\n\nAny rows that are kept but not matched to a row in the other table will just have missing data in the other table’s columns.\nThe process for doing these joins is very similar to an inner join.\n\n21.3.1.1 Left outer joins\n\nspecies %&gt;%\n  left_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 5 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 Balrog             NA &lt;NA&gt;           \n\n\n\n\n21.3.1.2 Right outer joins\n\nspecies %&gt;%\n  right_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 5 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 &lt;NA&gt;                4 The Shire      \n\n\n\n\n21.3.1.3 Full outer joins\n\nspecies %&gt;%\n  full_join(sightings, by=c(\"location\" = \"location_id\"))\n\n# A tibble: 6 × 3\n  species      location name           \n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Giant Eagle         2 Misty Mountains\n2 Goblin              2 Misty Mountains\n3 Goblin              3 Mordor         \n4 Giant Spider        1 Mirkwood Forest\n5 Balrog             NA &lt;NA&gt;           \n6 &lt;NA&gt;                4 The Shire"
  },
  {
    "objectID": "src/book/33_databases_chapter.html#non-relational-databases",
    "href": "src/book/33_databases_chapter.html#non-relational-databases",
    "title": "21  Databases",
    "section": "21.4 Non-relational databases",
    "text": "21.4 Non-relational databases"
  },
  {
    "objectID": "src/book/35_papers_chapter.html",
    "href": "src/book/35_papers_chapter.html",
    "title": "22  Papers",
    "section": "",
    "text": "Under Construction\n\n\n\nThis chapter is not yet complete."
  },
  {
    "objectID": "src/book/37_dashboards_chapter.html",
    "href": "src/book/37_dashboards_chapter.html",
    "title": "23  Dashboards",
    "section": "",
    "text": "Under Construction\n\n\n\nThis chapter is not yet complete."
  },
  {
    "objectID": "src/book/references.html",
    "href": "src/book/references.html",
    "title": "References",
    "section": "",
    "text": "Baker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nKnuth, D. E. 1984. “Literate Programming.”\nThe Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#sec-faqs-software-setup",
    "href": "src/book/A_faqs_appendix.html#sec-faqs-software-setup",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.1 Software setup",
    "text": "A.1 Software setup\n\nA.1.1 Git is no longer installed on my computer\nThis sometimes happens on Macs after updating the MacOS operating system. You should re-install Git by repeating the first item from Section 2.4.1. Then restart RStudio"
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#working-with-git-and-github",
    "href": "src/book/A_faqs_appendix.html#working-with-git-and-github",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.2 Working with Git and GitHub",
    "text": "A.2 Working with Git and GitHub\n\n\nA.2.1 Cannot push to GitHub: “Password authentication was removed”\nYou might find yourself with an error like this when you try to clone a repository from GitHub into RStudio, or push commits back to GitHub:\n\nremote: Support for password authentication was removed on August 13, 2021.\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\nfatal: Authentication failed for 'https://github.com/mason-cds101/final-project-dominicwhite/'\nThis usually means that you have one of the following problems with your GitHub Personal Access Token:\n\nIncorrect token: You have not correctly created and set your GitHub token (as per the instructions in step 4 of Section 2.4.1). If this is the first time you are trying to clone from or push to GitHub, this is probably the reason.\nMake sure that:\n\nYour token is the “Classic” token type, not the Fine-grained type.\nWhen you create your token, you give it permission to access and modify your repositories by clicking the “Repo”check-box in the list of Scopes.\n\nMissing token: If you are using the online version of RStudio at https://posit.cloud, you will need to repeat the commands to store your GitHub token every time you want to Push code from a new project.\nExpired token: Your previous token has expired, and you will need to create a new one and store it in RStudio. If your token used to allow you to clone from and push to GitHub but has suddenly stopped working, then this is probably the reason.\n\n__Solution:__In all cases, to fix the problem you should re-run the following three lines of R code in from Section 2.5 in the RStudio Console to create a new GitHub token and then store that new token in RStudio (if it asks what you want to do, such as setting a new token, type in the number of the option you want in the Console and then hit enter). Make sure to do this exactly as described in Section 2.5\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nusethis::create_github_token()\ngitcreds::gitcreds_set()\n\n\nA.2.2 Cannot push to GitHub: “Updates were rejected”\nSometimes an error like the following occurs when trying to push to GitHub.\n\n\n\nRStudio error message when Pushing: Updates where rejected\n\n\nWhat this means is that you have somehow ended up with extra commits in the version of the project on Github that you do not have in the version in RStudio. (This can happen if you do things like manually uploading files to a GitHub repository without going through RStudio, or editing files on GitHub via the website instead of in RStudio. Both of these actions create a new commit on GitHub, but since commits are not automatically synced, they will not be in RStudio.)\nTo fix this, you will need to download the new commits from GitHub to RStudio. This is called “Pulling” (since it is the opposite of Pushing). There is a button in the Git tab or pop-up in RStudio labelled Pull with a blue downwards arrow icon. Click this to Pull the changes from GitHub-only commits into RStudio, and then try to Push again to send your RStudio-only commits to GitHub.\n\n\nA.2.3 The Git tab is missing in RStudio\nThis typically occurs for one of two reasons: * Most common reason: you don’t have a project open in RStudio Desktop, or the project you do have open is not a Git-enabled project (probably because you created a new blank RStudio project instead). If you are trying to working on a project from this book and you created it in RStudio by cloning a GitHub repository, then it should be automatically Git-enabled. Try re-opening the RStudio project associated with the code. In RStudio Desktop, you can see the current project in the top-right, and change it by clicking there. At https://posit.cloud, you can change the project by going back to your homepage where all the projects are listed. * Less commonly, this happens in RStudio Desktop because Git is not installed (or RStudio can’t find it). Try either re-installing Git, or looking at the FAQs in the Section A.1 section for reasons why Git might be missing."
  },
  {
    "objectID": "src/book/A_faqs_appendix.html#packages",
    "href": "src/book/A_faqs_appendix.html#packages",
    "title": "Appendix A — Frequently Asked Questions",
    "section": "A.3 Packages",
    "text": "A.3 Packages\n\nA.3.1 Installing an older version of a package\nTo install an older version of a package (e.g. the lmvar package), you can run these two lines of code, making sure to replace the name of the package and the version that you want to install:\nrequire(remotes)\ninstall_version(\"lmvar\", version = \"1.5.2\", repos = \"http://cran.us.r-project.org\")"
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-create-new-rstudio-project",
    "href": "src/book/B_additional_setup_appendix.html#sec-create-new-rstudio-project",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.1 How to clone a GitHub repository into RStudio",
    "text": "B.1 How to clone a GitHub repository into RStudio\n\nB.1.1 In RStudio Cloud\n\nFrom your homepage, click on the New Project button in the top right of the screen (Figure B.1)\n\n\n\n\nFigure B.1: The New Project Button in RStudio Cloud\n\n\n\nIn the drop-down menu, click on the option that says “New Project from Git Repository” (Figure B.2).\n\n\n\n\nFigure B.2: The menu of different ways to create a new project.\n\n\n\nIn the pop-up window, paste in the URL (web address) of the GitHub repository that you wish to open in RStudio (Figure B.3).\n\n\n\n\nFigure B.3: An example of the kind of GitHub repository URL (web address) you need to type in.\n\n\n\n\nB.1.2 In RStudio Desktop\n\nClick on the New Project option in the Files dropdown menu).\nIn the New Project wizard that pops up, click on the option that says “Version Control” (Figure B.4)\n\n\n\n\nFigure B.4: Step 1 of RStudio’s New Project wizard.\n\n\n\nOn the next page of the wizard, click on the “Git” option (Figure B.5)\n\n\n\n\nFigure B.5: Step 2 of RStudio’s New Project wizard.\n\n\n\nOn the final page of the wizard (Figure B.6), fill in the details for the GitHub repository you wish to “clone” (i.e. download), and where to download it to:\n\n\n\n\nFigure B.6: Step 3 of RStudio’s New Project wizard.\n\n\n\nIn the first field (“Repository URL”) copy-and-paste the web address of the GitHub repository’s homepage.\nThe second field will be the name of the folder created on your computer to hold all the files you are downloading from the GitHub repository. It may auto-fill with the repository’s name - you can also type something in, or change it to a different folder name if you prefer.\nThe third field is the name of the parent folder that will hold the folder above. I would recommend organizing related projects (e.g. the projects from this book) in a single parent folder so that they are easy to find. If you click “Browse” you can choose or create a parent folder to hold all your project folders.\n\nThen click “Create Project”."
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-stage-commit-push",
    "href": "src/book/B_additional_setup_appendix.html#sec-stage-commit-push",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.2 Stage, commit and push from RStudio to GitHub",
    "text": "B.2 Stage, commit and push from RStudio to GitHub\nIf you are working on a Git-enabled project in RStudio, then you can periodically make checkpoints as you make progress on your work. Git refers to these checkpoints as commits, and we refer to this process in general as version control.\nIf your RStudio project is using Git for version control, then the top-right pane of RStudio will contain a Git tab.\n\n\n\n\n\n\nHelp: I don’t see the Git tab…\n\n\n\n\n\nIf you do not see the Git tab, but you are expecting to see it, then either:\n\nThe most likely cause is that you do not have the correct Project open in RStudio (check the current project in the top-right corner of RStudio, and then open the correct one).\nYou do not have the Git software installed. This needs to be installed separately to RStudio Desktop, as we did in Section 2.4.1 of this book. If you already have Git installed, then is is possible that…\nIn rare cases, RStudio can sometimes not find where Git is installed on your system. This can sometimes happen after a software or operating system update on a computer. The way to fix this is by manually telling RStudio where to find Git on your computer. To do this, first locate the path to the Git program on your computer, and then open up the Tools &gt; Global Options settings of RStudio and in the Git/SVN section, put this path in the Git executable box.\n\n\n\n\nThe first thing to remember is that you can only make a new commit if you have made changes to at least one of the Project’s files since the last commit. In addition, the changes to those files need to be saved to the hard drive (because Git cannot detect the changes otherwise).\nStep 1: Stage files. In the Git tab check the Staged checkbox next to any modified file that you wish to add to the next commit (Figure B.7).\n\n\n\nFigure B.7: The Git tab in RStudio with a staged file\n\n\n\nStep 2: Make the commit.: Make the commit (i.e. the checkpoint) by clicking the Commit button in the Git tab. A pop-up will appear. In the Commit message box, write a short message that briefly summarizes the changes that you made (Figure B.7). Then click the “Commit” button below the message box.\n\n\n\n\nFigure B.8: The commit pop-up dialog\n\n\nAssuming everything worked correctly, you should see another message appear that will include the commit message you just wrote along with the number of files you staged, and the number of “insertions” and “deletions” (i.e. the number of lines that you modified in those staged since they were last committed). Figure B.9 shows an example of what such a message looks like.\n\n\n\nFigure B.9: A successful commit message\n\n\nYou may see a different message, which probably means that the commit didn’t happen successfully.\n\nIf you get a message that says that Git doesn’t know who you are, and asks you to set your name and email, then you did not finish setting up RStudio as detailed in Section 2.5. To fix this, re-run the first two commands from that section (the two commands that begin git config ...). Then try to commit your work again.\n\nStep 3: Push the commit to GitHub.\nFinally, we need to upload the new commit to GitHub (because it currently only exists in RStudio - thus GitHub is out-of-date, so we need to force it to sync with the new version of the project in RStudio). Git refers to this process as Pushing.\nIn the top of either the Git tab or the commit pop-up is a upwards-pointing green arrow labelled Push. Click this to send your work back to Github. It should just work (without asking for your Github username or password), in which case you should see a message like ?fig-rstudio-successful-push .\n If you instead get asked for your GitHub username and password, then this means that RStudio does not have a valid Personal Access Token for GitHub. To fix this, repeat the second half of the setup instructions in Figure B.9, starting with this line: install.packages(c(\"usethis\",\"gitcreds\"))."
  },
  {
    "objectID": "src/book/B_additional_setup_appendix.html#sec-vscode-setup-appendix",
    "href": "src/book/B_additional_setup_appendix.html#sec-vscode-setup-appendix",
    "title": "Appendix B — Additional Software Set-up",
    "section": "B.3 VS Code IDE Set-up",
    "text": "B.3 VS Code IDE Set-up\n\n\n\n\n\n\nWarning\n\n\n\nSection coming soon."
  }
]